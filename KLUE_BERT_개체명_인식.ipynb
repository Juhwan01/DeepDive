{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGTfKBpUAoi+czmUOwep3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhwan01/DeepDive/blob/main/KLUE_BERT_%EA%B0%9C%EC%B2%B4%EB%AA%85_%EC%9D%B8%EC%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMSZv41rhrxo"
      },
      "outputs": [],
      "source": [
        "# ì´ì „ê³¼ ë˜‘ê°™ì´ ë ˆê±°ì‹œ ëª¨ë“œë¡œ\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "LTiaIj-NlCUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIO íƒœê·¸ ì²´ê³„ ì •ë¦¬\n",
        "\n",
        "### âœ… BIOë€?\n",
        "- B-XXX: ê°œì²´ëª… ì‹œì‘ (Begin)\n",
        "- I-XXX: ê°œì²´ëª… ë‚´ë¶€ (Inside)\n",
        "- O: ê°œì²´ëª… ì•„ë‹˜ (Outside)\n",
        "\n",
        "### ğŸ“Œ ì˜ˆì‹œ ë¬¸ì¥\n",
        "**ë¬¸ì¥:** í™ê¸¸ë™ì€ ì„œìš¸ëŒ€í•™êµë¥¼ ì¡¸ì—…í–ˆë‹¤.\n",
        "\n",
        "| ë‹¨ì–´       | íƒœê·¸    | ì„¤ëª…                         |\n",
        "|------------|---------|------------------------------|\n",
        "| í™ê¸¸ë™     | B-PER   | ì‚¬ëŒ ì´ë¦„ ì‹œì‘               |\n",
        "| ì€         | O       | ê°œì²´ëª… ì•„ë‹˜                  |\n",
        "| ì„œìš¸ëŒ€í•™êµ | B-ORG   | ê¸°ê´€ ì´ë¦„ ì‹œì‘               |\n",
        "| ë¥¼         | O       | ê°œì²´ëª… ì•„ë‹˜                  |\n",
        "| ì¡¸ì—…í–ˆë‹¤   | O       | ê°œì²´ëª… ì•„ë‹˜                  |\n",
        "\n",
        "### ğŸ¯ ì—¬ëŸ¬ ë‹¨ì–´ë¡œ ëœ ê°œì²´ëª… ì˜ˆì‹œ\n",
        "\n",
        "**ë¬¸ì¥:** ëŒ€í•œ ë¯¼êµ­ ì •ë¶€ëŠ”\n",
        "\n",
        "| ë‹¨ì–´   | íƒœê·¸    | ì„¤ëª…               |\n",
        "|--------|---------|--------------------|\n",
        "| ëŒ€í•œ   | B-ORG   | ê¸°ê´€ ì´ë¦„ ì‹œì‘     |\n",
        "| ë¯¼êµ­   | I-ORG   | ê¸°ê´€ ì´ë¦„ ë‚´ë¶€     |\n",
        "| ì •ë¶€   | I-ORG   | ê¸°ê´€ ì´ë¦„ ë‚´ë¶€     |\n",
        "\n",
        "### ğŸ·ï¸ ìì£¼ ì“°ì´ëŠ” ê°œì²´ëª… íƒœê·¸\n",
        "\n",
        "| íƒœê·¸       | ì˜ë¯¸           |\n",
        "|------------|----------------|\n",
        "| PER        | ì‚¬ëŒ ì´ë¦„      |\n",
        "| LOC        | ìœ„ì¹˜/ì¥ì†Œ      |\n",
        "| ORG        | ê¸°ê´€/íšŒì‚¬ ì´ë¦„ |\n",
        "| DATE       | ë‚ ì§œ           |\n",
        "| TIME       | ì‹œê°„           |\n",
        "| MONEY      | ê¸ˆì•¡           |\n",
        "| PERCENT    | ë°±ë¶„ìœ¨         |\n",
        "| O          | ê°œì²´ëª… ì•„ë‹˜    |\n",
        "\n",
        "### ğŸ”„ ë‹¤ë¥¸ íƒœê·¸ ì²´ê³„ (ì°¸ê³ ìš©)\n",
        "- BIO: ê°€ì¥ ì¼ë°˜ì ì¸ íƒœê·¸ ì²´ê³„\n",
        "- BILOU: Begin, Inside, Last, Outside, Unit\n",
        "- BIOES: Begin, Inside, Outside, End, Single\n"
      ],
      "metadata": {
        "id": "7O8nlJJhZmrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import shape_list, BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# seqevalì€ NERì²˜ëŸ¼ ë¬¸ì¥ ë‚´ ë‹¨ì–´ë§ˆë‹¤ ë¼ë²¨ì´ ë¶™ëŠ” ì‹œí€€ìŠ¤ ë¼ë²¨ë§ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ë•Œ ì‚¬ìš©í•¨\n",
        "# ì¼ë°˜ì ì¸ ì •í™•ë„ì™€ ë‹¤ë¥´ê²Œ, í† í° ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ ì—”í‹°í‹° ë‹¨ìœ„ í‰ê°€ë„ ê°€ëŠ¥í•¨\n",
        "# ì˜ˆ: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ì‹œí€€ìŠ¤ ë‹¨ìœ„ë¡œ ë¹„êµí•¨\n",
        "#y_true = [[\"B-PER\", \"I-PER\", \"O\", \"B-LOC\"]]\n",
        "#y_pred = [[\"B-PER\", \"I-PER\", \"O\", \"B-ORG\"]]\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "import tensorflow as tf\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "mZi49vAsj9ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "# í•™ìŠµ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/refs/heads/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_train_data.csv\",\n",
        "    filename=\"ner_train_data.csv\"\n",
        ")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/refs/heads/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_test_data.csv\",\n",
        "    filename=\"ner_test_data.csv\"\n",
        ")\n",
        "\n",
        "# ë¼ë²¨ ì •ë³´ ë‹¤ìš´ë¡œë“œ\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/refs/heads/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_label.txt\",\n",
        "    filename=\"ner_label.txt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "7OQDlHzlkHu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV íŒŒì¼ ë¡œë“œ\n",
        "train_ner_df = pd.read_csv(\"ner_train_data.csv\")[:30000]\n",
        "test_ner_df = pd.read_csv(\"ner_test_data.csv\")"
      ],
      "metadata": {
        "id": "7FC8FFnMmbwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ner_df.head()"
      ],
      "metadata": {
        "id": "pEPAxf5nmc4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ner_df.head()"
      ],
      "metadata": {
        "id": "1X14U2N-oT2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('í›ˆë ¨ìš© ìƒ˜í”Œ ê°œìˆ˜ :', len(train_ner_df))\n",
        "print('í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ê°œìˆ˜ :', len(test_ner_df))"
      ],
      "metadata": {
        "id": "XjJcPaXHLOgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ë°ì´í„°ì˜ 'Sentence' ì—´ì—ì„œ ê° ë¬¸ì¥ì„ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ í† í°í™”í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "# ì˜ˆ: \"ì´ìˆœì‹  ì€ ìœ„ì¸ ì´ë‹¤\" â†’ ['ì´ìˆœì‹ ', 'ì€', 'ìœ„ì¸', 'ì´ë‹¤']\n",
        "train_data_sentence = [sent.split() for sent in train_ner_df['Sentence'].values]\n",
        "\n",
        "# í•™ìŠµ ë°ì´í„°ì˜ 'Tag' ì—´ì—ì„œ ê° ë¬¸ì¥ì— ëŒ€í•œ ê°œì²´ëª… íƒœê·¸ë¥¼ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "# ì˜ˆ: \"PER-B O O O\" â†’ ['PER-B', 'O', 'O', 'O']\n",
        "train_data_label = [label.split() for label in train_ner_df['Tag'].values]\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'Sentence' ì—´ì—ì„œ ê° ë¬¸ì¥ì„ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ í† í°í™”í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "test_data_sentence = [sent.split() for sent in test_ner_df['Sentence'].values]\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'Tag' ì—´ì—ì„œ ê° ë¬¸ì¥ì— ëŒ€í•œ ê°œì²´ëª… íƒœê·¸ë¥¼ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "test_data_label = [label.split() for label in test_ner_df['Tag'].values]\n"
      ],
      "metadata": {
        "id": "PcHxUKkFLbfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data_sentence[2])\n",
        "print(train_data_label[2])"
      ],
      "metadata": {
        "id": "LoMmlca4OQSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ ë°ì´í„°ëŠ” í˜•íƒœì†Œ ë‹¨ìœ„ x -> ì–´ì ˆ(ë„ì–´ì“°ê¸°) ë‹¨ìœ„ë¡œ ê°œì²´ëª… ì¸ì‹\n",
        "# \"ner_label.txt\" íŒŒì¼ì„ ì½ì–´ì„œ ê° ë¼ë²¨ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "# open() í•¨ìˆ˜ë¡œ íŒŒì¼ì„ ì½ê¸° ëª¨ë“œ('r')ì™€ UTF-8 ì¸ì½”ë”©ì„ ì§€ì •í•˜ì—¬ ì—°ë‹¤\n",
        "# íŒŒì¼ì˜ ê° ì¤„(line)ì„ ì½ì–´ì™€ strip() í•¨ìˆ˜ë¡œ ì•ë’¤ ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ì œê±°\n",
        "# ì´ë ‡ê²Œ ì •ì œëœ ë¼ë²¨ë“¤ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ labels ë³€ìˆ˜ì— ì €ì¥\n",
        "labels = [label.strip() for label in open(\"ner_label.txt\", 'r', encoding='utf-8')]\n",
        "print('ê°œì²´ëª… íƒœê¹… ì •ë³´ :', labels)\n"
      ],
      "metadata": {
        "id": "kzX3t27QOZUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê°œì²´ëª… íƒœê¹…(tag)ë“¤ì„ ì¸ë±ìŠ¤ì™€ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "# enumerate()ëŠ” index ê°™ì´ ë¦¬í„´\n",
        "\n",
        "# tag_to_index: íƒœê¹… ì´ë¦„(tag)ì„ í‚¤(key)ë¡œ, í•´ë‹¹ íƒœê¹…ì˜ ì¸ë±ìŠ¤(index)ë¥¼ ê°’(value)ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
        "# ì˜ˆë¥¼ ë“¤ì–´, 'B-PER':0, 'I-PER':1 ê°™ì€ í˜•íƒœë¡œ íƒœê¹… ì´ë¦„ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ìš©ë„\n",
        "tag_to_index = {tag: index for index, tag in enumerate(labels)}\n",
        "print(tag_to_index)\n",
        "\n",
        "# index_to_tag: ì¸ë±ìŠ¤ë¥¼ í‚¤(key)ë¡œ, íƒœê¹… ì´ë¦„(tag)ì„ ê°’(value)ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
        "# ìˆ«ìë¡œ ëœ ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ì›ë˜ íƒœê¹… ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "index_to_tag = {index: tag for index, tag in enumerate(labels)}\n",
        "print(index_to_tag)\n",
        "\n",
        "# ì´ ê³¼ì •ì„ í•˜ëŠ” ì´ìœ :\n",
        "# ëª¨ë¸ í•™ìŠµ ì‹œì—ëŠ” íƒœê¹…(label)ì„ ìˆ«ì(index) í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ\n",
        "# ë°˜ëŒ€ë¡œ, ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì¸ ìˆ«ì ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” íƒœê¹… ì´ë¦„ìœ¼ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ index_to_tagê°€ í•„ìš”í•¨\n"
      ],
      "metadata": {
        "id": "DYmPy1V_Q7Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_size = len(tag_to_index)\n",
        "print('ê°œì²´ëª… íƒœê¹… ì •ë³´ì˜ ê°œìˆ˜ :', tag_size)"
      ],
      "metadata": {
        "id": "DZi79hDBS3L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "KnlTca_eaJuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = train_data_sentence[1]\n",
        "label = train_data_label[1]\n",
        "\n",
        "print('ë¬¸ì¥ :',sent)\n",
        "print('ë ˆì´ë¸” :',label)\n",
        "# ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì°¾ì•„ì„œ ì •ìˆ˜ ì¸ì½”ë”©\n",
        "print('ë ˆì´ë¸”ì˜ ì •ìˆ˜ ì¸ì½”ë”© :', [tag_to_index[idx] for idx in label])\n",
        "print('ë¬¸ì¥ì˜ ê¸¸ì´ :', len(sent))\n",
        "print('ë ˆì´ë¸”ì˜ ê¸¸ì´ :', len(label))"
      ],
      "metadata": {
        "id": "WpORZANsbuX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "# ì˜ˆë¥¼ ë“¤ì–´ 'ì¿ ë§ˆë¦¬'ë¼ëŠ” ë‹¨ì–´ê°€ BERT í† í¬ë‚˜ì´ì €ì— ì˜í•´ ['ì¿ ', '##ë§ˆë¦¬']ë¡œ ë¶„ë¦¬ëœ ê²½ìš°,\n",
        "# ì›ë˜ ë‹¨ì–´ 'ì¿ ë§ˆë¦¬'ì—ëŠ” 'B-PER'ì´ë¼ëŠ” ê°œì²´ëª… íƒœê·¸ê°€ ë¶™ì–´ ìˆì—ˆìŒ.\n",
        "\n",
        "# ë¬¸ì œëŠ” ì´ë ‡ê²Œ ë‹¨ì–´ê°€ ì„œë¸Œì›Œë“œë¡œ ë‚˜ë‰˜ë©´, ë ˆì´ë¸”ì˜ ê°œìˆ˜ì™€ í† í°ì˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•Šê²Œ ë˜ë¯€ë¡œ\n",
        "# ëª¨ë¸ì— ë„£ê¸° ìœ„í•´ì„œëŠ” ì„œë¸Œì›Œë“œ ìˆ˜ì— ë§ì¶° ë ˆì´ë¸”ë„ í™•ì¥í•´ì•¼ í•¨.\n",
        "\n",
        "# ì´ë•Œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹:\n",
        "# - ì²« ë²ˆì§¸ ì„œë¸Œì›Œë“œ(ì˜ˆ: 'ì¿ ')ì—ëŠ” ì›ë˜ ë ˆì´ë¸”(B-PER ë“±)ì„ ê·¸ëŒ€ë¡œ ë¶€ì—¬\n",
        "# - ê·¸ ë’¤ì— ë”°ë¼ì˜¤ëŠ” ì„œë¸Œì›Œë“œ(ì˜ˆ: '##ë§ˆë¦¬')ì—ëŠ” ë ˆì´ë¸”ì„ ë¶€ì—¬í•˜ì§€ ì•ŠìŒ (== ë¬´ì‹œë¨)\n",
        "\n",
        "# ì´ë ‡ê²Œ ë ˆì´ë¸”ì„ ë¶€ì—¬í•˜ì§€ ì•ŠëŠ” ìœ„ì¹˜ì—ëŠ” ë³´í†µ '-100'ì„ ë„£ìŒ.\n",
        "# - ì´ ê°’ì€ PyTorchë‚˜ TensorFlow ë“±ì—ì„œ ì†ì‹¤ ê³„ì‚° ì‹œ í•´ë‹¹ ìœ„ì¹˜ë¥¼ ë¬´ì‹œí•˜ë„ë¡ ì²˜ë¦¬ë¨.\n",
        "for one_word in sent:\n",
        "  # ê° ë‹¨ì–´ ì„œë¸Œì›Œë“œë¡œ ë¶„ë¦¬\n",
        "  subword_tokens = tokenizer.tokenize(one_word)\n",
        "  # extend()ëŠ” ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ìš”ì†Œë“¤ì„ í•˜ë‚˜ì”© ì¶”ê°€í•´ì¤€ë‹¤ <-> append()ëŠ” ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ê°\n",
        "  tokens.extend(subword_tokens)\n",
        "\n",
        "print('BERT í† í¬ë‚˜ì´ì € ì •ë£° ì¶œë ¥ :', tokens)\n",
        "print('ë ˆì´ë¸” :', label)\n",
        "print('ë ˆì´ë¸”ì˜ ì •ìˆ˜ ì¸ì½”ë”© :', [tag_to_index[idx] for idx in label])\n",
        "print('í† í°ì˜ ê¸¸ì´ :', len(tokens))\n",
        "print('ë ˆì´ë¸”ì˜ ê¸¸ì´ :', len(label))"
      ],
      "metadata": {
        "id": "SHjt97OTdOdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "labels_ids = []\n",
        "\n",
        "for one_word, label_token in zip(train_data_sentence[1], train_data_label[1]):\n",
        "  subword_tokens = tokenizer.tokenize(one_word)\n",
        "  tokens.extend(subword_tokens)\n",
        "  # ë¼ë²¨ ID ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
        "  labels_ids.extend(\n",
        "      [tag_to_index[label_token]]          # => [1] : ì²« ë²ˆì§¸ subwordì—ëŠ” ì›ë˜ ë¼ë²¨ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "      + [-100] * (len(subword_tokens) - 1) # => [-100, -100] : ë‚˜ë¨¸ì§€ subwordëŠ” ë¬´ì‹œ(-100) ì²˜ë¦¬\n",
        "  )\n",
        "print('í† í°í™” ëœ ë¬¸ì¥ :', tokens)\n",
        "print('ë ˆì´ë¸” :', ['[PAD]' if idx == -100 else index_to_tag[idx] for idx in labels_ids])\n",
        "print('íŒ¨ë”©ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”© :', labels_ids)\n",
        "print('í† í°ì˜ ê¸¸ì´ :', len(tokens))\n",
        "print('ë ˆì´ë¸”ì˜ ê¸¸ì´ :', len(labels_ids))"
      ],
      "metadata": {
        "id": "dlHHmoKPe1tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ğŸ§  `convert_examples_to_features` í•¨ìˆ˜ ì „ì²´ íë¦„ ì •ë¦¬\n",
        "\n",
        "ì´ í•¨ìˆ˜ëŠ” NER ë°ì´í„°ì…‹(ì˜ˆ: ë‹¨ì–´/ë¼ë²¨ ìŒ ë¦¬ìŠ¤íŠ¸)ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ **ì •ì œí•˜ê³  ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜**ì…ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œëŠ” BERTê°€ ìš”êµ¬í•˜ëŠ” 4ê°€ì§€ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤:\n",
        "\n",
        "* `input_ids`\n",
        "* `attention_mask`\n",
        "* `token_type_ids`\n",
        "* `data_labels` (NERìš© ë¼ë²¨ ì‹œí€€ìŠ¤)\n",
        "\n",
        "## 1ï¸âƒ£ íŠ¹ìˆ˜ í† í° ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "\n",
        "* BERT ëª¨ë¸ì€ ë¬¸ì¥ ì•ì— `[CLS]`, ëì— `[SEP]` ê°™ì€ **íŠ¹ìˆ˜ í† í°**ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "* Hugging Face í† í¬ë‚˜ì´ì €ëŠ” ì´ëŸ° íŠ¹ìˆ˜ í† í°ë“¤ì„ ìë™ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n",
        "* ì˜ˆë¥¼ ë“¤ì–´:\n",
        "\n",
        "  * `[CLS]`: ë¬¸ì¥ì˜ ì‹œì‘\n",
        "  * `[SEP]`: ë¬¸ì¥ì˜ ë\n",
        "  * `[PAD]`: íŒ¨ë”© ìë¦¬ ì±„ìš°ê¸°ìš©\n",
        "* ì´ í† í°ë“¤ì€ ëª¨ë¸ì˜ ë™ì‘ì— ë§¤ìš° ì¤‘ìš”í•˜ë©°, ì‹œí€€ìŠ¤ ë¶„ë¥˜ë‚˜ ë¬¸ì¥ìŒ ê´€ê³„ íŒŒì•…ì—ë„ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "## 2ï¸âƒ£ ê° ë¬¸ì¥-ë¼ë²¨ ìŒì— ëŒ€í•´ ë°˜ë³µ\n",
        "\n",
        "NER ë°ì´í„°ëŠ” ë³´í†µ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ì…ë‹ˆë‹¤:\n",
        "\n",
        "```python\n",
        "examples = [['ë‚˜ëŠ”', 'í•™ìƒì´ë‹¤'], ['ë„ˆëŠ”', 'ì˜ì‚¬ë‹¤']]\n",
        "labels = [['O', 'B-PER'], ['O', 'B-PROF']]\n",
        "```\n",
        "\n",
        "* ê° ë‹¨ì–´ë¥¼ BERT í† í¬ë‚˜ì´ì €ë¡œ **ì„œë¸Œì›Œë“œ ìˆ˜ì¤€ê¹Œì§€ ë¶„í•´**í•©ë‹ˆë‹¤.\n",
        "* ì˜ˆ: `'í•™ìƒì´ë‹¤' â†’ ['í•™ìƒ', '##ì´ë‹¤']`\n",
        "* ë¬¸ì œ: ì„œë¸Œì›Œë“œë¡œ ë‚˜ë‰˜ë©´ ì›ë˜ ë‹¨ì–´ì˜ ë¼ë²¨ì„ **ë³µìˆ˜ì˜ í† í°ì— ë¶„ë°°**í•´ì•¼ í•¨\n",
        "\n",
        "### ğŸ‘‰ ë¼ë²¨ ì²˜ë¦¬ ë°©ë²•\n",
        "\n",
        "* ì²« ì„œë¸Œì›Œë“œì—ëŠ” **ì •ìƒ ë¼ë²¨ ë¶€ì—¬**\n",
        "* ë‚˜ë¨¸ì§€ ì„œë¸Œì›Œë“œì—ëŠ” \\*\\*íŒ¨ë”©ìš© ë¼ë²¨ê°’ (-100)\\*\\*ì„ ë¶€ì—¬ (ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¬´ì‹œë¨)\n",
        "\n",
        "## 3ï¸âƒ£ ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ì˜ë¼ëƒ„\n",
        "\n",
        "* BERTëŠ” ì…ë ¥ í† í° ìˆ˜ê°€ **ìµœëŒ€ 512ê°œ**ì§€ë§Œ, ì´ í•¨ìˆ˜ì—ì„œëŠ” ì˜ˆë¥¼ ë“¤ì–´ `max_seq_len = 64`ë¡œ ì œí•œí•œë‹¤ê³  ê°€ì •\n",
        "* `[CLS]`ì™€ `[SEP]`ì„ í¬í•¨í•´ì•¼ í•˜ë¯€ë¡œ, ì‹¤ì œ ë‹¨ì–´ëŠ” **64 - 2 = 62ê°œê¹Œì§€ë§Œ** ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
        "* ì´ë¥¼ ì´ˆê³¼í•œ í† í°ì€ ë’¤ì—ì„œ ì˜ë¼ëƒ…ë‹ˆë‹¤ (**truncation**)\n",
        "\n",
        "## 4ï¸âƒ£ `[CLS]`, `[SEP]` íŠ¹ìˆ˜ í† í° ë¶™ì´ê¸°\n",
        "\n",
        "* ì•ì— `[CLS]`, ë’¤ì— `[SEP]`ì„ ë¶™ì…ë‹ˆë‹¤\n",
        "* í† í°ë¿ ì•„ë‹ˆë¼, ë¼ë²¨ ë°°ì—´ì—ë„ ê°™ì€ ìœ„ì¹˜ì— `-100`ì„ ë¶™ì…ë‹ˆë‹¤\n",
        "  ì´ìœ : ì´ íŠ¹ìˆ˜ í† í°ë“¤ì—” ë¼ë²¨ì´ ì—†ìœ¼ë¯€ë¡œ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸í•´ì•¼ í•©ë‹ˆë‹¤\n",
        "\n",
        "## 5ï¸âƒ£ ì •ìˆ˜ ì¸ì½”ë”© + ì–´í…ì…˜ ë§ˆìŠ¤í¬ + ì„¸ê·¸ë¨¼íŠ¸ ì•„ì´ë””\n",
        "\n",
        "* BERTëŠ” í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°›ì§€ ì•Šê³ , **ì •ìˆ˜ ID ë°°ì—´**ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤\n",
        "* í† í° ë°°ì—´ì„ `input_ids`ë¡œ ë°”ê¾¸ê³ \n",
        "  ê° í† í° ìœ„ì¹˜ì—” 1, íŒ¨ë”©ì—” 0ì„ ë‘ëŠ” **attention\\_mask**ë„ ìƒì„±í•©ë‹ˆë‹¤\n",
        "* `token_type_ids`ëŠ” ë¬¸ì¥ìŒì¼ ë•Œ ë¬¸ì¥ êµ¬ë¶„ìš©ì¸ë°, ì—¬ê¸°ì„  ì „ë¶€ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤ (ë‹¨ì¼ ë¬¸ì¥ì´ë¯€ë¡œ)\n",
        "\n",
        "## 6ï¸âƒ£ íŒ¨ë”© ì¶”ê°€\n",
        "\n",
        "* ìµœëŒ€ ê¸¸ì´ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ì€ ë¶€ì¡±í•œ ë§Œí¼ `[PAD]`ë¡œ ì±„ì›ë‹ˆë‹¤\n",
        "* `input_ids`, `attention_mask`, `token_type_ids`, `data_labels` ëª¨ë‘ **ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶°ì•¼** í•˜ë¯€ë¡œ,\n",
        "  ê°ê° ì ì ˆí•œ íŒ¨ë”© ê°’ìœ¼ë¡œ ë§ì¶¥ë‹ˆë‹¤:\n",
        "\n",
        "  * ì…ë ¥ ID â†’ `[PAD]` í† í°ì˜ ì¸ë±ìŠ¤\n",
        "  * ë§ˆìŠ¤í¬ â†’ `0`\n",
        "  * ì„¸ê·¸ë¨¼íŠ¸ ID â†’ `0`\n",
        "  * ë ˆì´ë¸” â†’ `-100` (ë¬´ì‹œìš©)\n",
        "\n",
        "## 7ï¸âƒ£ ì •í•©ì„± ì²´í¬ (ë””ë²„ê¹… ìš©ë„)\n",
        "\n",
        "* ê° ë°°ì—´ì˜ ê¸¸ì´ê°€ `max_seq_len`ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ `assert`ë¡œ í™•ì¸í•©ë‹ˆë‹¤\n",
        "* ê°œë°œ ì¤‘ ì˜ëª»ëœ ê¸¸ì´ì˜ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤\n",
        "\n",
        "## 8ï¸âƒ£ ëª¨ë“  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì \n",
        "\n",
        "* í•˜ë‚˜ì˜ ë¬¸ì¥ ì²˜ë¦¬ê°€ ëë‚˜ë©´, `input_ids`, `attention_mask`, `token_type_ids`, `data_labels` ë¦¬ìŠ¤íŠ¸ì— ê°ê° ì¶”ê°€í•©ë‹ˆë‹¤\n",
        "* ë¦¬ìŠ¤íŠ¸ ì•ˆì— **í•œ ë¬¸ì¥ì— ëŒ€í•œ ê²°ê³¼ê°€ ë°°ì—´ë¡œ ë“¤ì–´ê°€ëŠ” êµ¬ì¡°**ì…ë‹ˆë‹¤\n",
        "\n",
        "## 9ï¸âƒ£ NumPy ë°°ì—´ë¡œ ë³€í™˜\n",
        "\n",
        "* í•™ìŠµ ì „ì— ëª¨ë“  ë¦¬ìŠ¤íŠ¸ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
        "* í…ì„œí”Œë¡œìš° ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ NumPy ë˜ëŠ” Tensor í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë°›ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤\n",
        "\n",
        "## ğŸ”š ìµœì¢… ì¶œë ¥\n",
        "\n",
        "í•¨ìˆ˜ëŠ” ë‹¤ìŒì„ ë°˜í™˜í•©ë‹ˆë‹¤:\n",
        "\n",
        "```python\n",
        "(\n",
        "  (input_ids, attention_mask, token_type_ids),  # BERT ì…ë ¥\n",
        "  data_labels                                   # NER ë¼ë²¨\n",
        ")\n",
        "```\n",
        "\n",
        "## âœ… ìš”ì•½\n",
        "\n",
        "| ë‹¨ê³„ | ì„¤ëª…                                    |\n",
        "| -- | ------------------------------------- |\n",
        "| 1  | ë‹¨ì–´ â†’ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì§• ë° ë¼ë²¨ í™•ì¥               |\n",
        "| 2  | `[CLS]`, `[SEP]` ë¶™ì´ê¸°                  |\n",
        "| 3  | ì‹œí€€ìŠ¤ ê¸¸ì´ ì´ˆê³¼ ì‹œ ìë¥´ê¸°                       |\n",
        "| 4  | ì •ìˆ˜ ì¸ì½”ë”©, attention mask, segment id ìƒì„± |\n",
        "| 5  | íŒ¨ë”© ì¶”ê°€ ë° ê¸¸ì´ ë§ì¶¤                         |\n",
        "| 6  | ëª¨ë“  í•­ëª© ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì                          |\n",
        "| 7  | NumPy ë°°ì—´ë¡œ ë³€í™˜ í›„ ë°˜í™˜                     |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "evm47ogu91ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples,labels, max_seq_len, tokenizer,\n",
        "                                 pad_token_id_for_segment=0,\n",
        "                                 pad_token_id_for_label=-100):\n",
        "    # Hugging Face í† í¬ë‚˜ì´ì €ì—ì„œ íŠ¹ìˆ˜ í† í° ì •ë³´ë¥¼ ì¶”ì¶œ\n",
        "    # ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ í† í° (ì˜ˆ: '[CLS]')\n",
        "    # BERTì—ì„œëŠ” ë¬¸ì¥ì˜ ì²« í† í°ìœ¼ë¡œ í•­ìƒ ì‚¬ìš©ë˜ë©°, ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤\n",
        "    cls_token = tokenizer.cls_token\n",
        "\n",
        "    # ë¬¸ì¥ì˜ ë ë˜ëŠ” ë¬¸ì¥ ì‚¬ì´ì˜ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ í† í° (ì˜ˆ: '[SEP]')\n",
        "    # BERTëŠ” ë¬¸ì¥ í•˜ë‚˜ì¼ ê²½ìš° ë¬¸ì¥ ëì—, ë¬¸ì¥ ë‘ ê°œì¼ ê²½ìš° ì¤‘ê°„ê³¼ ëì— [SEP]ë¥¼ ë„£ëŠ”ë‹¤\n",
        "    sep_token = tokenizer.sep_token\n",
        "\n",
        "    # íŒ¨ë”© í† í°ì˜ ìˆ«ì ì¸ë±ìŠ¤ (ì˜ˆ: 0)\n",
        "    # ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶”ê¸° ìœ„í•´ ì‚¬ìš©ë˜ë©°, ì†ì‹¤ ê³„ì‚° ì‹œ ë¬´ì‹œë¨\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, data_labels = [],[],[],[]\n",
        "\n",
        "    for example, label in tqdm(zip(examples,labels), total = len(examples)):\n",
        "        tokens = []\n",
        "        labels_ids = []\n",
        "        # ìœ„ì— í•´ë³¸ê²ƒê³¼ ê°™ì´ í† í°í™” ì§„í–‰\n",
        "        for one_word, label_token in zip(example,label):\n",
        "            subword_tokens = tokenizer.tokenize(one_word)\n",
        "            tokens.extend(subword_tokens)\n",
        "            labels_ids.extend([tag_to_index[label_token]]+[pad_token_id_for_label]*(len(subword_tokens)-1))\n",
        "        # BERTì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ: BERTëŠ” ë³´í†µ 512 í† í°ê¹Œì§€ë§Œ ì²˜ë¦¬ ê°€ëŠ¥\n",
        "        # íŠ¹ìˆ˜ í† í° ê³µê°„ í™•ë³´: [CLS]ì™€ [SEP] í† í°ì„ ìœ„í•œ ìë¦¬(2ê°œ) í•„ìš”\n",
        "        special_tokens_count = 2\n",
        "        # max_seq_len = 64ë¡œ ì„¤ì •í–ˆë‹¤ë©´\n",
        "        # ì‹¤ì œ í† í°ì€ 64 - 2 = 62ê°œê¹Œì§€ë§Œ ì‚¬ìš©\n",
        "        # 62ê°œë³´ë‹¤ ê¸´ ë¬¸ì¥ì€ ë’¤ìª½ì„ ì˜ë¼ëƒ„ (truncation)\n",
        "        # [CLS] + í† í°ë“¤(ìµœëŒ€62ê°œ) + [SEP] í˜•íƒœë¡œ êµ¬ì„±\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
        "            labels_ids = labels_ids[:(max_seq_len - special_tokens_count)]\n",
        "\n",
        "        # sep í† í° ì¶”ê°€\n",
        "        # 1. í† í°í™” ê²°ê³¼ ë [SEP] í† í° ì¶”ê°€\n",
        "        # 2. ë ˆì´ë¸”ì—ë„ ë§¨ ë’· ë¶€ë¶„ì— -100 ì¶”ê°€.\n",
        "        tokens += [sep_token]\n",
        "        labels_ids += [pad_token_id_for_label]\n",
        "\n",
        "        # cls í† í° ì¶”ê°€\n",
        "        # 1. í† í°í™” ê²°ê³¼ ì• [CLS] í† í° ì¶”ê°€\n",
        "        # 2. ë ˆì´ë¸”ì˜ ë§¨ ì• ë¶€ë¶„ì—ë„ -100 ì¶”ê°€\n",
        "        tokens = [cls_token]+tokens\n",
        "        labels_ids = [pad_token_id_for_label]+labels_ids\n",
        "\n",
        "        # ì •ìˆ˜ ì¸ì½”ë”©\n",
        "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        attention_mask = [1] * len(input_id)\n",
        "\n",
        "        # ì •ìˆ˜ ì¸ì½”ë”©ì— ì¶”ê°€í•  íŒ¨ë”© ê¸¸ì´ ì—°ì‚°\n",
        "        padding_count = max_seq_len - len(tokens)\n",
        "\n",
        "        # ì •ìˆ˜ ì¸ì½”ë”©, ì–´í…ì…˜ ë§ˆìŠ¤í¬ íŒ¨ë”© ì¶”ê°€\n",
        "        input_id = input_id + ([pad_token_id]*padding_count)\n",
        "        attention_mask = attention_mask + ([0]*padding_count)\n",
        "\n",
        "        # ì„¸ê·¸ë¨¼íŠ¸ ì¸ì½”ë”©.\n",
        "        token_type_id = [pad_token_id_for_segment]*max_seq_len\n",
        "\n",
        "        # ë ˆì´ë¸” íŒ¨ë”© -> ì—¬ê¸°ì„œëŠ” íŒ¨ë”© ì•„ì´ë”” -100\n",
        "        label = labels_ids + ([pad_token_id_for_label]*padding_count)\n",
        "\n",
        "        # assert -> ë””ë²„ê¹… ë„êµ¬(ì• ë…¸íŠ¸ì—ì„œ ì„¤ëª…í•¨)\n",
        "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "        assert len(label) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label), max_seq_len)\n",
        "\n",
        "        # ìµœì¢… ê²°ê³¼ë¥¼ Listì— ì´ì–´ ë¶™ì¸ë‹¤ ì´ ê²½ìš° appendì´ê¸° ë•Œë¬¸ì— [[]] ì´ëŸ°ì‹ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ê°„ë‹¤\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(label)\n",
        "\n",
        "    # ì•ì˜ ë…¸íŠ¸ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ np ë°°ì—´ í˜•íƒœë¡œ ì „í™˜\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), data_labels"
      ],
      "metadata": {
        "id": "Gbbg9nGByBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = convert_examples_to_features(train_data_sentence,\n",
        "                                                train_data_label,max_seq_len=128, tokenizer=tokenizer)\n",
        "X_test, y_test = convert_examples_to_features(test_data_sentence,\n",
        "                                              test_data_label,max_seq_len=128, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "HkqVWKCb7Xl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ê¸°ì¡´ ì›ë¬¸ :', train_data_sentence[0])\n",
        "print('ê¸°ì¡´ ë ˆì´ë¸” :', train_data_label[0])\n",
        "print('-'*50)\n",
        "print('í† í°í™” í›„ ì›ë¬¸ :',tokenizer.convert_ids_to_tokens(X_train[0][0]))\n",
        "# íŠ¸ë ˆì¸ ë°ì´í„° ë¼ë²¨ì˜ ì²« ë²ˆì§¸ í–‰ì˜ ê°’ = ë°°ì—´\n",
        "# ë°˜ë³µë¬¸ ëŒì•„ì„œ í•˜ë‚˜í•˜ë‚˜ ìš”ì†Œ ì²´í¬í•˜ë©´ì„œ -100 ì´ë©´ [PAD] ì•„ë‹ˆë©´ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ì— ë§ëŠ” í† í° ì¶œë ¥\n",
        "print('í† í°í™” í›„ ë ˆì´ë¸” :',['[PAD]' if index == -100 else index_to_tag[index] for index in y_train[0]])\n",
        "print('-'*50)\n",
        "print('ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼',X_train[0][0])\n",
        "print('ì •ìˆ˜ ì¸ì½”ë”© ë ˆì´ë¸”',y_train[0])"
      ],
      "metadata": {
        "id": "Du166CJgXfVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ì„¸ê·¸ë¨¼íŠ¸ ì¸ì½”ë”© :', X_train[2][0])"
      ],
      "metadata": {
        "id": "Gbbtjr2Uaald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ì–´í…ì…˜ ë§ˆìŠ¤í¬ :', X_train[1][0])"
      ],
      "metadata": {
        "id": "Ab9eKG2gtI4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERTë¥¼ ì´ìš©í•œ ê°œì²´ëª… ì¸ì‹(NER) êµ¬í˜„ - ë‹¤ëŒ€ë‹¤(Many-to-Many) êµ¬ì¡°\n",
        "class TFBertForSequenceClassification(tf.keras.Model):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "\n",
        "        # ì‚¬ì „ í•™ìŠµëœ KoBERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (PyTorch ëª¨ë¸ì„ TFì—ì„œ ì‚¬ìš©)\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
        "\n",
        "        # ì¶œë ¥ì¸µ: ê° í† í°ì— ëŒ€í•´ num_labelsê°œì˜ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•´ì•¼ í•˜ë¯€ë¡œ Dense(num_labels) ì‚¬ìš©\n",
        "        # ì…ë ¥: (batch_size, seq_len, hidden_size=768)\n",
        "        # ì¶œë ¥: (batch_size, seq_len, num_labels)\n",
        "        # í™œì„±í™” í•¨ìˆ˜ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ â†’ softmaxëŠ” ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬í•¨ -> ë‚˜ì¤‘ì— ì•„ë˜ lossì •ì˜ ë¶€ë¶„ì—ì„œ from_logits=True -> ë‚´ë¶€ì ìœ¼ë¡œ softmax(logits)ë¥¼ ë¨¼ì € ê³„ì‚°í•œ ë‹¤ìŒì— cross entropyë¥¼ ê³„ì‚°í•¨.\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            num_labels,\n",
        "            kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
        "            name='classifier'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask, token_type_ids = inputs\n",
        "\n",
        "        # BERT ëª¨ë¸ì˜ ì¶œë ¥: ëª¨ë“  í† í°ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°\n",
        "        # shape: (batch_size, seq_len=128, hidden_size=768)\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        # ê°œì²´ëª… ì¸ì‹ì€ ë¬¸ì¥ì´ ì•„ë‹Œ ê° í† í°ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì´ë¯€ë¡œ\n",
        "        # BERTì˜ ì‹œí€€ìŠ¤ ì¶œë ¥ ì „ì²´(outputs[0])ë¥¼ ì‚¬ìš©í•¨\n",
        "        all_output = outputs[0]  # shape: (batch_size, 128, 768)\n",
        "\n",
        "        # ê° í† í°ì˜ ë²¡í„°(768ì°¨ì›)ë¥¼ Dense ë ˆì´ì–´ë¥¼ í†µí•´ 13ê°œ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜\n",
        "        # ìµœì¢… ì¶œë ¥: (batch_size, 128, 13)\n",
        "        prediction = self.classifier(all_output)\n",
        "\n",
        "        return prediction\n"
      ],
      "metadata": {
        "id": "FaTiqGjGtQyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.constantë€?\n",
        "# tf.constantëŠ” TensorFlowì—ì„œ ìƒìˆ˜ í…ì„œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "labels = tf.constant([[-100, 2, 1, -100]])\n",
        "logits = tf.constant([[[0.8, 0.1, 0.1], [0.06, 0.04, 0.9], [0.75, 0.1, 0.15],\n",
        "                      [0.4, 0.5, 0.1]]])"
      ],
      "metadata": {
        "id": "AwbaPaAK8LuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -1ì€ ë‚¨ì€ ì°¨ì›ì„ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ë¼ëŠ” ëœ»\n",
        "#(-1,) ì „ì²´ëŠ” 1ì°¨ì› í…ì„œë¡œ ë§Œë“¤ë˜\n",
        "# ê·¸ ê¸¸ì´ëŠ” ì „ì²´ ì›ì†Œ ìˆ˜ì— ë§ì¶° ìë™ ê³„ì‚°í•˜ë¼ëŠ” ì˜ë¯¸\n",
        "# != -100 -> Boolean ë§ˆìŠ¤í¬ë¡œ ë§Œë“ ë‹¤ -> 1ì°¨ì› ë¶ˆë¦¬ì–¸ í…ì„œ\n",
        "active_loss = tf.reshape(labels,(-1,)) != -100\n",
        "print(active_loss)"
      ],
      "metadata": {
        "id": "tlXqeZNN9o3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ğŸ“Œ  ê¸°ë³¸ ê°œë…\n",
        "#### 1ï¸âƒ£ `tf.reshape(tensor, new_shape)`\n",
        "\n",
        "* í…ì„œì˜ ëª¨ì–‘(ì°¨ì›)ì„ ë°”ê¾¸ëŠ” í•¨ìˆ˜ì•¼.\n",
        "* **ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , ë°°ì¹˜ë¥¼ í’€ê±°ë‚˜ ë¶™ì´ê¸° ìœ„í•´ ì£¼ë¡œ ì‚¬ìš©**í•´.\n",
        "* ì˜ˆ: `[1, 4, 3]` â†’ `[4, 3]`\n",
        "\n",
        "#### 2ï¸âƒ£ `tf.boolean_mask(tensor, mask)`\n",
        "\n",
        "* `tensor`ì—ì„œ `mask == True`ì¸ ìœ„ì¹˜ë§Œ ê³¨ë¼ë‚´ëŠ” í•¨ìˆ˜ì•¼.\n",
        "* ì˜ˆ:\n",
        "\n",
        "  ```python\n",
        "  x = [[10, 20], [30, 40], [50, 60]]\n",
        "  mask = [True, False, True]\n",
        "  tf.boolean_mask(x, mask)\n",
        "  â†’ ê²°ê³¼: [[10, 20], [50, 60]]\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "### âœ… ì§€ê¸ˆ ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ë°ì´í„°\n",
        "\n",
        "```python\n",
        "# ì •ë‹µ ë¼ë²¨\n",
        "labels = tf.constant([[-100, 2, 1, -100]])  # shape: [1, 4]\n",
        "\n",
        "# ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ (logits)\n",
        "logits = tf.constant([[\n",
        "    [0.8, 0.1, 0.1],    # í† í°1\n",
        "    [0.06, 0.04, 0.9],  # í† í°2\n",
        "    [0.75, 0.1, 0.15],  # í† í°3\n",
        "    [0.4, 0.5, 0.1]     # í† í°4\n",
        "]])  # shape: [1, 4, 3]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ‘£ í•œ ì¤„ì”© ë”°ë¼ê°€ ë³´ì\n",
        "\n",
        "\n",
        "#### âœ… 1. `tf.reshape(logits, (-1, 3))`\n",
        "\n",
        "```python\n",
        "reshaped_logits = tf.reshape(logits, (-1, 3))\n",
        "```\n",
        "\n",
        "* ì›ë˜ logitsëŠ” `[1, 4, 3]` â†’ (ë°°ì¹˜ 1, 4ê°œì˜ í† í°, í´ë˜ìŠ¤ 3ê°œ)\n",
        "* reshape ê²°ê³¼ëŠ” `[4, 3]`ì´ì•¼.\n",
        "\n",
        "```python\n",
        "reshaped_logits = tf.constant([\n",
        "  [0.8, 0.1, 0.1],     # CLS í† í°\n",
        "  [0.06, 0.04, 0.9],   # ë‹¨ì–´ 1\n",
        "  [0.75, 0.1, 0.15],   # ë‹¨ì–´ 2\n",
        "  [0.4, 0.5, 0.1]      # PAD í† í°\n",
        "])\n",
        "```\n",
        "\n",
        "* ì¦‰, í† í° 1ê°œë‹¹ í´ë˜ìŠ¤ 3ê°œ ì ìˆ˜ë¥¼ í‘œí˜„í•˜ëŠ” \\[4, 3]ì§œë¦¬ ë°°ì—´ì´ ë¨\n",
        "\n",
        "\n",
        "#### âœ… 2. `active_loss = tf.reshape(labels, (-1,)) != -100`\n",
        "\n",
        "```python\n",
        "# labels = [[-100, 2, 1, -100]]\n",
        "active_loss = tf.reshape(labels, (-1,)) != -100\n",
        "```\n",
        "\n",
        "* `tf.reshape(labels, (-1,))` â†’ `[ -100, 2, 1, -100 ]` (1ì°¨ì›ìœ¼ë¡œ í´ì¤Œ)\n",
        "* `!= -100` â†’ \\[False, True, True, False]\n",
        "\n",
        "ì´ê±´ ë­ëƒë©´:\n",
        "\n",
        "* 1ë²ˆì§¸ í† í° (CLS): -100 â†’ ë¬´ì‹œí•  ê±°ë‹ˆê¹Œ False\n",
        "* 2ë²ˆì§¸, 3ë²ˆì§¸ í† í°: ë¼ë²¨ ìˆìŒ â†’ True\n",
        "* 4ë²ˆì§¸ í† í° (PAD): -100 â†’ ë¬´ì‹œí•  ê±°ë‹ˆê¹Œ False\n",
        "\n",
        "```python\n",
        "active_loss = [False, True, True, False]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### âœ… 3. `tf.boolean_mask(reshaped_logits, active_loss)`\n",
        "\n",
        "```python\n",
        "reduced_logits = tf.boolean_mask(reshaped_logits, active_loss)\n",
        "```\n",
        "\n",
        "* `[4, 3]`ì§œë¦¬ reshaped\\_logits ì¤‘ì—ì„œ,\n",
        "* `active_loss == True`ì¸ í–‰ë§Œ ë‚¨ê²¨!\n",
        "\n",
        "ì¦‰,\n",
        "\n",
        "```python\n",
        "reshaped_logits = [\n",
        "  [0.8, 0.1, 0.1],     # ë¬´ì‹œë¨\n",
        "  [0.06, 0.04, 0.9],   # í¬í•¨\n",
        "  [0.75, 0.1, 0.15],   # í¬í•¨\n",
        "  [0.4, 0.5, 0.1]      # ë¬´ì‹œë¨\n",
        "]\n",
        "active_loss = [False, True, True, False]\n",
        "```\n",
        "\n",
        "â‡’ ê²°ê³¼:\n",
        "\n",
        "```python\n",
        "reduced_logits = [\n",
        "  [0.06, 0.04, 0.9],\n",
        "  [0.75, 0.1, 0.15]\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "### ğŸ§  ì´ì •ë¦¬ ìš”ì•½\n",
        "\n",
        "| ë‹¨ê³„                               | ì„¤ëª…                                                   |\n",
        "| -------------------------------- | ---------------------------------------------------- |\n",
        "| `reshape(logits, (-1, 3))`       | `[1, 4, 3]` â†’ `[4, 3]`ë¡œ í¼ì³ì„œ ê° í† í°ë§ˆë‹¤ì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡ì„ 1ì°¨ ì •ë ¬    |\n",
        "| `reshape(labels, (-1,)) != -100` | `[1, 4]` â†’ `[4]`ë¡œ í¼ì¹œ í›„, ìœ íš¨í•œ í† í°ë§Œ ê³¨ë¼ë‚´ëŠ” boolean mask ìƒì„± |\n",
        "| `boolean_mask(logits, mask)`     | ìœ íš¨í•œ í† í°ì˜ ì˜ˆì¸¡ ê²°ê³¼ë§Œ ì¶”ì¶œ (ì†ì‹¤ ê³„ì‚°ìš©)                           |\n",
        "\n",
        "\n",
        "í•„ìš”í•˜ë©´ `labels` ë„ ë˜‘ê°™ì´ `boolean_mask`ë¡œ ì¤„ì—¬ì„œ loss ê³„ì‚°í•˜ë©´ ë¼.\n",
        "ê·¸ ë¶€ë¶„ë„ ì´ì–´ì„œ ì„¤ëª…í•´ì¤„ ìˆ˜ ìˆì–´!\n",
        "ì´í•´ëëŠ”ì§€ ì•Œë ¤ì¤˜ â€” ê¶ê¸ˆí•œ ë¶€ë¶„ì€ ë” ì‰½ê²Œ ë‹¤ì‹œ í’€ì–´ì¤„ê²Œ.\n"
      ],
      "metadata": {
        "id": "eGflduU6BZFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# boolean_mask í•¨ìˆ˜ë¥¼ í†µí•˜ì—¬ False ìë¦¬ì˜ ê°’ ë§ˆìŠ¤í‚¹\n",
        "reduced_logits = tf.boolean_mask(tf.reshape(logits,(-1, shape_list(logits)[2])), active_loss)\n",
        "reduced_logits"
      ],
      "metadata": {
        "id": "hAdXmrvq_Lck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = tf.boolean_mask(tf.reshape(labels,(-1,)), active_loss)\n",
        "labels"
      ],
      "metadata": {
        "id": "QE5AtEkkGw0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì†ì‹¤ í•¨ìˆ˜ì˜ Reductionì´ë€?\n",
        "\n",
        "ëª¨ë¸ì´ í•œ ë²ˆì— ì—¬ëŸ¬ ë°ì´í„°(ë°°ì¹˜)ë¥¼ ì²˜ë¦¬í•  ë•Œ, ê° ë°ì´í„°ë§ˆë‹¤ ì†ì‹¤ê°’ì´ ë‚˜ì˜µë‹ˆë‹¤.\n",
        "\n",
        "**Reduction**ì€ ì´ ì†ì‹¤ê°’ë“¤ì„ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ì–´ë–»ê²Œ ì¤„ì¼ì§€(í†µí•©í• ì§€) ì •í•˜ëŠ” ì˜µì…˜ì´ì—ìš”.\n",
        "\n",
        "\n",
        "## ì£¼ìš” Reduction ì˜µì…˜\n",
        "\n",
        "- **NONE**  \n",
        "  â†’ ê° ìƒ˜í”Œë³„ ì†ì‹¤ì„ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¤„ì´ì§€ ì•ŠìŒ)  \n",
        "  â†’ í† í°ë³„ ì†ì‹¤ì„ ë”°ë¡œ ë³´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©\n",
        "\n",
        "- **SUM**  \n",
        "  â†’ ëª¨ë“  ì†ì‹¤ê°’ì„ ë‹¤ ë”í•¨  \n",
        "  â†’ ë°°ì¹˜ ì „ì²´ ì†ì‹¤ì„ í•©ì‚°\n",
        "\n",
        "- **SUM_OVER_BATCH_SIZE** (ë˜ëŠ” **AUTO**)  \n",
        "  â†’ ì†ì‹¤ê°’ì˜ í‰ê· ì„ ê³„ì‚° (ë³´í†µ ì´ê²Œ ê¸°ë³¸ê°’)  \n",
        "  â†’ ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ í‰ê·  ì†ì‹¤ ê³„ì‚°\n"
      ],
      "metadata": {
        "id": "K5rI1gwzNRiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ìœ„ì˜ ì˜ˆì‹œ ì½”ë“œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë ˆì´ë¸” ê°’ì´ -100 ì¸ ê²½ìš°ë¥¼ í•™ìŠµ ì‹œì— ë¬´ì‹œí•˜ëŠ” ë°©ë²• ì ìš© -> ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„\n",
        "def compute_loss(labels, logits):\n",
        "  # ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ softmax ì‚¬ìš© x -> from_logits=True ì„¤ì •\n",
        "  # NER ê°™ì€ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì—ì„œëŠ” ë³´í†µ ë ˆì´ë¸”ì´ ì •ìˆ˜ ì¸ë±ìŠ¤ í˜•íƒœë¡œ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— -> SparseCategoricalCrossentropy(ì •ìˆ˜ê°’)\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      # from_logits=TrueëŠ” ì…ë ¥ê°’ì´ ì•„ì§ softmaxë¥¼ ê±°ì¹˜ì§€ ì•Šì€ ì›ì‹œ ì ìˆ˜(ë¡œì§“, logits)ì„ì„ ì•Œë ¤ì£¼ëŠ” ì˜µì…˜ -> ì†ì‹¤ í•¨ìˆ˜ê°€ ë‚´ë¶€ì ìœ¼ë¡œ softmaxê³„ì‹¼ -> í™•ë¥  ë³€í™˜ -> ì†ì‹¤ ê³„ì‚°\n",
        "      # ì†ì‹¤í•¨ìˆ˜ ê³„ì‚°ì‹œ ëª¨ë“  ë°°ì¹˜ ë°ì´í„°ì˜ ì†ì‹¤ í‰ê·  or í•©ì‚° x -> ê° ë°ì´í„°ë³„ ì†ì‹¤ì„ ê°œë³„ì ìœ¼ë¡œ ë°˜í™˜\n",
        "      # ê°œì²´ëª… ì¸ì‹(Named Entity Recognition, NER) ë¬¸ì œëŠ” ì‹œí€€ìŠ¤ ë ˆì´ë¸”ë§ ë¬¸ì œë¼ì„œ, ì…ë ¥ ë¬¸ì¥ì˜ ê° í† í°ë§ˆë‹¤ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•´ìš”. ì¦‰, í† í°ë³„ë¡œ ì†ì‹¤(loss)ì„ ê³„ì‚°\n",
        "      # ë§Œì¼ í•©ì³ì§€ê±°ë‚˜ í‰ê· ë‚´ì§€ë©´ -> í† í°ë³„ ì„¸ë°€í•œ ì¡°ì • ë¶ˆê°€\n",
        "      from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "      )\n",
        "  # -100 ê°’ ë°˜ì˜ x í•˜ë„ë¡ ë¼ë²¨ ìˆ˜ì •\n",
        "  active_loss = tf.reshape(labels,(-1,))!= -100\n",
        "\n",
        "  # ë¼ë²¨ê³¼ ë¡œì§“ì— ë§ˆìŠ¤í‚¹ ì ìš©\n",
        "  reduced_logits = tf.boolean_mask(tf.reshape(logits,(-1,shape_list(logits)[2])), active_loss)\n",
        "  labels = tf.boolean_mask(tf.reshape(labels,(-1,)),active_loss)\n",
        "\n",
        "  return loss_fn(labels, reduced_logits)"
      ],
      "metadata": {
        "id": "DzY_SCcvJX4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification(\"klue/bert-base\",num_labels=tag_size)\n",
        "optimizer = tf.keras.optimizers.Adam(5e-5)\n",
        "model.compile(optimizer=optimizer, loss=compute_loss)"
      ],
      "metadata": {
        "id": "aD39LoPvIEbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ê°œì²´ëª… ì¸ì‹ì—ì„œ F1 Scoreë¥¼ ì—í­ë§ˆë‹¤ ê³„ì‚°í•˜ëŠ” ì´ìœ \n",
        "\n",
        "- **ì •í™•ë„(Accuracy)ëŠ” ë¶€ì¡±í•´ìš”**  \n",
        "  í† í° í•˜ë‚˜í•˜ë‚˜ë§Œ ë§ì•˜ëŠ”ì§€ ë³´ê¸° ë•Œë¬¸ì—, ì‹¤ì œ ê°œì²´ëª… ì „ì²´ë¥¼ ì˜ ì°¾ì•˜ëŠ”ì§€ ì•Œê¸° ì–´ë ¤ì›€\n",
        "\n",
        "- **F1 Scoreê°€ ë” ì¢‹ì•„ìš”**  \n",
        "  - ì •ë°€ë„(ëª¨ë¸ì´ ë§ë‹¤ê³  í•œ ê²ƒ ì¤‘ ì§„ì§œ ë§ì€ ë¹„ìœ¨)ì™€  \n",
        "  - ì¬í˜„ìœ¨(ì§„ì§œ ê°œì²´ëª… ì¤‘ ëª¨ë¸ì´ ì°¾ì•„ë‚¸ ë¹„ìœ¨)ì„ í•¨ê»˜ í‰ê°€\n",
        "  ë‘˜ì„ ì˜ ì¡°í™”ì‹œí‚¨ ê°’ì…ë‹ˆë‹¤.\n",
        "\n",
        "- **ê°œì²´ëª… ì¸ì‹ ë¬¸ì œì— ë”± ë§ì•„ìš”**  \n",
        "  ê°œì²´ëª…ì€ ì‹œì‘ê³¼ ë, ê·¸ë¦¬ê³  ë¼ë²¨ê¹Œì§€ ì •í™•íˆ ë§ì•„ì•¼ í•˜ë¯€ë¡œ,  \n",
        "  F1 Scoreê°€ ì„±ëŠ¥ì„ ì •í™•íˆ ë³´ì—¬ì¤Œ\n",
        "\n",
        "- **ê·¸ë˜ì„œ ë§¤ ì—í­ë§ˆë‹¤ F1 Scoreë¥¼ ê³„ì‚°í•´ìš”**  \n",
        "  ëª¨ë¸ì´ ì ì  ë” ì˜ í•™ìŠµë˜ëŠ”ì§€ íš¨ê³¼ì ìœ¼ë¡œ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ\n"
      ],
      "metadata": {
        "id": "5Bc1iXftOL4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ“Œ F1score ì½œë°± í´ë˜ìŠ¤ ì„¤ëª…\n",
        "\n",
        "ì´ ì½”ë“œëŠ” **Keras ëª¨ë¸ í•™ìŠµ ì¤‘ ë§¤ ì—í­ë§ˆë‹¤ F1 scoreì™€ ë¶„ë¥˜ ë¦¬í¬íŠ¸ë¥¼ ì¶œë ¥í•´ì£¼ëŠ” ì½œë°±** í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ì£¼ë¡œ **ê°œì²´ëª… ì¸ì‹(NER)** ê°™ì€ ì‹œí€€ìŠ¤ ë¼ë²¨ë§ ë¬¸ì œì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ”§ í´ë˜ìŠ¤ ì •ì˜\n",
        "\n",
        "```python\n",
        "class F1score(tf.keras.callbacks.Callback):\n",
        "```\n",
        "\n",
        "* `tf.keras.callbacks.Callback`ì„ ìƒì†ë°›ì•„ Keras í•™ìŠµ ê³¼ì •ì— ë¼ì–´ë“¤ ìˆ˜ ìˆìŒ\n",
        "* ë§¤ ì—í­ì´ ëë‚  ë•Œ ìë™ìœ¼ë¡œ í‰ê°€ ë¡œì§ ì‹¤í–‰\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ·ï¸ ìƒì„±ì (`__init__`)\n",
        "\n",
        "```python\n",
        "def __init__(self, X_test, y_test):\n",
        "    self.X_test = X_test\n",
        "    self.y_test = y_test\n",
        "```\n",
        "\n",
        "* í…ŒìŠ¤íŠ¸ìš© ì…ë ¥ (`X_test`)ê³¼ ì •ë‹µ ë¼ë²¨ (`y_test`)ì„ ë°›ì•„ ì €ì¥\n",
        "* ì´í›„ ì—í­ë§ˆë‹¤ ëª¨ë¸ ì„±ëŠ¥ì„ ì´ ë°ì´í„°ë¡œ í‰ê°€\n",
        "\n",
        "\n",
        "### ğŸ” ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íƒœê·¸ë¡œ ë³€í™˜ (`sequences_to_tags`)\n",
        "\n",
        "```python\n",
        "def sequences_to_tags(self, label_ids, pred_ids):\n",
        "```\n",
        "\n",
        "* ëª¨ë¸ì´ ì¶œë ¥í•œ ì˜ˆì¸¡ê°’ (`pred_ids`)ê³¼ ì‹¤ì œ ë¼ë²¨ê°’ (`label_ids`)ì„ í…ìŠ¤íŠ¸ íƒœê·¸(ì˜ˆ: `B-PER`, `O`, `I-LOC` ë“±)ë¡œ ë³€í™˜\n",
        "\n",
        "#### ì£¼ìš” ì²˜ë¦¬:\n",
        "\n",
        "* `-100`ì¸ ë¼ë²¨ì€ ë¬´ì‹œí•¨ (ë³´í†µ íŒ¨ë”© í† í°ìš©)\n",
        "* `index_to_tag`ë¥¼ ì‚¬ìš©í•´ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íƒœê·¸ë¡œ ë³€í™˜\n",
        "\n",
        "**ë°˜í™˜ê°’**:\n",
        "\n",
        "* `label_list`: ì‹¤ì œ ì •ë‹µ íƒœê·¸ë“¤\n",
        "* `pred_list`: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ íƒœê·¸ë“¤\n",
        "\n",
        "\n",
        "### âœ… ì—í­ ì¢…ë£Œ ì‹œ ì‹¤í–‰ (`on_epoch_end`)\n",
        "\n",
        "```python\n",
        "def on_epoch_end(self, epoch, logs={}):\n",
        "```\n",
        "\n",
        "* ê° ì—í­ì´ ëë‚  ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œë¨\n",
        "\n",
        "#### ìˆ˜í–‰ ìˆœì„œ:\n",
        "\n",
        "1. **ì˜ˆì¸¡ ìˆ˜í–‰**\n",
        "\n",
        "   ```python\n",
        "   y_predicted = self.model.predict(self.X_test)\n",
        "   y_predicted = np.argmax(y_predicted, axis=2)\n",
        "   ```\n",
        "\n",
        "   * ëª¨ë¸ì´ ì¶œë ¥í•œ í™•ë¥  ë¶„í¬ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ëŠ” í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ ì„ íƒ\n",
        "\n",
        "2. **ì •ë‹µ ë° ì˜ˆì¸¡ íƒœê·¸ë¡œ ë³€í™˜**\n",
        "\n",
        "   ```python\n",
        "   label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
        "   ```\n",
        "\n",
        "3. **F1 ì ìˆ˜ ê³„ì‚° ë° ì¶œë ¥**\n",
        "\n",
        "   ```python\n",
        "   score = f1_score(label_list, pred_list, suffix=True)\n",
        "   print(' - f1: {:04.2f}'.format(score * 100))\n",
        "   ```\n",
        "\n",
        "4. **ìì„¸í•œ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥**\n",
        "\n",
        "   ```python\n",
        "   print(classification_report(label_list, pred_list, suffix=True))\n",
        "   ```\n",
        "\n",
        "\n",
        "### ğŸ§© ì‚¬ìš© ì „ í™•ì¸í•´ì•¼ í•  ê²ƒ\n",
        "\n",
        "* `index_to_tag`: ì¸ë±ìŠ¤ë¥¼ íƒœê·¸ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ (ì „ì—­ ë³€ìˆ˜ë¡œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨)\n",
        "* `f1_score`, `classification_report`: `seqeval` íŒ¨í‚¤ì§€ì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜ì—¬ì•¼ í•¨\n",
        "\n",
        "```python\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "```\n",
        "\n",
        "\n",
        "ì´ ì½œë°±ì€ íŠ¹íˆ **NER ëª¨ë¸ í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ í‰ê°€ë¥¼ ìœ„í•´ ë§¤ìš° ìœ ìš©**í•˜ë©°, `model.fit()` ì‹œ `callbacks=[F1score(...)]` í˜•íƒœë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "qdn6yP7wUCe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "voooB8jyTqgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class F1score(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X_test, y_test):\n",
        "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°(ì…ë ¥, ì •ë‹µ)ë¥¼ ì½œë°± ê°ì²´ì— ì €ì¥\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def sequences_to_tags(self, label_ids, pred_ids):\n",
        "        # ëª¨ë¸ ì¶œë ¥ ë° ì‹¤ì œ ë ˆì´ë¸”ì„ í…ìŠ¤íŠ¸ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "        label_list = []  # ì‹¤ì œ ë ˆì´ë¸” íƒœê·¸ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
        "        pred_list = []   # ì˜ˆì¸¡ëœ ë ˆì´ë¸” íƒœê·¸ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "        # ë°°ì¹˜(ë¬¸ì¥) ë‹¨ìœ„ë¡œ ë°˜ë³µ\n",
        "        for i in range(0, len(label_ids)):\n",
        "            label_tag = []  # í˜„ì¬ ë¬¸ì¥ ì‹¤ì œ íƒœê·¸ ë¦¬ìŠ¤íŠ¸\n",
        "            pred_tag = []   # í˜„ì¬ ë¬¸ì¥ ì˜ˆì¸¡ íƒœê·¸ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "            # ê° ë¬¸ì¥ ë‚´ í† í°ë³„ë¡œ ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ë¹„êµ\n",
        "            # -100ì€ íŒ¨ë”© í† í°ì— í•´ë‹¹, í‰ê°€ì—ì„œ ì œì™¸í•´ì•¼ í•¨\n",
        "            for label_index, pred_index in zip(label_ids[i], pred_ids[i]):\n",
        "                if label_index != -100:  # íŒ¨ë”©ì´ ì•„ë‹Œ ì‹¤ì œ í† í°ë§Œ ì²˜ë¦¬\n",
        "                    # ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ íƒœê·¸ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "                    label_tag.append(index_to_tag[label_index])\n",
        "                    pred_tag.append(index_to_tag[pred_index])\n",
        "\n",
        "            # ë³€í™˜ëœ ë¬¸ì¥ ë‹¨ìœ„ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
        "            label_list.append(label_tag)\n",
        "            pred_list.append(pred_tag)\n",
        "\n",
        "        # ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
        "        return label_list, pred_list\n",
        "\n",
        "    # ë§¤ í•™ìŠµ ì—í­ì´ ëë‚  ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # í…ŒìŠ¤íŠ¸ ì…ë ¥(X_test)ì— ëŒ€í•´ í˜„ì¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "        y_predicted = self.model.predict(self.X_test)\n",
        "        # ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì„ íƒ (argmax)\n",
        "        # axis=2ëŠ” ê° í† í°ë³„ í´ë˜ìŠ¤ ì°¨ì›ì—ì„œ ì„ íƒí•œë‹¤ëŠ” ì˜ë¯¸ (batch, seq_len, num_classes)\n",
        "        # argmax\tìµœëŒ“ê°’ì˜ ìœ„ì¹˜/ì¸ë±ìŠ¤\n",
        "        y_predicted = np.argmax(y_predicted, axis=2)\n",
        "\n",
        "        # ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ í…ìŠ¤íŠ¸ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "        label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
        "\n",
        "        # F1 ì ìˆ˜ ê³„ì‚° (ê°œì²´ëª… ì¸ì‹ ì„±ëŠ¥ ì§€í‘œ)\n",
        "        # f1_score í•¨ìˆ˜:\n",
        "        #   - ì •ë°€ë„(Precision): ì˜ˆì¸¡í•œ ê°œì²´ ì¤‘ ì‹¤ì œë¡œ ë§ëŠ” ë¹„ìœ¨\n",
        "        #   - ì¬í˜„ìœ¨(Recall): ì‹¤ì œ ê°œì²´ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì°¾ì•„ë‚¸ ë¹„ìœ¨\n",
        "        #   - F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
        "        #\n",
        "        # suffix=True íŒŒë¼ë¯¸í„°:\n",
        "        #   - BIO íƒœê¹… ë°©ì‹ì—ì„œ ì ‘ë¯¸ì‚¬(-PER, -LOC ë“±)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°œì²´ ë‹¨ìœ„ í‰ê°€\n",
        "        #   - ì˜ˆ: ['B-PER', 'I-PER'] â†’ í•˜ë‚˜ì˜ PER ê°œì²´ë¡œ ì·¨ê¸‰\n",
        "        #   - ê°œì²´ì˜ ì‹œì‘ê³¼ ë ìœ„ì¹˜ê°€ ì™„ì „íˆ ì¼ì¹˜í•´ì•¼ë§Œ ì •ë‹µìœ¼ë¡œ ì¸ì • (ì—„ê²©í•œ í‰ê°€)\n",
        "        #   - ë¶€ë¶„ ì¼ì¹˜ëŠ” 0ì  ì²˜ë¦¬ (ì˜ˆ: \"í™ê¸¸ë™\" ì¤‘ \"í™ê¸¸\"ë§Œ ì˜ˆì¸¡í•˜ë©´ ì˜¤ë‹µ)\n",
        "        score = f1_score(label_list, pred_list, suffix=True)\n",
        "        # ê³„ì‚°ëœ F1 ì ìˆ˜ ì¶œë ¥ (ë°±ë¶„ìœ¨ë¡œ ë³´ê¸° ì¢‹ê²Œ ë³€í™˜)\n",
        "        print(' - f1: {:04.2f}'.format(score * 100))\n",
        "        # ê° í´ë˜ìŠ¤ë³„ ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ ë“± ìì„¸í•œ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
        "        print(classification_report(label_list, pred_list, suffix=True))\n"
      ],
      "metadata": {
        "id": "ZjaunWmnJEvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score_report = F1score(X_test,y_test)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train, epochs=3, batch_size=32,\n",
        "    callbacks=[f1_score_report]\n",
        ")"
      ],
      "metadata": {
        "id": "S7P4GCmvQRkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ğŸ“Œ `convert_examples_to_features_for_prediction` í•¨ìˆ˜ ì„¤ëª…\n",
        "\n",
        "ì´ í•¨ìˆ˜ëŠ” **ì˜ˆì¸¡ìš© ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ BERT ê¸°ë°˜ ì…ë ¥ í¬ë§·**ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "NER(Named Entity Recognition) ë˜ëŠ” ì‹œí€€ìŠ¤ íƒœê¹… ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ§© í•¨ìˆ˜ ì •ì˜\n",
        "\n",
        "```python\n",
        "def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer,\n",
        "                                                pad_token_id_for_segment=0,\n",
        "                                                pad_token_id_for_label=-100):\n",
        "```\n",
        "\n",
        "#### íŒŒë¼ë¯¸í„° ì„¤ëª…\n",
        "\n",
        "| íŒŒë¼ë¯¸í„°                       | ì„¤ëª…                                                              |\n",
        "| -------------------------- | --------------------------------------------------------------- |\n",
        "| `examples`                 | ë¬¸ì¥ ë‹¨ìœ„ì˜ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: `[['Hello', 'world'], ['My', 'name', 'is']]`) |\n",
        "| `max_seq_len`              | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì˜ˆ: 128)                                              |\n",
        "| `tokenizer`                | HuggingFaceì˜ tokenizer ê°ì²´                                       |\n",
        "| `pad_token_id_for_segment` | ì„¸ê·¸ë¨¼íŠ¸ IDìš© íŒ¨ë”©ê°’ (ë³´í†µ 0)                                             |\n",
        "| `pad_token_id_for_label`   | ë¼ë²¨ìš© íŒ¨ë”©ê°’ (ë³´í†µ -100ë¡œ ë§ˆìŠ¤í‚¹)                                          |\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ”„ ì „ì²´ ì²˜ë¦¬ íë¦„ ìš”ì•½\n",
        "\n",
        "1. ê° ë¬¸ì¥ì„ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§•\n",
        "2. ì„œë¸Œì›Œë“œ ì²˜ë¦¬ ì‹œ **ì²« ì„œë¸Œì›Œë“œë§Œ ë¼ë²¨ì„ ì˜ˆì¸¡**í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ë¬´ì‹œ(`-100`)\n",
        "3. `[CLS]`ì™€ `[SEP]` í† í° ì¶”ê°€ (ë ˆì´ë¸”ë„ ë§ì¶°ì„œ `-100` ì¶”ê°€)\n",
        "4. ì •ìˆ˜ ì¸ì½”ë”© + attention mask + segment ID ìƒì„±\n",
        "5. í•„ìš”í•œ ê²½ìš° ê¸¸ì´ë¥¼ `max_seq_len`ìœ¼ë¡œ íŒ¨ë”© ì²˜ë¦¬\n",
        "6. ëª¨ë‘ `numpy array`ë¡œ ë°˜í™˜\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ§± ë‚´ë¶€ ì£¼ìš” ë‹¨ê³„ ì„¤ëª…\n",
        "\n",
        "#### 1. ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì§• & ë¼ë²¨ ë§ˆìŠ¤í‚¹\n",
        "\n",
        "```python\n",
        "subword_tokens = tokenizer.tokenize(one_word)\n",
        "tokens.extend(subword_tokens)\n",
        "label_mask.extend([0] + [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
        "```\n",
        "\n",
        "* ì˜ˆ: `playing â†’ ['play', '##ing']` ì´ë¼ë©´\n",
        "  `label_mask = [0, -100]`\n",
        "  (ì²« ì„œë¸Œì›Œë“œë§Œ ë ˆì´ë¸” í‰ê°€ ëŒ€ìƒ)\n",
        "\n",
        "\n",
        "#### 2. `[CLS]`ì™€ `[SEP]` í† í° ì¶”ê°€\n",
        "\n",
        "```python\n",
        "tokens = [cls_token] + tokens + [sep_token]\n",
        "label_mask = [pad_token_id_for_label] + label_mask + [pad_token_id_for_label]\n",
        "```\n",
        "\n",
        "* ì…ë ¥ ì•ë’¤ì— íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
        "* ë¼ë²¨ ë§ˆìŠ¤í¬ì—ë„ ë§ì¶° `-100` ì‚½ì…\n",
        "\n",
        "\n",
        "#### 3. ì •ìˆ˜ ì¸ì½”ë”© ë° íŒ¨ë”©\n",
        "\n",
        "```python\n",
        "input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "padding_count = max_seq_len - len(input_id)\n",
        "\n",
        "input_id += [pad_token_id] * padding_count\n",
        "attention_mask = [1] * len(tokens) + [0] * padding_count\n",
        "token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
        "label_mask += [pad_token_id_for_label] * padding_count\n",
        "```\n",
        "\n",
        "* í† í°ì„ ì •ìˆ˜ë¡œ ë³€í™˜\n",
        "* ë¶€ì¡±í•œ ê¸¸ì´ë§Œí¼ íŒ¨ë”© ì¶”ê°€\n",
        "* attention maskëŠ” ì‹¤ì œ í† í°ì—ëŠ” 1, íŒ¨ë”©ì—ëŠ” 0\n",
        "\n",
        "\n",
        "\n",
        "#### 4. ê¸¸ì´ ê²€ì¦\n",
        "\n",
        "```python\n",
        "assert len(input_id) == max_seq_len\n",
        "assert len(attention_mask) == max_seq_len\n",
        "...\n",
        "```\n",
        "\n",
        "* ì „ì²˜ë¦¬ê°€ ì •í™•íˆ ì‘ë™í–ˆëŠ”ì§€ í™•ì¸\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ“¤ ë°˜í™˜ê°’\n",
        "\n",
        "```python\n",
        "return (input_ids, attention_masks, token_type_ids), label_masks\n",
        "```\n",
        "\n",
        "* `input_ids`: BERT ì…ë ¥ìš© ì •ìˆ˜ í† í° ë°°ì—´\n",
        "* `attention_masks`: 1ì€ ì‹¤ì œ í† í°, 0ì€ íŒ¨ë”©\n",
        "* `token_type_ids`: ë¬¸ì¥ êµ¬ë¶„ìš© ì„¸ê·¸ë¨¼íŠ¸ (ë³´í†µ NERì—ì„œëŠ” ëª¨ë‘ 0)\n",
        "* `label_masks`: ì‹¤ì œ í‰ê°€í•  í† í°ì€ 0, ë‚˜ë¨¸ì§€ëŠ” `-100` (loss ê³„ì‚° ì œì™¸ìš©)\n",
        "\n",
        "\n",
        "\n",
        "### âœ… ìµœì¢… ì‚¬ìš© ì˜ˆì‹œ\n",
        "\n",
        "```python\n",
        "X = [['ì˜¤ëŠ˜', 'ë‚ ì”¨', 'ì¢‹ë‹¤'], ['ë‚˜ëŠ”', 'í•™ìƒì…ë‹ˆë‹¤']]\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "inputs, label_masks = convert_examples_to_features_for_prediction(X, 64, tokenizer)\n",
        "```\n",
        "\n",
        "* ì˜ˆì¸¡ ë˜ëŠ” ì¶”ë¡ ìš© BERT ëª¨ë¸ ì…ë ¥ ìƒì„±ì— ìœ ìš©\n",
        "* ë¼ë²¨ì€ ì—†ì§€ë§Œ ë¼ë²¨ ë§ˆìŠ¤í¬ëŠ” í‰ê°€ìš© í•„í„°ë§ì„ ìœ„í•´ ìœ ì§€ë¨\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l2XuRGjyU7sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer,\n",
        "                                             pad_token_id_for_segment=0,\n",
        "                                             pad_token_id_for_label=-100):\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, label_masks = [], [], [], []\n",
        "\n",
        "    for example in tqdm(examples):\n",
        "        tokens = []\n",
        "        label_mask = []\n",
        "        for one_word in example:\n",
        "            # í•˜ë‚˜ì˜ ë‹¨ì–´ì— ëŒ€í•´ì„œ ì„œë¸Œì›Œë“œë¡œ í† í°í™”\n",
        "            subword_tokens = tokenizer.tokenize(one_word)\n",
        "            tokens.extend(subword_tokens)\n",
        "            # ì„œë¸Œì›Œë“œë¡œ ìª¼ê°œì§„ ì²«ë²ˆì§¸ ì„œë¸Œì›Œë“œë¥¼ ì œì™¸í•˜ê³  ê·¸ ë’¤ì˜ ì„œë¸Œì›Œë“œë“¤ì€ -100ìœ¼ë¡œ ë§ˆ\n",
        "            # ìŠ¤í‚¹í•¨\n",
        "            label_mask.extend([0]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
        "\n",
        "        # [CLS]ì™€ [SEP]ë¥¼ í›„ì— ì¶”ê°€í•  ê²ƒì„ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œì˜ ê²½\n",
        "        # ìš° max_seq_len - 2ì˜ ê¸¸ì´ë¡œ ë³€í™˜.\n",
        "        # ex) max_seq_len = 64ë¼ë©´ ê¸¸ì´ê°€ 62ë³´ë‹¤ ê¸´ ìƒ˜í”Œì€ ë’· ë¶€ë¶„ì„ ìë¥´ê³  ê¸¸ì´\n",
        "        # 62ë¡œ ë³€í™˜.\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
        "            label_mask = label_mask[:(max_seq_len - special_tokens_count)]\n",
        "\n",
        "        # [SEP]ë¥¼ ì¶”ê°€í•˜ëŠ” ì½”ë“œ\n",
        "        # 1. í† ë¥´í™” ê²°ê³¼ì˜ ë§¨ ë’· ë¶€ë¶„ì— [SEP] í† í° ì¶”ê°€\n",
        "        # 2. ë ˆì´ë¸”ì—ë„ ë§¨ ë’· ë¶€ë¶„ì— -100 ì¶”ê°€.\n",
        "        tokens += [sep_token]\n",
        "        label_mask += [pad_token_id_for_label]\n",
        "\n",
        "        # [CLS]ë¥¼ ì¶”ê°€í•˜ëŠ” ì½”ë“œ\n",
        "        # 1. í† í°í™” ê²°ê³¼ì˜ ì• ë¶€ë¶„ì— [CLS] í† í° ì¶”ê°€\n",
        "        # 2. ë ˆì´ë¸”ì˜ ë§¨ ì• ë¶€ë¶„ì—ë„ -100 ì¶”ê°€.\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_mask = [pad_token_id_for_label] + label_mask\n",
        "\n",
        "        # ì •ìˆ˜ ì¸ì½”ë”©\n",
        "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        attention_mask = [1] * len(input_id)\n",
        "\n",
        "        # ì •ìˆ˜ ì¸ì½”ë”©ì— ì¶”ê°€í•  íŒ¨ë”© ê¸¸ì´ ì—°ì‚°\n",
        "        padding_count = max_seq_len - len(input_id)\n",
        "\n",
        "        # ì •ìˆ˜ ì¸ì½”ë”©, ì–´í…ì…˜ ë§ˆìŠ¤í¬ì— íŒ¨ë”© ì¶”ê°€\n",
        "        input_id = input_id + ([pad_token_id] * padding_count)\n",
        "        attention_mask = attention_mask + ([0] * padding_count)\n",
        "\n",
        "        # ì„¸ê·¸ë¨¼íŠ¸ ì¸ì½”ë”©.\n",
        "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
        "\n",
        "        # ë ˆì´ë¸” íŒ¨ë”©. (ë‹¨, ì´ ê²½ìš°ëŠ” íŒ¨ë”© í† í°ì˜ IDê°€ -100)\n",
        "        label_mask = label_mask + ([pad_token_id_for_label] * padding_count)\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "        assert len(label_mask) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label_mask), max_seq_len)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        label_masks.append(label_mask)\n",
        "\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "    label_masks = np.array(label_masks, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), label_masks"
      ],
      "metadata": {
        "id": "K_SuAoYVRDRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pred, label_masks = convert_examples_to_features_for_prediction(\n",
        "    test_data_sentence[:5], max_seq_len=128, tokenizer=tokenizer)\n",
        "\n",
        "print('ê¸°ì¡´ ì›ë¬¸ :', test_data_sentence[0])\n",
        "print('-' * 50)\n",
        "print('í† í°í™” í›„ ì›ë¬¸ :', [tokenizer.decode([word]) for word in X_pred[0][0]])\n",
        "print('ë ˆì´ë¸” ë§ˆìŠ¤í¬ :', ['[PAD]' if idx == -100 else '[FIRST]' for idx in label_masks[0]])"
      ],
      "metadata": {
        "id": "GHCj1NZgVVgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ“Œ `ner_prediction` í•¨ìˆ˜ ì„¤ëª…\n",
        "\n",
        "ì´ í•¨ìˆ˜ëŠ” ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ **NER(Named Entity Recognition)** ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "ì˜ˆì¸¡ ê²°ê³¼ëŠ” ê° ë‹¨ì–´ì— ëŒ€í•´ (ë‹¨ì–´, ì˜ˆì¸¡ëœ ê°œì²´ëª… íƒœê·¸) í˜•íƒœë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ§© í•¨ìˆ˜ ì •ì˜\n",
        "\n",
        "```python\n",
        "def ner_prediction(examples, max_seq_len, tokenizer):\n",
        "```\n",
        "\n",
        "#### íŒŒë¼ë¯¸í„° ì„¤ëª…\n",
        "\n",
        "| íŒŒë¼ë¯¸í„°          | ì„¤ëª…                                          |\n",
        "| ------------- | ------------------------------------------- |\n",
        "| `examples`    | ë¬¸ìì—´ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: `[\"ë‚˜ëŠ” í•™ìƒì…ë‹ˆë‹¤\", \"ì´ìˆœì‹ ì€ ì¥êµ°ì´ë‹¤\"]`) |\n",
        "| `max_seq_len` | ëª¨ë¸ ì…ë ¥ì˜ ìµœëŒ€ ê¸¸ì´ (ì˜ˆ: 128)                       |\n",
        "| `tokenizer`   | HuggingFaceì˜ BERT tokenizer ê°ì²´              |\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ”„ ì „ì²´ ì²˜ë¦¬ íë¦„\n",
        "\n",
        "1. ë¬¸ì¥ë“¤ì„ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "2. ì˜ˆì¸¡ìš© ì…ë ¥ í¬ë§·ìœ¼ë¡œ ë³€í™˜ (`convert_examples_to_features_for_prediction` í˜¸ì¶œ)\n",
        "3. ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "4. ì˜ˆì¸¡ëœ ë¼ë²¨ ì¸ë±ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íƒœê·¸ë¡œ ë³€í™˜\n",
        "5. (ë‹¨ì–´, ì˜ˆì¸¡ íƒœê·¸) í˜•íƒœë¡œ ê²°ê³¼ êµ¬ì„±\n",
        "\n",
        "\n",
        "\n",
        "### 1ï¸âƒ£ ë¬¸ì¥ ë¶„í•  (ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)\n",
        "\n",
        "```python\n",
        "examples = [sent.split() for sent in examples]\n",
        "```\n",
        "\n",
        "* ë¬¸ìì—´ ë¬¸ì¥ â†’ ë‹¨ì–´ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "* ì˜ˆ: `\"ë‚˜ëŠ” í•™ìƒì…ë‹ˆë‹¤\"` â†’ `['ë‚˜ëŠ”', 'í•™ìƒì…ë‹ˆë‹¤']`\n",
        "\n",
        "\n",
        "\n",
        "### 2ï¸âƒ£ ì…ë ¥ í¬ë§· ë³€í™˜\n",
        "\n",
        "```python\n",
        "X_pred, label_masks = convert_examples_to_features_for_prediction(...)\n",
        "```\n",
        "\n",
        "* ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "* ë°˜í™˜ê°’:\n",
        "\n",
        "  * `X_pred`: (input\\_ids, attention\\_masks, token\\_type\\_ids)\n",
        "  * `label_masks`: ì‹¤ì œ í‰ê°€ ëŒ€ìƒì¸ í† í°ë§Œ `0`, ë‚˜ë¨¸ì§€ëŠ” `-100`\n",
        "\n",
        "\n",
        "\n",
        "### 3ï¸âƒ£ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "\n",
        "```python\n",
        "y_predicted = model.predict(X_pred)\n",
        "y_predicted = np.argmax(y_predicted, axis=2)\n",
        "```\n",
        "\n",
        "* ëª¨ë¸ ì¶œë ¥: (ë°°ì¹˜, ì‹œí€€ìŠ¤ ê¸¸ì´, í´ë˜ìŠ¤ ìˆ˜)\n",
        "* ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°–ëŠ” í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì„ íƒ\n",
        "\n",
        "\n",
        "\n",
        "### 4ï¸âƒ£ ì˜ˆì¸¡ê°’ì„ íƒœê·¸ë¡œ ë””ì½”ë”©\n",
        "\n",
        "```python\n",
        "for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n",
        "    if label_index != -100:\n",
        "        pred_tag.append(index_to_tag[pred_index])\n",
        "```\n",
        "\n",
        "* `-100`ì¸ ìœ„ì¹˜ëŠ” ë¬´ì‹œ (ì˜ˆ: `[CLS]`, `[SEP]`, ì„œë¸Œì›Œë“œ ë“±)\n",
        "* ë‚˜ë¨¸ì§€ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ ì¸ë±ìŠ¤ë¥¼ íƒœê·¸ë¡œ ë³€í™˜ (ì˜ˆ: `3 â†’ B-PER`)\n",
        "\n",
        "\n",
        "\n",
        "### 5ï¸âƒ£ ë‹¨ì–´ì™€ ì˜ˆì¸¡ íƒœê·¸ë¥¼ ë¬¶ê¸°\n",
        "\n",
        "```python\n",
        "for example, pred in zip(examples, pred_list):\n",
        "    one_sample_result = []\n",
        "    for one_word, label_token in zip(example, pred):\n",
        "        one_sample_result.append((one_word, label_token))\n",
        "```\n",
        "\n",
        "* ì˜ˆ:\n",
        "  ì…ë ¥: `['ì´ìˆœì‹ ', 'ì¥êµ°']`\n",
        "  ì˜ˆì¸¡: `['B-PER', 'O']`\n",
        "  ê²°ê³¼: `[('ì´ìˆœì‹ ', 'B-PER'), ('ì¥êµ°', 'O')]`\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ“¤ ë°˜í™˜ê°’\n",
        "\n",
        "```python\n",
        "return result_list\n",
        "```\n",
        "\n",
        "* ê²°ê³¼ í˜•íƒœ: `[[('ë‹¨ì–´1', 'íƒœê·¸1'), ('ë‹¨ì–´2', 'íƒœê·¸2'), ...], [...], ...]`\n",
        "* ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‹¨ì–´ì™€ íƒœê·¸ê°€ ë¬¶ì—¬ ìˆìŒ\n",
        "\n",
        "\n",
        "### âœ… ì˜ˆì‹œ ì‚¬ìš©\n",
        "\n",
        "```python\n",
        "sentences = [\"ì´ìˆœì‹ ì€ ì¥êµ°ì´ë‹¤\", \"ì‚¼ì„±ì€ í•œêµ­ ê¸°ì—…ì´ë‹¤\"]\n",
        "results = ner_prediction(sentences, 128, tokenizer)\n",
        "\n",
        "# ì¶œë ¥ ì˜ˆì‹œ\n",
        "# [[('ì´ìˆœì‹ ì€', 'B-PER'), ('ì¥êµ°ì´ë‹¤', 'O')],\n",
        "#  [('ì‚¼ì„±ì€', 'B-ORG'), ('í•œêµ­', 'B-LOC'), ('ê¸°ì—…ì´ë‹¤', 'O')]]\n",
        "```\n",
        "\n",
        "\n",
        "### ğŸ”” ì£¼ì˜ì‚¬í•­\n",
        "\n",
        "* ì „ì—­ ë³€ìˆ˜ `model`, `index_to_tag` ê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.\n",
        "\n",
        "  ```python\n",
        "  # ì˜ˆì‹œ\n",
        "  model = tf.keras.models.load_model(...)\n",
        "  index_to_tag = {0: 'O', 1: 'B-PER', 2: 'I-PER', ...}\n",
        "  ```\n",
        "\n",
        "* ì…ë ¥ ë¬¸ì¥ì€ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‚˜ë‰˜ë¯€ë¡œ í˜•íƒœì†Œ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ì•ŠëŠ” í•œ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ\n"
      ],
      "metadata": {
        "id": "H0j3nCeEV3Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_prediction(examples, max_seq_len, tokenizer):\n",
        "    examples = [sent.split() for sent in examples]\n",
        "    X_pred, label_masks = convert_examples_to_features_for_prediction(\n",
        "        examples,\n",
        "        max_seq_len=128,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    y_predicted = model.predict(X_pred)\n",
        "    # ë§ˆì§€ë§‰ ì°¨ì› í´ë˜ìŠ¤ì°¨ì›ì—ì„œ ìµœëŒ€ê°’ì„ ì°¾ëŠ”ë‹¤\n",
        "    # ë§ˆì§€ë§‰ ì°¨ì› = í´ë˜ìŠ¤ ì°¨ì› -> ê° í´ë˜ìŠ¤ í™•ë¥ ì´ ë‚˜ì™€ìˆì„ê±°ë‹ˆ ì•„ë§ˆë„ softmax -> ê·¸ì¤‘ì— ìµœëŒ€ê°’ -> í•´ë‹¹ í´ë˜ìŠ¤ì¼ í™•ë¥  ë†’ìŒ\n",
        "    y_predicted = np.argmax(y_predicted, axis=2)\n",
        "\n",
        "    pred_list = []\n",
        "    result_list = []\n",
        "\n",
        "    for i in range(0, len(label_masks)):\n",
        "        pred_tag = []\n",
        "\n",
        "        # ex) ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ë””ì½”ë”© ê³¼ì •\n",
        "        # ì˜ˆì¸¡ê°’(y_predicted)ì—ì„œ ë ˆì´ë¸” ë§ˆìŠ¤í¬(label_masks)ì˜ ê°’ì´ -100ì¸ ë™ì¼ ìœ„ì¹˜ì˜ ê°’ì€ ì‚­ì œ\n",
        "        # label_masks : [-100 -100 0 0 -100]\n",
        "        # y_predicted : [ 0   2   0 2   0 ] ==> [1 2] ==> ìµœì¢… ì˜ˆì¸¡(pred_tag) : [PER-B PER-I]\n",
        "        for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n",
        "            if label_index != -100:\n",
        "                pred_tag.append(index_to_tag[pred_index])\n",
        "\n",
        "        pred_list.append(pred_tag)\n",
        "\n",
        "    for example, pred in zip(examples, pred_list):\n",
        "        one_sample_result = []\n",
        "        for one_word, label_token in zip(example, pred):\n",
        "            one_sample_result.append((one_word, label_token))\n",
        "        result_list.append(one_sample_result)\n",
        "\n",
        "    return result_list\n"
      ],
      "metadata": {
        "id": "MQKjA2mJVtO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ì°¨ì› ë³€í™” ê³¼ì •\n",
        "\n",
        "### 1. ì›ë³¸ `y_predicted`\n",
        "\n",
        "- ëª¨ë¸ ì¶œë ¥: `(batch_size, sequence_length, num_classes)`\n",
        "- ì˜ˆì‹œ: `y_predicted.shape = (2, 5, 9)`  \n",
        "  (ë°°ì¹˜ 2ê°œ, ì‹œí€€ìŠ¤ ê¸¸ì´ 5, í´ë˜ìŠ¤ ìˆ˜ 9)\n",
        "\n",
        "```python\n",
        "y_predicted = [\n",
        "    # ì²« ë²ˆì§¸ ë¬¸ì¥\n",
        "    [\n",
        "        [0.1, 0.0, 0.9, 0.0, ...],  # ì²« ë²ˆì§¸ í† í°ì˜ ê° í´ë˜ìŠ¤ í™•ë¥  -> 2\n",
        "        [0.8, 0.1, 0.1, 0.0, ...],  # ë‘ ë²ˆì§¸ í† í°ì˜ ê° í´ë˜ìŠ¤ í™•ë¥  -> 0\n",
        "        [0.0, 0.0, 0.0, 0.9, ...],  # ì„¸ ë²ˆì§¸ í† í°ì˜ ê° í´ë˜ìŠ¤ í™•ë¥  -> 3\n",
        "        ...\n",
        "    ],\n",
        "    # ë‘ ë²ˆì§¸ ë¬¸ì¥\n",
        "    [\n",
        "        ...\n",
        "    ]\n",
        "]\n",
        "````\n",
        "\n",
        "\n",
        "\n",
        "### 2. `argmax` ì ìš© í›„\n",
        "\n",
        "```python\n",
        "y_predicted = np.argmax(y_predicted, axis=2)\n",
        "```\n",
        "\n",
        "* ê²°ê³¼ ì°¨ì›: `(batch_size, sequence_length)` â†’ ì—¬ì „íˆ 2ì°¨ì›!\n",
        "* `y_predicted.shape = (2, 5)`\n",
        "\n",
        "```python\n",
        "y_predicted = [\n",
        "    [2, 0, 3, 1, 0],  # ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ê° í† í° ì˜ˆì¸¡ í´ë˜ìŠ¤\n",
        "    [1, 2, 0, 0, 4]   # ë‘ ë²ˆì§¸ ë¬¸ì¥ì˜ ê° í† í° ì˜ˆì¸¡ í´ë˜ìŠ¤\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "n4RbRrhgyI6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ˆì¸¡í•  ë¬¸ì¥\n",
        "sent1 = 'ì˜¤ë¦¬ì˜¨ìŠ¤ëŠ” ë¦¬ê·¸ ìµœì •ìƒê¸‰ í¬ì¸íŠ¸ê°€ë“œ ê¹€ë™í›ˆì„ ì•ì„¸ìš°ëŠ” ë¹ ë¥¸ ê³µìˆ˜ì „í™˜ì´ ë‹ë³´ì´ëŠ” íŒ€ì´ë‹¤'\n",
        "sent2 = 'í•˜ì´ì‹ ì‚¬ì— ì†í•œ ì„¬ë“¤ë„ ìœ„ë¡œ ì†Ÿì•„ ìˆëŠ”ë° íƒ€ì¸ì€ ì‚´ê³  ìˆì–´ìš”'\n",
        "\n",
        "# ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "test_samples = [sent1, sent2]\n",
        "\n",
        "# NER ì˜ˆì¸¡ í•¨ìˆ˜ í˜¸ì¶œ (128 í† í° ì œí•œ)\n",
        "result_list = ner_prediction(test_samples, max_seq_len=128, tokenizer=tokenizer)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "result_list"
      ],
      "metadata": {
        "id": "p0x_0JlDWSon"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}