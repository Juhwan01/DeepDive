{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGTfKBpUAoi+czmUOwep3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhwan01/DeepDive/blob/main/KLUE_BERT_%EA%B0%9C%EC%B2%B4%EB%AA%85_%EC%9D%B8%EC%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMSZv41rhrxo"
      },
      "outputs": [],
      "source": [
        "# 이전과 똑같이 레거시 모드로\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "LTiaIj-NlCUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIO 태그 체계 정리\n",
        "\n",
        "### ✅ BIO란?\n",
        "- B-XXX: 개체명 시작 (Begin)\n",
        "- I-XXX: 개체명 내부 (Inside)\n",
        "- O: 개체명 아님 (Outside)\n",
        "\n",
        "### 📌 예시 문장\n",
        "**문장:** 홍길동은 서울대학교를 졸업했다.\n",
        "\n",
        "| 단어       | 태그    | 설명                         |\n",
        "|------------|---------|------------------------------|\n",
        "| 홍길동     | B-PER   | 사람 이름 시작               |\n",
        "| 은         | O       | 개체명 아님                  |\n",
        "| 서울대학교 | B-ORG   | 기관 이름 시작               |\n",
        "| 를         | O       | 개체명 아님                  |\n",
        "| 졸업했다   | O       | 개체명 아님                  |\n",
        "\n",
        "### 🎯 여러 단어로 된 개체명 예시\n",
        "\n",
        "**문장:** 대한 민국 정부는\n",
        "\n",
        "| 단어   | 태그    | 설명               |\n",
        "|--------|---------|--------------------|\n",
        "| 대한   | B-ORG   | 기관 이름 시작     |\n",
        "| 민국   | I-ORG   | 기관 이름 내부     |\n",
        "| 정부   | I-ORG   | 기관 이름 내부     |\n",
        "\n",
        "### 🏷️ 자주 쓰이는 개체명 태그\n",
        "\n",
        "| 태그       | 의미           |\n",
        "|------------|----------------|\n",
        "| PER        | 사람 이름      |\n",
        "| LOC        | 위치/장소      |\n",
        "| ORG        | 기관/회사 이름 |\n",
        "| DATE       | 날짜           |\n",
        "| TIME       | 시간           |\n",
        "| MONEY      | 금액           |\n",
        "| PERCENT    | 백분율         |\n",
        "| O          | 개체명 아님    |\n",
        "\n",
        "### 🔄 다른 태그 체계 (참고용)\n",
        "- BIO: 가장 일반적인 태그 체계\n",
        "- BILOU: Begin, Inside, Last, Outside, Unit\n",
        "- BIOES: Begin, Inside, Outside, End, Single\n"
      ],
      "metadata": {
        "id": "7O8nlJJhZmrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import shape_list, BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# seqeval은 NER처럼 문장 내 단어마다 라벨이 붙는 시퀀스 라벨링 작업의 성능을 평가할 때 사용함\n",
        "# 일반적인 정확도와 다르게, 토큰 단위가 아니라 엔티티 단위 평가도 가능함\n",
        "# 예: 예측값과 실제값을 시퀀스 단위로 비교함\n",
        "#y_true = [[\"B-PER\", \"I-PER\", \"O\", \"B-LOC\"]]\n",
        "#y_pred = [[\"B-PER\", \"I-PER\", \"O\", \"B-ORG\"]]\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "import tensorflow as tf\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "mZi49vAsj9ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "# 학습 데이터 다운로드\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/refs/heads/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_train_data.csv\",\n",
        "    filename=\"ner_train_data.csv\"\n",
        ")\n",
        "\n",
        "# 테스트 데이터 다운로드\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/refs/heads/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_test_data.csv\",\n",
        "    filename=\"ner_test_data.csv\"\n",
        ")\n",
        "\n",
        "# 라벨 정보 다운로드\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/refs/heads/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_label.txt\",\n",
        "    filename=\"ner_label.txt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "7OQDlHzlkHu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV 파일 로드\n",
        "train_ner_df = pd.read_csv(\"ner_train_data.csv\")[:30000]\n",
        "test_ner_df = pd.read_csv(\"ner_test_data.csv\")"
      ],
      "metadata": {
        "id": "7FC8FFnMmbwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ner_df.head()"
      ],
      "metadata": {
        "id": "pEPAxf5nmc4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ner_df.head()"
      ],
      "metadata": {
        "id": "1X14U2N-oT2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 샘플 개수 :', len(train_ner_df))\n",
        "print('테스트용 샘플 개수 :', len(test_ner_df))"
      ],
      "metadata": {
        "id": "XjJcPaXHLOgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터의 'Sentence' 열에서 각 문장을 공백 기준으로 토큰화하여 리스트로 저장\n",
        "# 예: \"이순신 은 위인 이다\" → ['이순신', '은', '위인', '이다']\n",
        "train_data_sentence = [sent.split() for sent in train_ner_df['Sentence'].values]\n",
        "\n",
        "# 학습 데이터의 'Tag' 열에서 각 문장에 대한 개체명 태그를 공백 기준으로 분할하여 리스트로 저장\n",
        "# 예: \"PER-B O O O\" → ['PER-B', 'O', 'O', 'O']\n",
        "train_data_label = [label.split() for label in train_ner_df['Tag'].values]\n",
        "\n",
        "# 테스트 데이터의 'Sentence' 열에서 각 문장을 공백 기준으로 토큰화하여 리스트로 저장\n",
        "test_data_sentence = [sent.split() for sent in test_ner_df['Sentence'].values]\n",
        "\n",
        "# 테스트 데이터의 'Tag' 열에서 각 문장에 대한 개체명 태그를 공백 기준으로 분할하여 리스트로 저장\n",
        "test_data_label = [label.split() for label in test_ner_df['Tag'].values]\n"
      ],
      "metadata": {
        "id": "PcHxUKkFLbfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data_sentence[2])\n",
        "print(train_data_label[2])"
      ],
      "metadata": {
        "id": "LoMmlca4OQSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 데이터는 형태소 단위 x -> 어절(띄어쓰기) 단위로 개체명 인식\n",
        "# \"ner_label.txt\" 파일을 읽어서 각 라벨을 리스트로 저장\n",
        "# open() 함수로 파일을 읽기 모드('r')와 UTF-8 인코딩을 지정하여 연다\n",
        "# 파일의 각 줄(line)을 읽어와 strip() 함수로 앞뒤 공백이나 줄바꿈 문자를 제거\n",
        "# 이렇게 정제된 라벨들을 리스트에 담아 labels 변수에 저장\n",
        "labels = [label.strip() for label in open(\"ner_label.txt\", 'r', encoding='utf-8')]\n",
        "print('개체명 태깅 정보 :', labels)\n"
      ],
      "metadata": {
        "id": "kzX3t27QOZUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels 리스트에 있는 개체명 태깅(tag)들을 인덱스와 매핑하는 딕셔너리 생성\n",
        "# enumerate()는 index 같이 리턴\n",
        "\n",
        "# tag_to_index: 태깅 이름(tag)을 키(key)로, 해당 태깅의 인덱스(index)를 값(value)으로 저장하는 딕셔너리\n",
        "# 예를 들어, 'B-PER':0, 'I-PER':1 같은 형태로 태깅 이름을 숫자로 변환하는 용도\n",
        "tag_to_index = {tag: index for index, tag in enumerate(labels)}\n",
        "print(tag_to_index)\n",
        "\n",
        "# index_to_tag: 인덱스를 키(key)로, 태깅 이름(tag)을 값(value)으로 저장하는 딕셔너리\n",
        "# 숫자로 된 인덱스를 다시 원래 태깅 이름으로 변환할 때 사용합니다.\n",
        "index_to_tag = {index: tag for index, tag in enumerate(labels)}\n",
        "print(index_to_tag)\n",
        "\n",
        "# 이 과정을 하는 이유:\n",
        "# 모델 학습 시에는 태깅(label)을 숫자(index) 형태로 변환해야 컴퓨터가 이해하고 처리할 수 있음\n",
        "# 반대로, 모델의 예측 결과인 숫자 인덱스를 다시 사람이 이해할 수 있는 태깅 이름으로 바꾸기 위해 index_to_tag가 필요함\n"
      ],
      "metadata": {
        "id": "DYmPy1V_Q7Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_size = len(tag_to_index)\n",
        "print('개체명 태깅 정보의 개수 :', tag_size)"
      ],
      "metadata": {
        "id": "DZi79hDBS3L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "KnlTca_eaJuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = train_data_sentence[1]\n",
        "label = train_data_label[1]\n",
        "\n",
        "print('문장 :',sent)\n",
        "print('레이블 :',label)\n",
        "# 매핑 딕셔너리에서 찾아서 정수 인코딩\n",
        "print('레이블의 정수 인코딩 :', [tag_to_index[idx] for idx in label])\n",
        "print('문장의 길이 :', len(sent))\n",
        "print('레이블의 길이 :', len(label))"
      ],
      "metadata": {
        "id": "WpORZANsbuX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "# 예를 들어 '쿠마리'라는 단어가 BERT 토크나이저에 의해 ['쿠', '##마리']로 분리된 경우,\n",
        "# 원래 단어 '쿠마리'에는 'B-PER'이라는 개체명 태그가 붙어 있었음.\n",
        "\n",
        "# 문제는 이렇게 단어가 서브워드로 나뉘면, 레이블의 개수와 토큰의 개수가 일치하지 않게 되므로\n",
        "# 모델에 넣기 위해서는 서브워드 수에 맞춰 레이블도 확장해야 함.\n",
        "\n",
        "# 이때 일반적으로 사용하는 방식:\n",
        "# - 첫 번째 서브워드(예: '쿠')에는 원래 레이블(B-PER 등)을 그대로 부여\n",
        "# - 그 뒤에 따라오는 서브워드(예: '##마리')에는 레이블을 부여하지 않음 (== 무시됨)\n",
        "\n",
        "# 이렇게 레이블을 부여하지 않는 위치에는 보통 '-100'을 넣음.\n",
        "# - 이 값은 PyTorch나 TensorFlow 등에서 손실 계산 시 해당 위치를 무시하도록 처리됨.\n",
        "for one_word in sent:\n",
        "  # 각 단어 서브워드로 분리\n",
        "  subword_tokens = tokenizer.tokenize(one_word)\n",
        "  # extend()는 리스트 안의 요소들을 하나씩 추가해준다 <-> append()는 리스트 안에 리스트로 들어감\n",
        "  tokens.extend(subword_tokens)\n",
        "\n",
        "print('BERT 토크나이저 정룰 출력 :', tokens)\n",
        "print('레이블 :', label)\n",
        "print('레이블의 정수 인코딩 :', [tag_to_index[idx] for idx in label])\n",
        "print('토큰의 길이 :', len(tokens))\n",
        "print('레이블의 길이 :', len(label))"
      ],
      "metadata": {
        "id": "SHjt97OTdOdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "labels_ids = []\n",
        "\n",
        "for one_word, label_token in zip(train_data_sentence[1], train_data_label[1]):\n",
        "  subword_tokens = tokenizer.tokenize(one_word)\n",
        "  tokens.extend(subword_tokens)\n",
        "  # 라벨 ID 리스트에 추가\n",
        "  labels_ids.extend(\n",
        "      [tag_to_index[label_token]]          # => [1] : 첫 번째 subword에는 원래 라벨 그대로 사용\n",
        "      + [-100] * (len(subword_tokens) - 1) # => [-100, -100] : 나머지 subword는 무시(-100) 처리\n",
        "  )\n",
        "print('토큰화 된 문장 :', tokens)\n",
        "print('레이블 :', ['[PAD]' if idx == -100 else index_to_tag[idx] for idx in labels_ids])\n",
        "print('패딩에 대한 정수 인코딩 :', labels_ids)\n",
        "print('토큰의 길이 :', len(tokens))\n",
        "print('레이블의 길이 :', len(labels_ids))"
      ],
      "metadata": {
        "id": "dlHHmoKPe1tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 🧠 `convert_examples_to_features` 함수 전체 흐름 정리\n",
        "\n",
        "이 함수는 NER 데이터셋(예: 단어/라벨 쌍 리스트)을 BERT 입력 형식에 맞게 **정제하고 변환하는 전처리 함수**입니다. 최종적으로는 BERT가 요구하는 4가지 입력을 생성합니다:\n",
        "\n",
        "* `input_ids`\n",
        "* `attention_mask`\n",
        "* `token_type_ids`\n",
        "* `data_labels` (NER용 라벨 시퀀스)\n",
        "\n",
        "## 1️⃣ 특수 토큰 정보 가져오기\n",
        "\n",
        "* BERT 모델은 문장 앞에 `[CLS]`, 끝에 `[SEP]` 같은 **특수 토큰**이 필요합니다.\n",
        "* Hugging Face 토크나이저는 이런 특수 토큰들을 자동으로 제공합니다.\n",
        "* 예를 들어:\n",
        "\n",
        "  * `[CLS]`: 문장의 시작\n",
        "  * `[SEP]`: 문장의 끝\n",
        "  * `[PAD]`: 패딩 자리 채우기용\n",
        "* 이 토큰들은 모델의 동작에 매우 중요하며, 시퀀스 분류나 문장쌍 관계 파악에도 사용됩니다.\n",
        "\n",
        "## 2️⃣ 각 문장-라벨 쌍에 대해 반복\n",
        "\n",
        "NER 데이터는 보통 다음과 같은 구조입니다:\n",
        "\n",
        "```python\n",
        "examples = [['나는', '학생이다'], ['너는', '의사다']]\n",
        "labels = [['O', 'B-PER'], ['O', 'B-PROF']]\n",
        "```\n",
        "\n",
        "* 각 단어를 BERT 토크나이저로 **서브워드 수준까지 분해**합니다.\n",
        "* 예: `'학생이다' → ['학생', '##이다']`\n",
        "* 문제: 서브워드로 나뉘면 원래 단어의 라벨을 **복수의 토큰에 분배**해야 함\n",
        "\n",
        "### 👉 라벨 처리 방법\n",
        "\n",
        "* 첫 서브워드에는 **정상 라벨 부여**\n",
        "* 나머지 서브워드에는 \\*\\*패딩용 라벨값 (-100)\\*\\*을 부여 (손실 계산에서 무시됨)\n",
        "\n",
        "## 3️⃣ 너무 긴 문장은 잘라냄\n",
        "\n",
        "* BERT는 입력 토큰 수가 **최대 512개**지만, 이 함수에서는 예를 들어 `max_seq_len = 64`로 제한한다고 가정\n",
        "* `[CLS]`와 `[SEP]`을 포함해야 하므로, 실제 단어는 **64 - 2 = 62개까지만** 사용할 수 있음\n",
        "* 이를 초과한 토큰은 뒤에서 잘라냅니다 (**truncation**)\n",
        "\n",
        "## 4️⃣ `[CLS]`, `[SEP]` 특수 토큰 붙이기\n",
        "\n",
        "* 앞에 `[CLS]`, 뒤에 `[SEP]`을 붙입니다\n",
        "* 토큰뿐 아니라, 라벨 배열에도 같은 위치에 `-100`을 붙입니다\n",
        "  이유: 이 특수 토큰들엔 라벨이 없으므로 손실 계산에서 제외해야 합니다\n",
        "\n",
        "## 5️⃣ 정수 인코딩 + 어텐션 마스크 + 세그먼트 아이디\n",
        "\n",
        "* BERT는 텍스트를 그대로 받지 않고, **정수 ID 배열**로 변환해야 합니다\n",
        "* 토큰 배열을 `input_ids`로 바꾸고\n",
        "  각 토큰 위치엔 1, 패딩엔 0을 두는 **attention\\_mask**도 생성합니다\n",
        "* `token_type_ids`는 문장쌍일 때 문장 구분용인데, 여기선 전부 0으로 설정합니다 (단일 문장이므로)\n",
        "\n",
        "## 6️⃣ 패딩 추가\n",
        "\n",
        "* 최대 길이보다 짧은 문장은 부족한 만큼 `[PAD]`로 채웁니다\n",
        "* `input_ids`, `attention_mask`, `token_type_ids`, `data_labels` 모두 **동일한 길이로 맞춰야** 하므로,\n",
        "  각각 적절한 패딩 값으로 맞춥니다:\n",
        "\n",
        "  * 입력 ID → `[PAD]` 토큰의 인덱스\n",
        "  * 마스크 → `0`\n",
        "  * 세그먼트 ID → `0`\n",
        "  * 레이블 → `-100` (무시용)\n",
        "\n",
        "## 7️⃣ 정합성 체크 (디버깅 용도)\n",
        "\n",
        "* 각 배열의 길이가 `max_seq_len`과 정확히 일치하는지 `assert`로 확인합니다\n",
        "* 개발 중 잘못된 길이의 데이터가 들어오는 것을 방지합니다\n",
        "\n",
        "## 8️⃣ 모든 결과 리스트에 누적\n",
        "\n",
        "* 하나의 문장 처리가 끝나면, `input_ids`, `attention_mask`, `token_type_ids`, `data_labels` 리스트에 각각 추가합니다\n",
        "* 리스트 안에 **한 문장에 대한 결과가 배열로 들어가는 구조**입니다\n",
        "\n",
        "## 9️⃣ NumPy 배열로 변환\n",
        "\n",
        "* 학습 전에 모든 리스트를 NumPy 배열로 변환합니다\n",
        "* 텐서플로우 모델은 일반적으로 NumPy 또는 Tensor 형태의 데이터를 받기 때문입니다\n",
        "\n",
        "## 🔚 최종 출력\n",
        "\n",
        "함수는 다음을 반환합니다:\n",
        "\n",
        "```python\n",
        "(\n",
        "  (input_ids, attention_mask, token_type_ids),  # BERT 입력\n",
        "  data_labels                                   # NER 라벨\n",
        ")\n",
        "```\n",
        "\n",
        "## ✅ 요약\n",
        "\n",
        "| 단계 | 설명                                    |\n",
        "| -- | ------------------------------------- |\n",
        "| 1  | 단어 → 서브워드 토크나이징 및 라벨 확장               |\n",
        "| 2  | `[CLS]`, `[SEP]` 붙이기                  |\n",
        "| 3  | 시퀀스 길이 초과 시 자르기                       |\n",
        "| 4  | 정수 인코딩, attention mask, segment id 생성 |\n",
        "| 5  | 패딩 추가 및 길이 맞춤                         |\n",
        "| 6  | 모든 항목 리스트에 누적                         |\n",
        "| 7  | NumPy 배열로 변환 후 반환                     |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "evm47ogu91ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples,labels, max_seq_len, tokenizer,\n",
        "                                 pad_token_id_for_segment=0,\n",
        "                                 pad_token_id_for_label=-100):\n",
        "    # Hugging Face 토크나이저에서 특수 토큰 정보를 추출\n",
        "    # 문장의 시작을 나타내는 특수 토큰 (예: '[CLS]')\n",
        "    # BERT에서는 문장의 첫 토큰으로 항상 사용되며, 분류 작업에서 중요한 역할을 한다\n",
        "    cls_token = tokenizer.cls_token\n",
        "\n",
        "    # 문장의 끝 또는 문장 사이의 구분을 나타내는 특수 토큰 (예: '[SEP]')\n",
        "    # BERT는 문장 하나일 경우 문장 끝에, 문장 두 개일 경우 중간과 끝에 [SEP]를 넣는다\n",
        "    sep_token = tokenizer.sep_token\n",
        "\n",
        "    # 패딩 토큰의 숫자 인덱스 (예: 0)\n",
        "    # 입력 시퀀스를 동일한 길이로 맞추기 위해 사용되며, 손실 계산 시 무시됨\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, data_labels = [],[],[],[]\n",
        "\n",
        "    for example, label in tqdm(zip(examples,labels), total = len(examples)):\n",
        "        tokens = []\n",
        "        labels_ids = []\n",
        "        # 위에 해본것과 같이 토큰화 진행\n",
        "        for one_word, label_token in zip(example,label):\n",
        "            subword_tokens = tokenizer.tokenize(one_word)\n",
        "            tokens.extend(subword_tokens)\n",
        "            labels_ids.extend([tag_to_index[label_token]]+[pad_token_id_for_label]*(len(subword_tokens)-1))\n",
        "        # BERT의 최대 시퀀스 길이 제한: BERT는 보통 512 토큰까지만 처리 가능\n",
        "        # 특수 토큰 공간 확보: [CLS]와 [SEP] 토큰을 위한 자리(2개) 필요\n",
        "        special_tokens_count = 2\n",
        "        # max_seq_len = 64로 설정했다면\n",
        "        # 실제 토큰은 64 - 2 = 62개까지만 사용\n",
        "        # 62개보다 긴 문장은 뒤쪽을 잘라냄 (truncation)\n",
        "        # [CLS] + 토큰들(최대62개) + [SEP] 형태로 구성\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
        "            labels_ids = labels_ids[:(max_seq_len - special_tokens_count)]\n",
        "\n",
        "        # sep 토큰 추가\n",
        "        # 1. 토큰화 결과 끝 [SEP] 토큰 추가\n",
        "        # 2. 레이블에도 맨 뒷 부분에 -100 추가.\n",
        "        tokens += [sep_token]\n",
        "        labels_ids += [pad_token_id_for_label]\n",
        "\n",
        "        # cls 토큰 추가\n",
        "        # 1. 토큰화 결과 앞 [CLS] 토큰 추가\n",
        "        # 2. 레이블의 맨 앞 부분에도 -100 추가\n",
        "        tokens = [cls_token]+tokens\n",
        "        labels_ids = [pad_token_id_for_label]+labels_ids\n",
        "\n",
        "        # 정수 인코딩\n",
        "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # 어텐션 마스크 생성\n",
        "        attention_mask = [1] * len(input_id)\n",
        "\n",
        "        # 정수 인코딩에 추가할 패딩 길이 연산\n",
        "        padding_count = max_seq_len - len(tokens)\n",
        "\n",
        "        # 정수 인코딩, 어텐션 마스크 패딩 추가\n",
        "        input_id = input_id + ([pad_token_id]*padding_count)\n",
        "        attention_mask = attention_mask + ([0]*padding_count)\n",
        "\n",
        "        # 세그먼트 인코딩.\n",
        "        token_type_id = [pad_token_id_for_segment]*max_seq_len\n",
        "\n",
        "        # 레이블 패딩 -> 여기서는 패딩 아이디 -100\n",
        "        label = labels_ids + ([pad_token_id_for_label]*padding_count)\n",
        "\n",
        "        # assert -> 디버깅 도구(앞 노트에서 설명함)\n",
        "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "        assert len(label) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label), max_seq_len)\n",
        "\n",
        "        # 최종 결과를 List에 이어 붙인다 이 경우 append이기 때문에 [[]] 이런식으로 리스트 안에 리스트로 들어간다\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(label)\n",
        "\n",
        "    # 앞의 노트에서 설명한 것처럼 np 배열 형태로 전환\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), data_labels"
      ],
      "metadata": {
        "id": "Gbbg9nGByBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = convert_examples_to_features(train_data_sentence,\n",
        "                                                train_data_label,max_seq_len=128, tokenizer=tokenizer)\n",
        "X_test, y_test = convert_examples_to_features(test_data_sentence,\n",
        "                                              test_data_label,max_seq_len=128, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "HkqVWKCb7Xl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('기존 원문 :', train_data_sentence[0])\n",
        "print('기존 레이블 :', train_data_label[0])\n",
        "print('-'*50)\n",
        "print('토큰화 후 원문 :',tokenizer.convert_ids_to_tokens(X_train[0][0]))\n",
        "# 트레인 데이터 라벨의 첫 번째 행의 값 = 배열\n",
        "# 반복문 돌아서 하나하나 요소 체크하면서 -100 이면 [PAD] 아니면 매핑 딕셔너리에 맞는 토큰 출력\n",
        "print('토큰화 후 레이블 :',['[PAD]' if index == -100 else index_to_tag[index] for index in y_train[0]])\n",
        "print('-'*50)\n",
        "print('정수 인코딩 결과',X_train[0][0])\n",
        "print('정수 인코딩 레이블',y_train[0])"
      ],
      "metadata": {
        "id": "Du166CJgXfVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('세그먼트 인코딩 :', X_train[2][0])"
      ],
      "metadata": {
        "id": "Gbbtjr2Uaald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('어텐션 마스크 :', X_train[1][0])"
      ],
      "metadata": {
        "id": "Ab9eKG2gtI4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT를 이용한 개체명 인식(NER) 구현 - 다대다(Many-to-Many) 구조\n",
        "class TFBertForSequenceClassification(tf.keras.Model):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "\n",
        "        # 사전 학습된 KoBERT 모델 불러오기 (PyTorch 모델을 TF에서 사용)\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
        "\n",
        "        # 출력층: 각 토큰에 대해 num_labels개의 클래스로 분류해야 하므로 Dense(num_labels) 사용\n",
        "        # 입력: (batch_size, seq_len, hidden_size=768)\n",
        "        # 출력: (batch_size, seq_len, num_labels)\n",
        "        # 활성화 함수는 사용하지 않음 → softmax는 손실 함수에서 처리함 -> 나중에 아래 loss정의 부분에서 from_logits=True -> 내부적으로 softmax(logits)를 먼저 계산한 다음에 cross entropy를 계산함.\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            num_labels,\n",
        "            kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
        "            name='classifier'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask, token_type_ids = inputs\n",
        "\n",
        "        # BERT 모델의 출력: 모든 토큰에 대한 임베딩 벡터\n",
        "        # shape: (batch_size, seq_len=128, hidden_size=768)\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        # 개체명 인식은 문장이 아닌 각 토큰을 분류하는 작업이므로\n",
        "        # BERT의 시퀀스 출력 전체(outputs[0])를 사용함\n",
        "        all_output = outputs[0]  # shape: (batch_size, 128, 768)\n",
        "\n",
        "        # 각 토큰의 벡터(768차원)를 Dense 레이어를 통해 13개 클래스 중 하나로 분류\n",
        "        # 최종 출력: (batch_size, 128, 13)\n",
        "        prediction = self.classifier(all_output)\n",
        "\n",
        "        return prediction\n"
      ],
      "metadata": {
        "id": "FaTiqGjGtQyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.constant란?\n",
        "# tf.constant는 TensorFlow에서 상수 텐서를 생성하는 함수입니다.\n",
        "labels = tf.constant([[-100, 2, 1, -100]])\n",
        "logits = tf.constant([[[0.8, 0.1, 0.1], [0.06, 0.04, 0.9], [0.75, 0.1, 0.15],\n",
        "                      [0.4, 0.5, 0.1]]])"
      ],
      "metadata": {
        "id": "AwbaPaAK8LuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -1은 남은 차원을 자동으로 계산하라는 뜻\n",
        "#(-1,) 전체는 1차원 텐서로 만들되\n",
        "# 그 길이는 전체 원소 수에 맞춰 자동 계산하라는 의미\n",
        "# != -100 -> Boolean 마스크로 만든다 -> 1차원 불리언 텐서\n",
        "active_loss = tf.reshape(labels,(-1,)) != -100\n",
        "print(active_loss)"
      ],
      "metadata": {
        "id": "tlXqeZNN9o3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 📌  기본 개념\n",
        "#### 1️⃣ `tf.reshape(tensor, new_shape)`\n",
        "\n",
        "* 텐서의 모양(차원)을 바꾸는 함수야.\n",
        "* **데이터는 그대로 두고, 배치를 풀거나 붙이기 위해 주로 사용**해.\n",
        "* 예: `[1, 4, 3]` → `[4, 3]`\n",
        "\n",
        "#### 2️⃣ `tf.boolean_mask(tensor, mask)`\n",
        "\n",
        "* `tensor`에서 `mask == True`인 위치만 골라내는 함수야.\n",
        "* 예:\n",
        "\n",
        "  ```python\n",
        "  x = [[10, 20], [30, 40], [50, 60]]\n",
        "  mask = [True, False, True]\n",
        "  tf.boolean_mask(x, mask)\n",
        "  → 결과: [[10, 20], [50, 60]]\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "### ✅ 지금 우리가 다루는 데이터\n",
        "\n",
        "```python\n",
        "# 정답 라벨\n",
        "labels = tf.constant([[-100, 2, 1, -100]])  # shape: [1, 4]\n",
        "\n",
        "# 모델 예측 결과 (logits)\n",
        "logits = tf.constant([[\n",
        "    [0.8, 0.1, 0.1],    # 토큰1\n",
        "    [0.06, 0.04, 0.9],  # 토큰2\n",
        "    [0.75, 0.1, 0.15],  # 토큰3\n",
        "    [0.4, 0.5, 0.1]     # 토큰4\n",
        "]])  # shape: [1, 4, 3]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 👣 한 줄씩 따라가 보자\n",
        "\n",
        "\n",
        "#### ✅ 1. `tf.reshape(logits, (-1, 3))`\n",
        "\n",
        "```python\n",
        "reshaped_logits = tf.reshape(logits, (-1, 3))\n",
        "```\n",
        "\n",
        "* 원래 logits는 `[1, 4, 3]` → (배치 1, 4개의 토큰, 클래스 3개)\n",
        "* reshape 결과는 `[4, 3]`이야.\n",
        "\n",
        "```python\n",
        "reshaped_logits = tf.constant([\n",
        "  [0.8, 0.1, 0.1],     # CLS 토큰\n",
        "  [0.06, 0.04, 0.9],   # 단어 1\n",
        "  [0.75, 0.1, 0.15],   # 단어 2\n",
        "  [0.4, 0.5, 0.1]      # PAD 토큰\n",
        "])\n",
        "```\n",
        "\n",
        "* 즉, 토큰 1개당 클래스 3개 점수를 표현하는 \\[4, 3]짜리 배열이 됨\n",
        "\n",
        "\n",
        "#### ✅ 2. `active_loss = tf.reshape(labels, (-1,)) != -100`\n",
        "\n",
        "```python\n",
        "# labels = [[-100, 2, 1, -100]]\n",
        "active_loss = tf.reshape(labels, (-1,)) != -100\n",
        "```\n",
        "\n",
        "* `tf.reshape(labels, (-1,))` → `[ -100, 2, 1, -100 ]` (1차원으로 펴줌)\n",
        "* `!= -100` → \\[False, True, True, False]\n",
        "\n",
        "이건 뭐냐면:\n",
        "\n",
        "* 1번째 토큰 (CLS): -100 → 무시할 거니까 False\n",
        "* 2번째, 3번째 토큰: 라벨 있음 → True\n",
        "* 4번째 토큰 (PAD): -100 → 무시할 거니까 False\n",
        "\n",
        "```python\n",
        "active_loss = [False, True, True, False]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### ✅ 3. `tf.boolean_mask(reshaped_logits, active_loss)`\n",
        "\n",
        "```python\n",
        "reduced_logits = tf.boolean_mask(reshaped_logits, active_loss)\n",
        "```\n",
        "\n",
        "* `[4, 3]`짜리 reshaped\\_logits 중에서,\n",
        "* `active_loss == True`인 행만 남겨!\n",
        "\n",
        "즉,\n",
        "\n",
        "```python\n",
        "reshaped_logits = [\n",
        "  [0.8, 0.1, 0.1],     # 무시됨\n",
        "  [0.06, 0.04, 0.9],   # 포함\n",
        "  [0.75, 0.1, 0.15],   # 포함\n",
        "  [0.4, 0.5, 0.1]      # 무시됨\n",
        "]\n",
        "active_loss = [False, True, True, False]\n",
        "```\n",
        "\n",
        "⇒ 결과:\n",
        "\n",
        "```python\n",
        "reduced_logits = [\n",
        "  [0.06, 0.04, 0.9],\n",
        "  [0.75, 0.1, 0.15]\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "### 🧠 총정리 요약\n",
        "\n",
        "| 단계                               | 설명                                                   |\n",
        "| -------------------------------- | ---------------------------------------------------- |\n",
        "| `reshape(logits, (-1, 3))`       | `[1, 4, 3]` → `[4, 3]`로 펼쳐서 각 토큰마다의 클래스 예측을 1차 정렬    |\n",
        "| `reshape(labels, (-1,)) != -100` | `[1, 4]` → `[4]`로 펼친 후, 유효한 토큰만 골라내는 boolean mask 생성 |\n",
        "| `boolean_mask(logits, mask)`     | 유효한 토큰의 예측 결과만 추출 (손실 계산용)                           |\n",
        "\n",
        "\n",
        "필요하면 `labels` 도 똑같이 `boolean_mask`로 줄여서 loss 계산하면 돼.\n",
        "그 부분도 이어서 설명해줄 수 있어!\n",
        "이해됐는지 알려줘 — 궁금한 부분은 더 쉽게 다시 풀어줄게.\n"
      ],
      "metadata": {
        "id": "eGflduU6BZFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# boolean_mask 함수를 통하여 False 자리의 값 마스킹\n",
        "reduced_logits = tf.boolean_mask(tf.reshape(logits,(-1, shape_list(logits)[2])), active_loss)\n",
        "reduced_logits"
      ],
      "metadata": {
        "id": "hAdXmrvq_Lck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = tf.boolean_mask(tf.reshape(labels,(-1,)), active_loss)\n",
        "labels"
      ],
      "metadata": {
        "id": "QE5AtEkkGw0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 손실 함수의 Reduction이란?\n",
        "\n",
        "모델이 한 번에 여러 데이터(배치)를 처리할 때, 각 데이터마다 손실값이 나옵니다.\n",
        "\n",
        "**Reduction**은 이 손실값들을 하나의 값으로 어떻게 줄일지(통합할지) 정하는 옵션이에요.\n",
        "\n",
        "\n",
        "## 주요 Reduction 옵션\n",
        "\n",
        "- **NONE**  \n",
        "  → 각 샘플별 손실을 그대로 유지 (줄이지 않음)  \n",
        "  → 토큰별 손실을 따로 보고 싶을 때 사용\n",
        "\n",
        "- **SUM**  \n",
        "  → 모든 손실값을 다 더함  \n",
        "  → 배치 전체 손실을 합산\n",
        "\n",
        "- **SUM_OVER_BATCH_SIZE** (또는 **AUTO**)  \n",
        "  → 손실값의 평균을 계산 (보통 이게 기본값)  \n",
        "  → 배치 크기로 나누어 평균 손실 계산\n"
      ],
      "metadata": {
        "id": "K5rI1gwzNRiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 예시 코드들을 바탕으로 레이블 값이 -100 인 경우를 학습 시에 무시하는 방법 적용 -> 손실 함수 구현\n",
        "def compute_loss(labels, logits):\n",
        "  # 다중 클래스 분류 문제에서 softmax 사용 x -> from_logits=True 설정\n",
        "  # NER 같은 다중 클래스 분류에서는 보통 레이블이 정수 인덱스 형태로 존재하기 때문에 -> SparseCategoricalCrossentropy(정수값)\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      # from_logits=True는 입력값이 아직 softmax를 거치지 않은 원시 점수(로짓, logits)임을 알려주는 옵션 -> 손실 함수가 내부적으로 softmax계싼 -> 확률 변환 -> 손실 계산\n",
        "      # 손실함수 계산시 모든 배치 데이터의 손실 평균 or 합산 x -> 각 데이터별 손실을 개별적으로 반환\n",
        "      # 개체명 인식(Named Entity Recognition, NER) 문제는 시퀀스 레이블링 문제라서, 입력 문장의 각 토큰마다 클래스를 예측해야 해요. 즉, 토큰별로 손실(loss)을 계산\n",
        "      # 만일 합쳐지거나 평균내지면 -> 토큰별 세밀한 조정 불가\n",
        "      from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "      )\n",
        "  # -100 값 반영 x 하도록 라벨 수정\n",
        "  active_loss = tf.reshape(labels,(-1,))!= -100\n",
        "\n",
        "  # 라벨과 로짓에 마스킹 적용\n",
        "  reduced_logits = tf.boolean_mask(tf.reshape(logits,(-1,shape_list(logits)[2])), active_loss)\n",
        "  labels = tf.boolean_mask(tf.reshape(labels,(-1,)),active_loss)\n",
        "\n",
        "  return loss_fn(labels, reduced_logits)"
      ],
      "metadata": {
        "id": "DzY_SCcvJX4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification(\"klue/bert-base\",num_labels=tag_size)\n",
        "optimizer = tf.keras.optimizers.Adam(5e-5)\n",
        "model.compile(optimizer=optimizer, loss=compute_loss)"
      ],
      "metadata": {
        "id": "aD39LoPvIEbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개체명 인식에서 F1 Score를 에폭마다 계산하는 이유\n",
        "\n",
        "- **정확도(Accuracy)는 부족해요**  \n",
        "  토큰 하나하나만 맞았는지 보기 때문에, 실제 개체명 전체를 잘 찾았는지 알기 어려움\n",
        "\n",
        "- **F1 Score가 더 좋아요**  \n",
        "  - 정밀도(모델이 맞다고 한 것 중 진짜 맞은 비율)와  \n",
        "  - 재현율(진짜 개체명 중 모델이 찾아낸 비율)을 함께 평가\n",
        "  둘을 잘 조화시킨 값입니다.\n",
        "\n",
        "- **개체명 인식 문제에 딱 맞아요**  \n",
        "  개체명은 시작과 끝, 그리고 라벨까지 정확히 맞아야 하므로,  \n",
        "  F1 Score가 성능을 정확히 보여줌\n",
        "\n",
        "- **그래서 매 에폭마다 F1 Score를 계산해요**  \n",
        "  모델이 점점 더 잘 학습되는지 효과적으로 확인하기 위해서\n"
      ],
      "metadata": {
        "id": "5Bc1iXftOL4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 📌 F1score 콜백 클래스 설명\n",
        "\n",
        "이 코드는 **Keras 모델 학습 중 매 에폭마다 F1 score와 분류 리포트를 출력해주는 콜백** 클래스입니다. 주로 **개체명 인식(NER)** 같은 시퀀스 라벨링 문제에 사용됩니다.\n",
        "\n",
        "\n",
        "\n",
        "### 🔧 클래스 정의\n",
        "\n",
        "```python\n",
        "class F1score(tf.keras.callbacks.Callback):\n",
        "```\n",
        "\n",
        "* `tf.keras.callbacks.Callback`을 상속받아 Keras 학습 과정에 끼어들 수 있음\n",
        "* 매 에폭이 끝날 때 자동으로 평가 로직 실행\n",
        "\n",
        "\n",
        "\n",
        "### 🏷️ 생성자 (`__init__`)\n",
        "\n",
        "```python\n",
        "def __init__(self, X_test, y_test):\n",
        "    self.X_test = X_test\n",
        "    self.y_test = y_test\n",
        "```\n",
        "\n",
        "* 테스트용 입력 (`X_test`)과 정답 라벨 (`y_test`)을 받아 저장\n",
        "* 이후 에폭마다 모델 성능을 이 데이터로 평가\n",
        "\n",
        "\n",
        "### 🔁 시퀀스를 텍스트 태그로 변환 (`sequences_to_tags`)\n",
        "\n",
        "```python\n",
        "def sequences_to_tags(self, label_ids, pred_ids):\n",
        "```\n",
        "\n",
        "* 모델이 출력한 예측값 (`pred_ids`)과 실제 라벨값 (`label_ids`)을 텍스트 태그(예: `B-PER`, `O`, `I-LOC` 등)로 변환\n",
        "\n",
        "#### 주요 처리:\n",
        "\n",
        "* `-100`인 라벨은 무시함 (보통 패딩 토큰용)\n",
        "* `index_to_tag`를 사용해 정수 인덱스를 텍스트 태그로 변환\n",
        "\n",
        "**반환값**:\n",
        "\n",
        "* `label_list`: 실제 정답 태그들\n",
        "* `pred_list`: 모델이 예측한 태그들\n",
        "\n",
        "\n",
        "### ✅ 에폭 종료 시 실행 (`on_epoch_end`)\n",
        "\n",
        "```python\n",
        "def on_epoch_end(self, epoch, logs={}):\n",
        "```\n",
        "\n",
        "* 각 에폭이 끝날 때 자동으로 호출됨\n",
        "\n",
        "#### 수행 순서:\n",
        "\n",
        "1. **예측 수행**\n",
        "\n",
        "   ```python\n",
        "   y_predicted = self.model.predict(self.X_test)\n",
        "   y_predicted = np.argmax(y_predicted, axis=2)\n",
        "   ```\n",
        "\n",
        "   * 모델이 출력한 확률 분포에서 가장 높은 확률을 가지는 클래스 인덱스를 선택\n",
        "\n",
        "2. **정답 및 예측 태그로 변환**\n",
        "\n",
        "   ```python\n",
        "   label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
        "   ```\n",
        "\n",
        "3. **F1 점수 계산 및 출력**\n",
        "\n",
        "   ```python\n",
        "   score = f1_score(label_list, pred_list, suffix=True)\n",
        "   print(' - f1: {:04.2f}'.format(score * 100))\n",
        "   ```\n",
        "\n",
        "4. **자세한 분류 리포트 출력**\n",
        "\n",
        "   ```python\n",
        "   print(classification_report(label_list, pred_list, suffix=True))\n",
        "   ```\n",
        "\n",
        "\n",
        "### 🧩 사용 전 확인해야 할 것\n",
        "\n",
        "* `index_to_tag`: 인덱스를 태그로 매핑하는 딕셔너리 (전역 변수로 정의되어 있어야 함)\n",
        "* `f1_score`, `classification_report`: `seqeval` 패키지에서 가져온 함수여야 함\n",
        "\n",
        "```python\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "```\n",
        "\n",
        "\n",
        "이 콜백은 특히 **NER 모델 학습 중 실시간 평가를 위해 매우 유용**하며, `model.fit()` 시 `callbacks=[F1score(...)]` 형태로 사용됩니다.\n"
      ],
      "metadata": {
        "id": "qdn6yP7wUCe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "voooB8jyTqgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class F1score(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X_test, y_test):\n",
        "        # 테스트 데이터(입력, 정답)를 콜백 객체에 저장\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def sequences_to_tags(self, label_ids, pred_ids):\n",
        "        # 모델 출력 및 실제 레이블을 텍스트 태그 리스트로 변환하는 함수\n",
        "        label_list = []  # 실제 레이블 태그를 담을 리스트\n",
        "        pred_list = []   # 예측된 레이블 태그를 담을 리스트\n",
        "\n",
        "        # 배치(문장) 단위로 반복\n",
        "        for i in range(0, len(label_ids)):\n",
        "            label_tag = []  # 현재 문장 실제 태그 리스트\n",
        "            pred_tag = []   # 현재 문장 예측 태그 리스트\n",
        "\n",
        "            # 각 문장 내 토큰별로 실제 레이블과 예측 레이블 비교\n",
        "            # -100은 패딩 토큰에 해당, 평가에서 제외해야 함\n",
        "            for label_index, pred_index in zip(label_ids[i], pred_ids[i]):\n",
        "                if label_index != -100:  # 패딩이 아닌 실제 토큰만 처리\n",
        "                    # 정수 인덱스를 실제 태그 문자열로 변환\n",
        "                    label_tag.append(index_to_tag[label_index])\n",
        "                    pred_tag.append(index_to_tag[pred_index])\n",
        "\n",
        "            # 변환된 문장 단위 태그 리스트를 전체 리스트에 추가\n",
        "            label_list.append(label_tag)\n",
        "            pred_list.append(pred_tag)\n",
        "\n",
        "        # 실제 레이블과 예측 레이블 리스트 반환\n",
        "        return label_list, pred_list\n",
        "\n",
        "    # 매 학습 에폭이 끝날 때 자동으로 호출되는 함수\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # 테스트 입력(X_test)에 대해 현재 모델의 예측 수행\n",
        "        y_predicted = self.model.predict(self.X_test)\n",
        "        # 예측 결과에서 가장 높은 확률을 가진 클래스를 선택 (argmax)\n",
        "        # axis=2는 각 토큰별 클래스 차원에서 선택한다는 의미 (batch, seq_len, num_classes)\n",
        "        # argmax\t최댓값의 위치/인덱스\n",
        "        y_predicted = np.argmax(y_predicted, axis=2)\n",
        "\n",
        "        # 실제 레이블과 예측 레이블을 텍스트 태그 리스트로 변환\n",
        "        label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
        "\n",
        "        # F1 점수 계산 (개체명 인식 성능 지표)\n",
        "        # f1_score 함수:\n",
        "        #   - 정밀도(Precision): 예측한 개체 중 실제로 맞는 비율\n",
        "        #   - 재현율(Recall): 실제 개체 중 올바르게 찾아낸 비율\n",
        "        #   - F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "        #\n",
        "        # suffix=True 파라미터:\n",
        "        #   - BIO 태깅 방식에서 접미사(-PER, -LOC 등)를 기준으로 개체 단위 평가\n",
        "        #   - 예: ['B-PER', 'I-PER'] → 하나의 PER 개체로 취급\n",
        "        #   - 개체의 시작과 끝 위치가 완전히 일치해야만 정답으로 인정 (엄격한 평가)\n",
        "        #   - 부분 일치는 0점 처리 (예: \"홍길동\" 중 \"홍길\"만 예측하면 오답)\n",
        "        score = f1_score(label_list, pred_list, suffix=True)\n",
        "        # 계산된 F1 점수 출력 (백분율로 보기 좋게 변환)\n",
        "        print(' - f1: {:04.2f}'.format(score * 100))\n",
        "        # 각 클래스별 정밀도, 재현율, F1 점수 등 자세한 리포트 출력\n",
        "        print(classification_report(label_list, pred_list, suffix=True))\n"
      ],
      "metadata": {
        "id": "ZjaunWmnJEvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score_report = F1score(X_test,y_test)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train, epochs=3, batch_size=32,\n",
        "    callbacks=[f1_score_report]\n",
        ")"
      ],
      "metadata": {
        "id": "S7P4GCmvQRkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 📌 `convert_examples_to_features_for_prediction` 함수 설명\n",
        "\n",
        "이 함수는 **예측용 문장 리스트를 BERT 기반 입력 포맷**으로 변환합니다.\n",
        "NER(Named Entity Recognition) 또는 시퀀스 태깅 모델의 입력으로 사용됩니다.\n",
        "\n",
        "\n",
        "\n",
        "### 🧩 함수 정의\n",
        "\n",
        "```python\n",
        "def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer,\n",
        "                                                pad_token_id_for_segment=0,\n",
        "                                                pad_token_id_for_label=-100):\n",
        "```\n",
        "\n",
        "#### 파라미터 설명\n",
        "\n",
        "| 파라미터                       | 설명                                                              |\n",
        "| -------------------------- | --------------------------------------------------------------- |\n",
        "| `examples`                 | 문장 단위의 단어 리스트 (예: `[['Hello', 'world'], ['My', 'name', 'is']]`) |\n",
        "| `max_seq_len`              | 최대 시퀀스 길이 (예: 128)                                              |\n",
        "| `tokenizer`                | HuggingFace의 tokenizer 객체                                       |\n",
        "| `pad_token_id_for_segment` | 세그먼트 ID용 패딩값 (보통 0)                                             |\n",
        "| `pad_token_id_for_label`   | 라벨용 패딩값 (보통 -100로 마스킹)                                          |\n",
        "\n",
        "\n",
        "\n",
        "### 🔄 전체 처리 흐름 요약\n",
        "\n",
        "1. 각 문장을 서브워드 단위로 토크나이징\n",
        "2. 서브워드 처리 시 **첫 서브워드만 라벨을 예측**하고, 나머지는 무시(`-100`)\n",
        "3. `[CLS]`와 `[SEP]` 토큰 추가 (레이블도 맞춰서 `-100` 추가)\n",
        "4. 정수 인코딩 + attention mask + segment ID 생성\n",
        "5. 필요한 경우 길이를 `max_seq_len`으로 패딩 처리\n",
        "6. 모두 `numpy array`로 반환\n",
        "\n",
        "\n",
        "\n",
        "### 🧱 내부 주요 단계 설명\n",
        "\n",
        "#### 1. 서브워드 토크나이징 & 라벨 마스킹\n",
        "\n",
        "```python\n",
        "subword_tokens = tokenizer.tokenize(one_word)\n",
        "tokens.extend(subword_tokens)\n",
        "label_mask.extend([0] + [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
        "```\n",
        "\n",
        "* 예: `playing → ['play', '##ing']` 이라면\n",
        "  `label_mask = [0, -100]`\n",
        "  (첫 서브워드만 레이블 평가 대상)\n",
        "\n",
        "\n",
        "#### 2. `[CLS]`와 `[SEP]` 토큰 추가\n",
        "\n",
        "```python\n",
        "tokens = [cls_token] + tokens + [sep_token]\n",
        "label_mask = [pad_token_id_for_label] + label_mask + [pad_token_id_for_label]\n",
        "```\n",
        "\n",
        "* 입력 앞뒤에 특수 토큰 추가\n",
        "* 라벨 마스크에도 맞춰 `-100` 삽입\n",
        "\n",
        "\n",
        "#### 3. 정수 인코딩 및 패딩\n",
        "\n",
        "```python\n",
        "input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "padding_count = max_seq_len - len(input_id)\n",
        "\n",
        "input_id += [pad_token_id] * padding_count\n",
        "attention_mask = [1] * len(tokens) + [0] * padding_count\n",
        "token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
        "label_mask += [pad_token_id_for_label] * padding_count\n",
        "```\n",
        "\n",
        "* 토큰을 정수로 변환\n",
        "* 부족한 길이만큼 패딩 추가\n",
        "* attention mask는 실제 토큰에는 1, 패딩에는 0\n",
        "\n",
        "\n",
        "\n",
        "#### 4. 길이 검증\n",
        "\n",
        "```python\n",
        "assert len(input_id) == max_seq_len\n",
        "assert len(attention_mask) == max_seq_len\n",
        "...\n",
        "```\n",
        "\n",
        "* 전처리가 정확히 작동했는지 확인\n",
        "\n",
        "\n",
        "\n",
        "### 📤 반환값\n",
        "\n",
        "```python\n",
        "return (input_ids, attention_masks, token_type_ids), label_masks\n",
        "```\n",
        "\n",
        "* `input_ids`: BERT 입력용 정수 토큰 배열\n",
        "* `attention_masks`: 1은 실제 토큰, 0은 패딩\n",
        "* `token_type_ids`: 문장 구분용 세그먼트 (보통 NER에서는 모두 0)\n",
        "* `label_masks`: 실제 평가할 토큰은 0, 나머지는 `-100` (loss 계산 제외용)\n",
        "\n",
        "\n",
        "\n",
        "### ✅ 최종 사용 예시\n",
        "\n",
        "```python\n",
        "X = [['오늘', '날씨', '좋다'], ['나는', '학생입니다']]\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "inputs, label_masks = convert_examples_to_features_for_prediction(X, 64, tokenizer)\n",
        "```\n",
        "\n",
        "* 예측 또는 추론용 BERT 모델 입력 생성에 유용\n",
        "* 라벨은 없지만 라벨 마스크는 평가용 필터링을 위해 유지됨\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l2XuRGjyU7sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer,\n",
        "                                             pad_token_id_for_segment=0,\n",
        "                                             pad_token_id_for_label=-100):\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, label_masks = [], [], [], []\n",
        "\n",
        "    for example in tqdm(examples):\n",
        "        tokens = []\n",
        "        label_mask = []\n",
        "        for one_word in example:\n",
        "            # 하나의 단어에 대해서 서브워드로 토큰화\n",
        "            subword_tokens = tokenizer.tokenize(one_word)\n",
        "            tokens.extend(subword_tokens)\n",
        "            # 서브워드로 쪼개진 첫번째 서브워드를 제외하고 그 뒤의 서브워드들은 -100으로 마\n",
        "            # 스킹함\n",
        "            label_mask.extend([0]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
        "\n",
        "        # [CLS]와 [SEP]를 후에 추가할 것을 고려하여 최대 길이를 초과하는 샘플의 경\n",
        "        # 우 max_seq_len - 2의 길이로 변환.\n",
        "        # ex) max_seq_len = 64라면 길이가 62보다 긴 샘플은 뒷 부분을 자르고 길이\n",
        "        # 62로 변환.\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
        "            label_mask = label_mask[:(max_seq_len - special_tokens_count)]\n",
        "\n",
        "        # [SEP]를 추가하는 코드\n",
        "        # 1. 토르화 결과의 맨 뒷 부분에 [SEP] 토큰 추가\n",
        "        # 2. 레이블에도 맨 뒷 부분에 -100 추가.\n",
        "        tokens += [sep_token]\n",
        "        label_mask += [pad_token_id_for_label]\n",
        "\n",
        "        # [CLS]를 추가하는 코드\n",
        "        # 1. 토큰화 결과의 앞 부분에 [CLS] 토큰 추가\n",
        "        # 2. 레이블의 맨 앞 부분에도 -100 추가.\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_mask = [pad_token_id_for_label] + label_mask\n",
        "\n",
        "        # 정수 인코딩\n",
        "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # 어텐션 마스크 생성\n",
        "        attention_mask = [1] * len(input_id)\n",
        "\n",
        "        # 정수 인코딩에 추가할 패딩 길이 연산\n",
        "        padding_count = max_seq_len - len(input_id)\n",
        "\n",
        "        # 정수 인코딩, 어텐션 마스크에 패딩 추가\n",
        "        input_id = input_id + ([pad_token_id] * padding_count)\n",
        "        attention_mask = attention_mask + ([0] * padding_count)\n",
        "\n",
        "        # 세그먼트 인코딩.\n",
        "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
        "\n",
        "        # 레이블 패딩. (단, 이 경우는 패딩 토큰의 ID가 -100)\n",
        "        label_mask = label_mask + ([pad_token_id_for_label] * padding_count)\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "        assert len(label_mask) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label_mask), max_seq_len)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        label_masks.append(label_mask)\n",
        "\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "    label_masks = np.array(label_masks, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), label_masks"
      ],
      "metadata": {
        "id": "K_SuAoYVRDRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pred, label_masks = convert_examples_to_features_for_prediction(\n",
        "    test_data_sentence[:5], max_seq_len=128, tokenizer=tokenizer)\n",
        "\n",
        "print('기존 원문 :', test_data_sentence[0])\n",
        "print('-' * 50)\n",
        "print('토큰화 후 원문 :', [tokenizer.decode([word]) for word in X_pred[0][0]])\n",
        "print('레이블 마스크 :', ['[PAD]' if idx == -100 else '[FIRST]' for idx in label_masks[0]])"
      ],
      "metadata": {
        "id": "GHCj1NZgVVgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 📌 `ner_prediction` 함수 설명\n",
        "\n",
        "이 함수는 문장 리스트에 대해 **NER(Named Entity Recognition)** 예측 결과를 반환합니다.\n",
        "예측 결과는 각 단어에 대해 (단어, 예측된 개체명 태그) 형태로 구성됩니다.\n",
        "\n",
        "\n",
        "\n",
        "### 🧩 함수 정의\n",
        "\n",
        "```python\n",
        "def ner_prediction(examples, max_seq_len, tokenizer):\n",
        "```\n",
        "\n",
        "#### 파라미터 설명\n",
        "\n",
        "| 파라미터          | 설명                                          |\n",
        "| ------------- | ------------------------------------------- |\n",
        "| `examples`    | 문자열 문장 리스트 (예: `[\"나는 학생입니다\", \"이순신은 장군이다\"]`) |\n",
        "| `max_seq_len` | 모델 입력의 최대 길이 (예: 128)                       |\n",
        "| `tokenizer`   | HuggingFace의 BERT tokenizer 객체              |\n",
        "\n",
        "\n",
        "\n",
        "### 🔄 전체 처리 흐름\n",
        "\n",
        "1. 문장들을 단어 리스트로 변환\n",
        "2. 예측용 입력 포맷으로 변환 (`convert_examples_to_features_for_prediction` 호출)\n",
        "3. 모델 예측 수행\n",
        "4. 예측된 라벨 인덱스를 텍스트 태그로 변환\n",
        "5. (단어, 예측 태그) 형태로 결과 구성\n",
        "\n",
        "\n",
        "\n",
        "### 1️⃣ 문장 분할 (단어 리스트로 변환)\n",
        "\n",
        "```python\n",
        "examples = [sent.split() for sent in examples]\n",
        "```\n",
        "\n",
        "* 문자열 문장 → 단어 단위 리스트로 변환\n",
        "* 예: `\"나는 학생입니다\"` → `['나는', '학생입니다']`\n",
        "\n",
        "\n",
        "\n",
        "### 2️⃣ 입력 포맷 변환\n",
        "\n",
        "```python\n",
        "X_pred, label_masks = convert_examples_to_features_for_prediction(...)\n",
        "```\n",
        "\n",
        "* 문장을 BERT 입력 형식으로 변환\n",
        "* 반환값:\n",
        "\n",
        "  * `X_pred`: (input\\_ids, attention\\_masks, token\\_type\\_ids)\n",
        "  * `label_masks`: 실제 평가 대상인 토큰만 `0`, 나머지는 `-100`\n",
        "\n",
        "\n",
        "\n",
        "### 3️⃣ 모델 예측 수행\n",
        "\n",
        "```python\n",
        "y_predicted = model.predict(X_pred)\n",
        "y_predicted = np.argmax(y_predicted, axis=2)\n",
        "```\n",
        "\n",
        "* 모델 출력: (배치, 시퀀스 길이, 클래스 수)\n",
        "* 가장 높은 확률을 갖는 클래스 인덱스 선택\n",
        "\n",
        "\n",
        "\n",
        "### 4️⃣ 예측값을 태그로 디코딩\n",
        "\n",
        "```python\n",
        "for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n",
        "    if label_index != -100:\n",
        "        pred_tag.append(index_to_tag[pred_index])\n",
        "```\n",
        "\n",
        "* `-100`인 위치는 무시 (예: `[CLS]`, `[SEP]`, 서브워드 등)\n",
        "* 나머지 위치의 예측 인덱스를 태그로 변환 (예: `3 → B-PER`)\n",
        "\n",
        "\n",
        "\n",
        "### 5️⃣ 단어와 예측 태그를 묶기\n",
        "\n",
        "```python\n",
        "for example, pred in zip(examples, pred_list):\n",
        "    one_sample_result = []\n",
        "    for one_word, label_token in zip(example, pred):\n",
        "        one_sample_result.append((one_word, label_token))\n",
        "```\n",
        "\n",
        "* 예:\n",
        "  입력: `['이순신', '장군']`\n",
        "  예측: `['B-PER', 'O']`\n",
        "  결과: `[('이순신', 'B-PER'), ('장군', 'O')]`\n",
        "\n",
        "\n",
        "\n",
        "### 📤 반환값\n",
        "\n",
        "```python\n",
        "return result_list\n",
        "```\n",
        "\n",
        "* 결과 형태: `[[('단어1', '태그1'), ('단어2', '태그2'), ...], [...], ...]`\n",
        "* 문장 단위로 단어와 태그가 묶여 있음\n",
        "\n",
        "\n",
        "### ✅ 예시 사용\n",
        "\n",
        "```python\n",
        "sentences = [\"이순신은 장군이다\", \"삼성은 한국 기업이다\"]\n",
        "results = ner_prediction(sentences, 128, tokenizer)\n",
        "\n",
        "# 출력 예시\n",
        "# [[('이순신은', 'B-PER'), ('장군이다', 'O')],\n",
        "#  [('삼성은', 'B-ORG'), ('한국', 'B-LOC'), ('기업이다', 'O')]]\n",
        "```\n",
        "\n",
        "\n",
        "### 🔔 주의사항\n",
        "\n",
        "* 전역 변수 `model`, `index_to_tag` 가 정의되어 있어야 정상 작동합니다.\n",
        "\n",
        "  ```python\n",
        "  # 예시\n",
        "  model = tf.keras.models.load_model(...)\n",
        "  index_to_tag = {0: 'O', 1: 'B-PER', 2: 'I-PER', ...}\n",
        "  ```\n",
        "\n",
        "* 입력 문장은 띄어쓰기 기준으로 나뉘므로 형태소 기준으로 나누지 않는 한 성능에 영향을 줄 수 있음\n"
      ],
      "metadata": {
        "id": "H0j3nCeEV3Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_prediction(examples, max_seq_len, tokenizer):\n",
        "    examples = [sent.split() for sent in examples]\n",
        "    X_pred, label_masks = convert_examples_to_features_for_prediction(\n",
        "        examples,\n",
        "        max_seq_len=128,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    y_predicted = model.predict(X_pred)\n",
        "    # 마지막 차원 클래스차원에서 최대값을 찾는다\n",
        "    # 마지막 차원 = 클래스 차원 -> 각 클래스 확률이 나와있을거니 아마도 softmax -> 그중에 최대값 -> 해당 클래스일 확률 높음\n",
        "    y_predicted = np.argmax(y_predicted, axis=2)\n",
        "\n",
        "    pred_list = []\n",
        "    result_list = []\n",
        "\n",
        "    for i in range(0, len(label_masks)):\n",
        "        pred_tag = []\n",
        "\n",
        "        # ex) 모델의 예측값 디코딩 과정\n",
        "        # 예측값(y_predicted)에서 레이블 마스크(label_masks)의 값이 -100인 동일 위치의 값은 삭제\n",
        "        # label_masks : [-100 -100 0 0 -100]\n",
        "        # y_predicted : [ 0   2   0 2   0 ] ==> [1 2] ==> 최종 예측(pred_tag) : [PER-B PER-I]\n",
        "        for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n",
        "            if label_index != -100:\n",
        "                pred_tag.append(index_to_tag[pred_index])\n",
        "\n",
        "        pred_list.append(pred_tag)\n",
        "\n",
        "    for example, pred in zip(examples, pred_list):\n",
        "        one_sample_result = []\n",
        "        for one_word, label_token in zip(example, pred):\n",
        "            one_sample_result.append((one_word, label_token))\n",
        "        result_list.append(one_sample_result)\n",
        "\n",
        "    return result_list\n"
      ],
      "metadata": {
        "id": "MQKjA2mJVtO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 차원 변화 과정\n",
        "\n",
        "### 1. 원본 `y_predicted`\n",
        "\n",
        "- 모델 출력: `(batch_size, sequence_length, num_classes)`\n",
        "- 예시: `y_predicted.shape = (2, 5, 9)`  \n",
        "  (배치 2개, 시퀀스 길이 5, 클래스 수 9)\n",
        "\n",
        "```python\n",
        "y_predicted = [\n",
        "    # 첫 번째 문장\n",
        "    [\n",
        "        [0.1, 0.0, 0.9, 0.0, ...],  # 첫 번째 토큰의 각 클래스 확률 -> 2\n",
        "        [0.8, 0.1, 0.1, 0.0, ...],  # 두 번째 토큰의 각 클래스 확률 -> 0\n",
        "        [0.0, 0.0, 0.0, 0.9, ...],  # 세 번째 토큰의 각 클래스 확률 -> 3\n",
        "        ...\n",
        "    ],\n",
        "    # 두 번째 문장\n",
        "    [\n",
        "        ...\n",
        "    ]\n",
        "]\n",
        "````\n",
        "\n",
        "\n",
        "\n",
        "### 2. `argmax` 적용 후\n",
        "\n",
        "```python\n",
        "y_predicted = np.argmax(y_predicted, axis=2)\n",
        "```\n",
        "\n",
        "* 결과 차원: `(batch_size, sequence_length)` → 여전히 2차원!\n",
        "* `y_predicted.shape = (2, 5)`\n",
        "\n",
        "```python\n",
        "y_predicted = [\n",
        "    [2, 0, 3, 1, 0],  # 첫 번째 문장의 각 토큰 예측 클래스\n",
        "    [1, 2, 0, 0, 4]   # 두 번째 문장의 각 토큰 예측 클래스\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "n4RbRrhgyI6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측할 문장\n",
        "sent1 = '오리온스는 리그 최정상급 포인트가드 김동훈을 앞세우는 빠른 공수전환이 돋보이는 팀이다'\n",
        "sent2 = '하이신사에 속한 섬들도 위로 솟아 있는데 타인은 살고 있어요'\n",
        "\n",
        "# 문장 리스트\n",
        "test_samples = [sent1, sent2]\n",
        "\n",
        "# NER 예측 함수 호출 (128 토큰 제한)\n",
        "result_list = ner_prediction(test_samples, max_seq_len=128, tokenizer=tokenizer)\n",
        "\n",
        "# 결과 출력\n",
        "result_list"
      ],
      "metadata": {
        "id": "p0x_0JlDWSon"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}