{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhwan01/DeepDive/blob/main/transformer%EC%82%AC%EC%9A%A9_%EB%84%A4%EC%9D%B4%EB%B2%84%EB%A6%AC%EB%B7%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ìƒˆ ì…€ ì²« ë²ˆì§¸ ì¤„\n",
        "# TF_USE_LEGACY_KERAS='1' ì„¤ì •í•˜ë©´ ì˜›ë‚  ë°©ì‹ìœ¼ë¡œ ëŒì•„ê°€ì„œ ëª¨ë“  ê²Œ ì •ìƒ ì‘ë™\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ],
      "metadata": {
        "id": "6QPaLcpw4kE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10SAN25WzjUc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00NVS-B2z3Zw"
      },
      "outputs": [],
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
        "                          filename=\"ratings_train.txt\")\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\",\n",
        "                          filename=\"ratings_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF-KPpR00LMC"
      },
      "outputs": [],
      "source": [
        "test_data=pd.read_table('ratings_test.txt')\n",
        "train_data=pd.read_table('ratings_train.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdd3qNce0lZJ"
      },
      "outputs": [],
      "source": [
        "print('í›ˆë ¨ìš© ë¦¬ë·° ê°œìˆ˜ :', len(train_data))\n",
        "print('í…ŒìŠ¤íŠ¸ìš© ë¦¬ë·° ê°œìˆ˜ :', len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ71Y8g-0qa_"
      },
      "outputs": [],
      "source": [
        "train_data.drop_duplicates(subset=['document'],inplace=True)\n",
        "train_data=train_data.dropna(how='any')\n",
        "print('í›ˆë ¨ ë°ì´í„°ì˜ ë¦¬ë·° ìˆ˜ :',len(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1wwpw6p016v"
      },
      "outputs": [],
      "source": [
        "test_data=test_data.dropna(how='any')\n",
        "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¦¬ë·° ìˆ˜ :',len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHHEja5l1b87"
      },
      "outputs": [],
      "source": [
        "# ì‚¬ì „í•™ìŠµëœ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "# Hugging Face transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ”,Rust ì–¸ì–´ë¡œ êµ¬í˜„ëœ ì´ˆê³ ì† í† í¬ë‚˜ì´ì €\n",
        "from transformers import BertTokenizerFast\n",
        "tokenizer=BertTokenizerFast.from_pretrained('klue/bert-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4eBaVE51oov"
      },
      "outputs": [],
      "source": [
        "#.valuesëŠ” DataFrameì—ì„œ ì—´ì„ numpy ë°°ì—´ë¡œ ê°€ì ¸ì˜¤ê³ ,\n",
        "#.tolist()ëŠ” numpy ë°°ì—´ì„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "# í•˜ì§€ë§Œ ìµœì‹  íŒŒì´ì¬ì—ì„œëŠ” tolistë§Œì¨ë„ ì¶©ë¶„í•¨\n",
        "# ë¨¸ì‹ ëŸ¬ë‹Â·ë”¥ëŸ¬ë‹ ëª¨ë¸, ì „ì²˜ë¦¬ ë„êµ¬, í† í¬ë‚˜ì´ì € ë“±ì€ ëŒ€ë¶€ë¶„ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë°°ì—´ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ìš”êµ¬í•©ë‹ˆë‹¤\n",
        "X_train_list = train_data['document'].tolist()\n",
        "X_test_list = test_data['document'].tolist()\n",
        "y_train = train_data['label'].tolist()\n",
        "y_test = test_data['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3pGS71y2b-o"
      },
      "outputs": [],
      "source": [
        "# truncation = True -> Bertì˜ ê¸°ë³¸ max_length=512ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì— ë’¤ë¥¼ ìë¥¸ë‹¤\n",
        "# padding = True -> ì§§ì€ ë¬¸ì¥ 0ìœ¼ë¡œ íŒ¨ë”©\n",
        "X_train = tokenizer(X_train_list, truncation=True, padding=True)\n",
        "X_test = tokenizer(X_test_list, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylbJ_6_v4ZjX"
      },
      "outputs": [],
      "source": [
        "# ìƒ˜í”Œì— .tokensë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™” ê²°ê³¼ í™•ì¸\n",
        "print(X_train[0].tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZbEDUuM4owK"
      },
      "outputs": [],
      "source": [
        "# ìƒ˜í”Œì— .idsë¥¼ í•˜ë©´ ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ í™•ì¸\n",
        "print(X_train[0].ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì„¸ê·¸ë¨¼íŠ¸ ì¸ì½”ë”© ê°’ í™•ì¸ ëª¨ë“  ìœ„ì¹˜ ê°’ì´ 0 -> ì¸ì‹í•  ë¬¸ì¥ ì¢…ë¥˜ 1ê°œ\n",
        "print(X_train[0].type_ids)"
      ],
      "metadata": {
        "id": "Cgj9tNFNTj9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í†µí•´ ì‹¤ì œ ë‹¨ì–´ê°€ ìˆëŠ” ìœ„ì¹˜ì™€ íŒ¨ë”©ì˜ ìœ„ì¹˜ë¥¼ í™•ì¸\n",
        "print(X_train[0].attention_mask)"
      ],
      "metadata": {
        "id": "L2Z-M6thUcS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ğŸ“˜ `tf.data.Dataset.from_tensor_slices()` ì´ì •ë¦¬\n",
        "\n",
        "## âœ… ê°œë… ìš”ì•½\n",
        "\n",
        "### ğŸ“Œ í•œ ì¤„ ì„¤ëª…\n",
        "> ìƒ˜í”Œ í•˜ë‚˜í•˜ë‚˜ë¡œ ë‚˜ëˆ ì„œ `tf.data.Dataset` ê°ì²´ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜\n",
        "\n",
        "### ğŸ”§ ì–¸ì œ ì¨?\n",
        "TensorFlow ëª¨ë¸(`model.fit()`)ì— í•™ìŠµìš© ë°ì´í„°ë¥¼ ë„£ê¸° ìœ„í•œ Dataset ê°ì²´ ìƒì„±í•  ë•Œ ì‚¬ìš©\n",
        "\n",
        "## âœ… ê¸°ë³¸ ì‚¬ìš©ë²•\n",
        "\n",
        "```python\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "````\n",
        "\n",
        "* `X`: ì…ë ¥ ë°ì´í„° (ë„˜íŒŒì´ ë°°ì—´, í…ì„œ, ë”•ì…”ë„ˆë¦¬ ë“± ê°€ëŠ¥)\n",
        "* `y`: ì •ë‹µ(ë ˆì´ë¸”)\n",
        "\n",
        "## âœ… ì‘ë™ ë°©ì‹ (ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ìª¼ê°œì§)\n",
        "\n",
        "```python\n",
        "X = [[1, 2], [3, 4], [5, 6]]\n",
        "y = [0, 1, 0]\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "```\n",
        "\n",
        "â¡ï¸ ìœ„ ì½”ë“œëŠ” ì•„ë˜ì²˜ëŸ¼ ë¶„í• ë¨:\n",
        "\n",
        "* ìƒ˜í”Œ 1: `([1, 2], 0)`\n",
        "* ìƒ˜í”Œ 2: `([3, 4], 1)`\n",
        "* ìƒ˜í”Œ 3: `([5, 6], 0)`\n",
        "\n",
        "## âœ… `dict(X_train)`ì´ í•„ìš”í•œ ê²½ìš°\n",
        "\n",
        "| `X_train` ìë£Œí˜•      | dictë¡œ ë°”ê¿”ì•¼ í•˜ë‚˜? | ì´ìœ                  |\n",
        "| ------------------ | ------------- | ------------------ |\n",
        "| `pandas.DataFrame` | âœ… **í•„ìš”í•¨**     | ì•ˆ ë°”ê¾¸ë©´ ì—´ ì´ë¦„ ì •ë³´ê°€ ì‚¬ë¼ì§ |\n",
        "| `dict`             | âŒ í•„ìš” ì—†ìŒ       | ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ë‹ˆê¹Œ ê·¸ëŒ€ë¡œ ì‚¬ìš©   |\n",
        "| `numpy.ndarray`    | âŒ í•„ìš” ì—†ìŒ       | ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥          |\n",
        "| `Tensor`           | âŒ í•„ìš” ì—†ìŒ       | ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥          |\n",
        "\n",
        "## âœ… ì˜ˆì œ ëª¨ìŒ\n",
        "\n",
        "### 1. `DataFrame`ì¼ ë•Œ â†’ dict í•„ìš”í•¨\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "X_train = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n",
        "y_train = [0, 1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
        "```\n",
        "\n",
        "### 2. `numpy.ndarray`ì¼ ë•Œ\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "X_train = np.array([[1, 3], [2, 4]])\n",
        "y_train = np.array([0, 1])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "```\n",
        "\n",
        "### 3. `dict`ì¼ ë•Œ\n",
        "\n",
        "```python\n",
        "X_train = {'a': [1, 2], 'b': [3, 4]}\n",
        "y_train = [0, 1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "```\n",
        "\n",
        "## âœ… ê·¸ ë‹¤ìŒì€ ë³´í†µ ì´ë ‡ê²Œ ì‚¬ìš©í•¨\n",
        "\n",
        "```python\n",
        "dataset = dataset.shuffle(buffer_size=100).batch(32)\n",
        "model.fit(dataset, epochs=5)\n",
        "```\n",
        "\n",
        "* `.shuffle()`: ë°ì´í„° ì„ê¸°\n",
        "* `.batch()`: ë°°ì¹˜ë¡œ ë¬¶ê¸°\n",
        "* `model.fit()`: í•™ìŠµ ì‹œì‘\n",
        "\n",
        "## ğŸ“Œ í•µì‹¬ ìš”ì•½\n",
        "\n",
        "| ì§ˆë¬¸                        | ë‹µë³€                            |\n",
        "| ------------------------- | ----------------------------- |\n",
        "| `from_tensor_slices`ëŠ” ë­ì•¼? | ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ Dataset ë§Œë“œëŠ” í•¨ìˆ˜     |\n",
        "| ì™œ ì¨?                      | TensorFlow ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹ ë§Œë“¤ê¸°    |\n",
        "| dict(X\\_train)ì€ ì–¸ì œ ì¨?     | `X_train`ì´ DataFrameì´ë©´ ê¼­ ì¨ì•¼ í•¨ |\n",
        "| dict ë§ê³ ë„ ì…ë ¥ ê°€ëŠ¥í•œ í˜•íƒœëŠ”?      | dict, ndarray, tensor ëª¨ë‘ ê°€ëŠ¥   |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VPWJzvfx3Tr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "FNT7LdNjUnC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.data.Dataset.from_tensor_slices() -> ìƒ˜í”Œ í•˜ë‚˜í•˜ë‚˜ë¡œ ë‚˜ëˆ ì„œ tf.data.Dataset ê°ì²´ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(X_train),\n",
        "    y_train\n",
        "))"
      ],
      "metadata": {
        "id": "pkG6nz3ZVLab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(X_test),\n",
        "    y_test\n",
        "))"
      ],
      "metadata": {
        "id": "6JYboQOWiuCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer -> Adam ì„ íƒ\n",
        "# optimizerë¥¼ ì„ íƒí•˜ì§€ ì•Šì„ ê²½ìš° BERT ì¶œë ¥ë§Œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— model(input_ids)ë¡œ ì˜ˆì¸¡ ì‹œ ë‹¨ìˆœ ë²¡í„°ë§Œ ì¶œë ¥ë˜ê³ ,\n",
        "# Dense(num_labels) ê°™ì€ ë¶„ë¥˜ìš© ë ˆì´ì–´ë¥¼ ë³„ë„ë¡œ ì—°ê²°í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n",
        "# ë²„ì „ì„ ë³´ë‹ˆ TensorFlow 2.18.0 + Keras 3.8.0 ì¡°í•©ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œê°€ ìˆìŒ. Keras 3.xëŠ” TensorFlowì™€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë©´ì„œ í˜¸í™˜ì„± ì´ìŠˆê°€ ë°œìƒ\n",
        "# tf.optimizers ì‚¬ìš© (tf.keras.optimizers ì•„ë‹˜)\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5)"
      ],
      "metadata": {
        "id": "KjXOPob5zt85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  PyTorchìš© ëª¨ë¸ ê°€ì¤‘ì¹˜ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë˜ëŠ” PyTorchë¡œ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ TensorFlow ëª¨ë¸ì— ë¶ˆëŸ¬ì˜¤ê³  ì‹¶ë‹¤ë©´, from_pt=Trueë¥¼ ëª…ì‹œí•´ì•¼í•¨\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2, from_pt = True)\n",
        "# hf_compute_loss -> ì´ ë©”ì„œë“œëŠ” ëª¨ë¸ì— ë§ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ê³„ì‚°\n",
        "# hf_compute_loss = ëª¨ë¸ ë‚´ì¥ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©\n",
        "model.compile(optimizer=optimizer, loss=model.hf_compute_loss,metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QG4jVqlRjGO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¡°ê¸° ì¢…ë£Œ ì„¤ì • -> ì—¬ê¸°ì„œë„ ìœ„ì— optimizerê°ì²´ ì‚¬ìš©í• ë•Œë„ ë²„ì „ë¬¸ì œ ë°œìƒìœ¼ë¡œ ë§¨ ìœ—ì¤„ì—\n",
        "# ë ˆê±°ì‹œ ëª¨ë“œ ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ì‹œì \n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "model.fit(\n",
        "    train_dataset.shuffle(10000).batch(32),  # ì—¬ê¸°ì„œ ë°°ì¹˜ ì²˜ë¦¬!\n",
        "    epochs=2,\n",
        "    # batch_size=32,  # ì´ ì¤„ì€ ì œê±° (Datasetì—ì„œëŠ” ë¬´ì‹œë¨)\n",
        "    validation_data=val_dataset.shuffle(10000).batch(32),  # ê²€ì¦ ë°ì´í„°ë„ ë°°ì¹˜ ì²˜ë¦¬\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "fuwiiKSTqJD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‰ê°€\n",
        "model.evaluate(val_dataset.batch(1024))"
      ],
      "metadata": {
        "id": "jypc3OPG5OiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµëœ BERT ëª¨ë¸ì„ 'nsmc_model/bert-base' ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "# save_pretrained()ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ ì„¤ì • íŒŒì¼ì„ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "model.save_pretrained('nsmc_model/bert-base')\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ë„ ê°™ì€ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "# ëª¨ë¸ê³¼ í•¨ê»˜ ë°˜ë“œì‹œ ì €ì¥í•´ì•¼ ë‚˜ì¤‘ì— ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¡œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. -> í˜¹ì‹œë‚˜ ë²„ì „ ì°¨ì´ë¡œ ì¸í•˜ì—¬ ì˜¤ë¥˜ ì¼ìœ¼í‚¬ ìˆ˜ë„ ìˆìŒ ë‚´ê°€ ì•„ê¹Œ ë§Œë‚œ ì˜¤ë¥˜ ë˜ ë§Œë‚  ê°€ëŠ¥ì„±\n",
        "tokenizer.save_pretrained('nsmc_model/bert-base')"
      ],
      "metadata": {
        "id": "H--94orFINfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline"
      ],
      "metadata": {
        "id": "b4TeGcZFNj96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‚¬ì „ì— í•™ìŠµí•´ë‘” í† í¬ë‚˜ì´ì €ë‘ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "loaded_tokenizer = BertTokenizerFast.from_pretrained('nsmc_model/bert-base')\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('nsmc_model/bert-base')"
      ],
      "metadata": {
        "id": "WX1KKAzXSMmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextClassificationPipeline ìƒì„±\n",
        "# ì´ íŒŒì´í”„ë¼ì¸ì€ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ê³¼ì •ì„ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
        "# ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ìŒ ì‘ì—…ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
        "# â‘  í† í°í™” â†’ â‘¡ ëª¨ë¸ ì¶”ë¡  â†’ â‘¢ ê²°ê³¼ ì ìˆ˜ ë°˜í™˜\n",
        "text_classifier = TextClassificationPipeline(\n",
        "    tokenizer = loaded_tokenizer,   # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ë³€í™˜í•  í† í¬ë‚˜ì´ì €\n",
        "    model = loaded_model,           # ì‚¬ì „ í•™ìŠµëœ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸\n",
        "    framework = 'tf',               # TensorFlow ê¸°ë°˜ ëª¨ë¸ì„ì„ ëª…ì‹œ (PyTorchì¼ ê²½ìš° 'pt')\n",
        "    return_all_scores = True        # ëª¨ë“  í´ë˜ìŠ¤(label)ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë°˜í™˜ (Falseì¼ ê²½ìš° ìµœê³  ì ìˆ˜ labelë§Œ ë°˜í™˜)\n",
        ")\n",
        "# ì˜ˆë¥¼ ë“¤ì–´ ì´ì§„ ë¶„ë¥˜ì¼ ê²½ìš° LABEL_0, LABEL_1 ëª¨ë‘ì— ëŒ€í•œ scoreë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "IMC8NYYDSNE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•œ ë¬¸ì¥ì„ í…ìŠ¤íŠ¸ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ì— ë„£ìŠµë‹ˆë‹¤.\n",
        "# return_all_scores=True ì´ë¯€ë¡œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì€ 2ì¤‘ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ì…ë‹ˆë‹¤:\n",
        "# [[{'label': 'LABEL_0', 'score': 0.85}, {'label': 'LABEL_1', 'score': 0.15}]]\n",
        "# â†’ ë°”ê¹¥ ë¦¬ìŠ¤íŠ¸ëŠ” ë¬¸ì¥ ìˆ˜, ì•ˆìª½ ë¦¬ìŠ¤íŠ¸ëŠ” ê° ë ˆì´ë¸”ì˜ ì ìˆ˜\n",
        "result = text_classifier(\"ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„\")\n",
        "\n",
        "# ì²« ë²ˆì§¸ ë¬¸ì¥ì— ëŒ€í•œ ê²°ê³¼ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë¬¸ì¥ 1ê°œë§Œ ë„£ì—ˆìœ¼ë¯€ë¡œ [0]ìœ¼ë¡œ ì¶”ì¶œ)\n",
        "first_result = result[0]\n",
        "\n",
        "print(first_result)\n",
        "# ì¶œë ¥: [{'label': 'LABEL_0', 'score': ...}, {'label': 'LABEL_1', 'score': ...}]"
      ],
      "metadata": {
        "id": "5Dg_ap_3SrXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n0YYeuV0S-mH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPVSDc7fMWC90r4iqTmRhq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}