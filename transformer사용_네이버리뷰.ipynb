{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhwan01/DeepDive/blob/main/transformer%EC%82%AC%EC%9A%A9_%EB%84%A4%EC%9D%B4%EB%B2%84%EB%A6%AC%EB%B7%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 새 셀 첫 번째 줄\n",
        "# TF_USE_LEGACY_KERAS='1' 설정하면 옛날 방식으로 돌아가서 모든 게 정상 작동\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ],
      "metadata": {
        "id": "6QPaLcpw4kE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10SAN25WzjUc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00NVS-B2z3Zw"
      },
      "outputs": [],
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
        "                          filename=\"ratings_train.txt\")\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\",\n",
        "                          filename=\"ratings_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF-KPpR00LMC"
      },
      "outputs": [],
      "source": [
        "test_data=pd.read_table('ratings_test.txt')\n",
        "train_data=pd.read_table('ratings_train.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdd3qNce0lZJ"
      },
      "outputs": [],
      "source": [
        "print('훈련용 리뷰 개수 :', len(train_data))\n",
        "print('테스트용 리뷰 개수 :', len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ71Y8g-0qa_"
      },
      "outputs": [],
      "source": [
        "train_data.drop_duplicates(subset=['document'],inplace=True)\n",
        "train_data=train_data.dropna(how='any')\n",
        "print('훈련 데이터의 리뷰 수 :',len(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1wwpw6p016v"
      },
      "outputs": [],
      "source": [
        "test_data=test_data.dropna(how='any')\n",
        "print('테스트 데이터의 리뷰 수 :',len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHHEja5l1b87"
      },
      "outputs": [],
      "source": [
        "# 사전학습된 토크나이저 불러오기\n",
        "# Hugging Face transformers 라이브러리에서 제공하는,Rust 언어로 구현된 초고속 토크나이저\n",
        "from transformers import BertTokenizerFast\n",
        "tokenizer=BertTokenizerFast.from_pretrained('klue/bert-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4eBaVE51oov"
      },
      "outputs": [],
      "source": [
        "#.values는 DataFrame에서 열을 numpy 배열로 가져오고,\n",
        "#.tolist()는 numpy 배열을 파이썬 리스트로 변환합니다.\n",
        "# 하지만 최신 파이썬에서는 tolist만써도 충분함\n",
        "# 머신러닝·딥러닝 모델, 전처리 도구, 토크나이저 등은 대부분 리스트 또는 배열 형태의 데이터를 입력으로 요구합니다\n",
        "X_train_list = train_data['document'].tolist()\n",
        "X_test_list = test_data['document'].tolist()\n",
        "y_train = train_data['label'].tolist()\n",
        "y_test = test_data['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3pGS71y2b-o"
      },
      "outputs": [],
      "source": [
        "# truncation = True -> Bert의 기본 max_length=512를 초과하는 경우에 뒤를 자른다\n",
        "# padding = True -> 짧은 문장 0으로 패딩\n",
        "X_train = tokenizer(X_train_list, truncation=True, padding=True)\n",
        "X_test = tokenizer(X_test_list, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylbJ_6_v4ZjX"
      },
      "outputs": [],
      "source": [
        "# 샘플에 .tokens를 사용하여 토큰화 결과 확인\n",
        "print(X_train[0].tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZbEDUuM4owK"
      },
      "outputs": [],
      "source": [
        "# 샘플에 .ids를 하면 정수 인코딩 결과 확인\n",
        "print(X_train[0].ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 세그먼트 인코딩 값 확인 모든 위치 값이 0 -> 인식할 문장 종류 1개\n",
        "print(X_train[0].type_ids)"
      ],
      "metadata": {
        "id": "Cgj9tNFNTj9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어텐션 마스크를 통해 실제 단어가 있는 위치와 패딩의 위치를 확인\n",
        "print(X_train[0].attention_mask)"
      ],
      "metadata": {
        "id": "L2Z-M6thUcS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 📘 `tf.data.Dataset.from_tensor_slices()` 총정리\n",
        "\n",
        "## ✅ 개념 요약\n",
        "\n",
        "### 📌 한 줄 설명\n",
        "> 샘플 하나하나로 나눠서 `tf.data.Dataset` 객체를 만들어주는 함수\n",
        "\n",
        "### 🔧 언제 써?\n",
        "TensorFlow 모델(`model.fit()`)에 학습용 데이터를 넣기 위한 Dataset 객체 생성할 때 사용\n",
        "\n",
        "## ✅ 기본 사용법\n",
        "\n",
        "```python\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "````\n",
        "\n",
        "* `X`: 입력 데이터 (넘파이 배열, 텐서, 딕셔너리 등 가능)\n",
        "* `y`: 정답(레이블)\n",
        "\n",
        "## ✅ 작동 방식 (샘플 단위로 쪼개짐)\n",
        "\n",
        "```python\n",
        "X = [[1, 2], [3, 4], [5, 6]]\n",
        "y = [0, 1, 0]\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "```\n",
        "\n",
        "➡️ 위 코드는 아래처럼 분할됨:\n",
        "\n",
        "* 샘플 1: `([1, 2], 0)`\n",
        "* 샘플 2: `([3, 4], 1)`\n",
        "* 샘플 3: `([5, 6], 0)`\n",
        "\n",
        "## ✅ `dict(X_train)`이 필요한 경우\n",
        "\n",
        "| `X_train` 자료형      | dict로 바꿔야 하나? | 이유                 |\n",
        "| ------------------ | ------------- | ------------------ |\n",
        "| `pandas.DataFrame` | ✅ **필요함**     | 안 바꾸면 열 이름 정보가 사라짐 |\n",
        "| `dict`             | ❌ 필요 없음       | 이미 딕셔너리니까 그대로 사용   |\n",
        "| `numpy.ndarray`    | ❌ 필요 없음       | 그대로 사용 가능          |\n",
        "| `Tensor`           | ❌ 필요 없음       | 그대로 사용 가능          |\n",
        "\n",
        "## ✅ 예제 모음\n",
        "\n",
        "### 1. `DataFrame`일 때 → dict 필요함\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "X_train = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n",
        "y_train = [0, 1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
        "```\n",
        "\n",
        "### 2. `numpy.ndarray`일 때\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "X_train = np.array([[1, 3], [2, 4]])\n",
        "y_train = np.array([0, 1])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "```\n",
        "\n",
        "### 3. `dict`일 때\n",
        "\n",
        "```python\n",
        "X_train = {'a': [1, 2], 'b': [3, 4]}\n",
        "y_train = [0, 1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "```\n",
        "\n",
        "## ✅ 그 다음은 보통 이렇게 사용함\n",
        "\n",
        "```python\n",
        "dataset = dataset.shuffle(buffer_size=100).batch(32)\n",
        "model.fit(dataset, epochs=5)\n",
        "```\n",
        "\n",
        "* `.shuffle()`: 데이터 섞기\n",
        "* `.batch()`: 배치로 묶기\n",
        "* `model.fit()`: 학습 시작\n",
        "\n",
        "## 📌 핵심 요약\n",
        "\n",
        "| 질문                        | 답변                            |\n",
        "| ------------------------- | ----------------------------- |\n",
        "| `from_tensor_slices`는 뭐야? | 샘플 단위로 잘라서 Dataset 만드는 함수     |\n",
        "| 왜 써?                      | TensorFlow 모델 학습용 데이터셋 만들기    |\n",
        "| dict(X\\_train)은 언제 써?     | `X_train`이 DataFrame이면 꼭 써야 함 |\n",
        "| dict 말고도 입력 가능한 형태는?      | dict, ndarray, tensor 모두 가능   |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VPWJzvfx3Tr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "FNT7LdNjUnC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.data.Dataset.from_tensor_slices() -> 샘플 하나하나로 나눠서 tf.data.Dataset 객체를 만들어주는 함수.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(X_train),\n",
        "    y_train\n",
        "))"
      ],
      "metadata": {
        "id": "pkG6nz3ZVLab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(X_test),\n",
        "    y_test\n",
        "))"
      ],
      "metadata": {
        "id": "6JYboQOWiuCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer -> Adam 선택\n",
        "# optimizer를 선택하지 않을 경우 BERT 출력만 나오기 때문에 model(input_ids)로 예측 시 단순 벡터만 출력되고,\n",
        "# Dense(num_labels) 같은 분류용 레이어를 별도로 연결해줘야 합니다.\n",
        "# 버전을 보니 TensorFlow 2.18.0 + Keras 3.8.0 조합에서 발생하는 문제가 있음. Keras 3.x는 TensorFlow와 독립적으로 동작하면서 호환성 이슈가 발생\n",
        "# tf.optimizers 사용 (tf.keras.optimizers 아님)\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5)"
      ],
      "metadata": {
        "id": "KjXOPob5zt85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  PyTorch용 모델 가중치만 존재하는 경우 또는 PyTorch로 저장된 체크포인트를 TensorFlow 모델에 불러오고 싶다면, from_pt=True를 명시해야함\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2, from_pt = True)\n",
        "# hf_compute_loss -> 이 메서드는 모델에 맞는 손실 함수를 자동으로 계산\n",
        "# hf_compute_loss = 모델 내장 손실 함수 사용\n",
        "model.compile(optimizer=optimizer, loss=model.hf_compute_loss,metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QG4jVqlRjGO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 조기 종료 설정 -> 여기서도 위에 optimizer객체 사용할때도 버전문제 발생으로 맨 윗줄에\n",
        "# 레거시 모드 사용으로 변경하는 시점\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "model.fit(\n",
        "    train_dataset.shuffle(10000).batch(32),  # 여기서 배치 처리!\n",
        "    epochs=2,\n",
        "    # batch_size=32,  # 이 줄은 제거 (Dataset에서는 무시됨)\n",
        "    validation_data=val_dataset.shuffle(10000).batch(32),  # 검증 데이터도 배치 처리\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "fuwiiKSTqJD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터를 바탕으로 평가\n",
        "model.evaluate(val_dataset.batch(1024))"
      ],
      "metadata": {
        "id": "jypc3OPG5OiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 BERT 모델을 'nsmc_model/bert-base' 경로에 저장합니다.\n",
        "# save_pretrained()는 모델의 가중치와 설정 파일을 함께 저장합니다.\n",
        "model.save_pretrained('nsmc_model/bert-base')\n",
        "\n",
        "# 토크나이저도 같은 경로에 저장합니다.\n",
        "# 모델과 함께 반드시 저장해야 나중에 동일한 토크나이저로 재사용할 수 있습니다. -> 혹시나 버전 차이로 인하여 오류 일으킬 수도 있음 내가 아까 만난 오류 또 만날 가능성\n",
        "tokenizer.save_pretrained('nsmc_model/bert-base')"
      ],
      "metadata": {
        "id": "H--94orFINfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline"
      ],
      "metadata": {
        "id": "b4TeGcZFNj96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전에 학습해둔 토크나이저랑 모델 불러오기\n",
        "loaded_tokenizer = BertTokenizerFast.from_pretrained('nsmc_model/bert-base')\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('nsmc_model/bert-base')"
      ],
      "metadata": {
        "id": "WX1KKAzXSMmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextClassificationPipeline 생성\n",
        "# 이 파이프라인은 텍스트 분류 과정을 간단하게 처리해주는 도구입니다.\n",
        "# 내부적으로 다음 작업을 자동으로 수행합니다:\n",
        "# ① 토큰화 → ② 모델 추론 → ③ 결과 점수 반환\n",
        "text_classifier = TextClassificationPipeline(\n",
        "    tokenizer = loaded_tokenizer,   # 입력 텍스트를 토큰 ID로 변환할 토크나이저\n",
        "    model = loaded_model,           # 사전 학습된 텍스트 분류 모델\n",
        "    framework = 'tf',               # TensorFlow 기반 모델임을 명시 (PyTorch일 경우 'pt')\n",
        "    return_all_scores = True        # 모든 클래스(label)에 대한 점수를 반환 (False일 경우 최고 점수 label만 반환)\n",
        ")\n",
        "# 예를 들어 이진 분류일 경우 LABEL_0, LABEL_1 모두에 대한 score를 반환합니다."
      ],
      "metadata": {
        "id": "IMC8NYYDSNE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한 문장을 텍스트 분류 파이프라인에 넣습니다.\n",
        "# return_all_scores=True 이므로 결과는 다음과 같은 2중 리스트 구조입니다:\n",
        "# [[{'label': 'LABEL_0', 'score': 0.85}, {'label': 'LABEL_1', 'score': 0.15}]]\n",
        "# → 바깥 리스트는 문장 수, 안쪽 리스트는 각 레이블의 점수\n",
        "result = text_classifier(\"뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\")\n",
        "\n",
        "# 첫 번째 문장에 대한 결과만 가져옵니다. (문장 1개만 넣었으므로 [0]으로 추출)\n",
        "first_result = result[0]\n",
        "\n",
        "print(first_result)\n",
        "# 출력: [{'label': 'LABEL_0', 'score': ...}, {'label': 'LABEL_1', 'score': ...}]"
      ],
      "metadata": {
        "id": "5Dg_ap_3SrXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n0YYeuV0S-mH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPVSDc7fMWC90r4iqTmRhq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}