{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPqwK+axWHlCKDJDyGAoSMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhwan01/DeepDive/blob/main/KoBERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EA%B8%B0%EA%B3%84_%EB%8F%85%ED%95%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fF69RpIMarQp"
      },
      "outputs": [],
      "source": [
        "# 이전과 똑같이 레거시 모드로\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import BertTokenizerFast\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "UTHrbgCdbKYs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json -O KorQuAD_v1.0_train.json\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json -O KorQuAD_v1.0_dev.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAA2dB2jbSIQ",
        "outputId": "5c0ed98c-5483-47ab-813c-a96dc727cb69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-23 13:13:07--  https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
            "Resolving korquad.github.io (korquad.github.io)... 185.199.109.153, 185.199.111.153, 185.199.110.153, ...\n",
            "Connecting to korquad.github.io (korquad.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38527475 (37M) [application/json]\n",
            "Saving to: ‘KorQuAD_v1.0_train.json’\n",
            "\n",
            "KorQuAD_v1.0_train. 100%[===================>]  36.74M  59.5MB/s    in 0.6s    \n",
            "\n",
            "2025-07-23 13:13:08 (59.5 MB/s) - ‘KorQuAD_v1.0_train.json’ saved [38527475/38527475]\n",
            "\n",
            "--2025-07-23 13:13:09--  https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
            "Resolving korquad.github.io (korquad.github.io)... 185.199.109.153, 185.199.111.153, 185.199.110.153, ...\n",
            "Connecting to korquad.github.io (korquad.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3881058 (3.7M) [application/json]\n",
            "Saving to: ‘KorQuAD_v1.0_dev.json’\n",
            "\n",
            "KorQuAD_v1.0_dev.js 100%[===================>]   3.70M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-07-23 13:13:09 (53.0 MB/s) - ‘KorQuAD_v1.0_dev.json’ saved [3881058/3881058]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# `open(path, 'rb') as f` 설명\n",
        "\n",
        "## 📌 `open()` 함수 기본 구조\n",
        "\n",
        "```python\n",
        "open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 1. `path` - 파일 경로 지정\n",
        "\n",
        "```python\n",
        "open('data.json', 'rb')             # 상대 경로\n",
        "open('/home/user/data.json', 'rb')  # 절대 경로 (Linux/Mac)\n",
        "open(r'C:\\Users\\data.json', 'rb')   # 절대 경로 (Windows)\n",
        "open(Path('data.json'), 'rb')       # Path 객체 사용\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 2. `'rb'` - 모드 설정 (읽기 + 바이너리)\n",
        "\n",
        "### 📜 파일 모드 정리\n",
        "\n",
        "| 모드     | 설명          | 사용 예시           |\n",
        "| ------ | ----------- | --------------- |\n",
        "| `'r'`  | 텍스트 읽기 (기본) | `.txt`, `.json` |\n",
        "| `'w'`  | 텍스트 쓰기      | 로그 덮어쓰기 등       |\n",
        "| `'a'`  | 텍스트 추가      | 로그 추가           |\n",
        "| `'rb'` | **바이너리 읽기** | 이미지, JSON 등     |\n",
        "| `'wb'` | 바이너리 쓰기     | 이진 파일 생성        |\n",
        "| `'ab'` | 바이너리 추가     | 로그 추가 (이진)      |\n",
        "\n",
        "\n",
        "## 3. `as f` - 파일 객체 변수 할당\n",
        "\n",
        "```python\n",
        "with open(path, 'rb') as f:           # 일반적인 사용\n",
        "with open(path, 'rb') as file:        # file로 이름 지정\n",
        "with open(path, 'rb') as data_file:   # 의미 부여\n",
        "```\n",
        "\n",
        "\n",
        "## 4. 📦 'rb' 모드 자세히 알아보기\n",
        "\n",
        "### ✅ 바이너리 모드란?\n",
        "\n",
        "* **바이트 단위**로 읽기\n",
        "* **인코딩 처리 없이 원본 그대로**\n",
        "* 모든 파일 형식 가능 (텍스트, 이미지, 오디오 등)\n",
        "\n",
        "### 🆚 텍스트 모드 vs 바이너리 모드\n",
        "\n",
        "```python\n",
        "# 텍스트 모드\n",
        "with open('text.txt', 'r') as f:\n",
        "    content = f.read()\n",
        "    print(type(content))  # <class 'str'>\n",
        "\n",
        "# 바이너리 모드\n",
        "with open('text.txt', 'rb') as f:\n",
        "    content = f.read()\n",
        "    print(type(content))  # <class 'bytes'>\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 5. JSON 파일에서 `'rb'` 쓰는 이유\n",
        "\n",
        "### ❌ 문제 상황\n",
        "\n",
        "```python\n",
        "# 기본 인코딩은 플랫폼 따라 달라 오류 발생 가능\n",
        "with open('korean.json', 'r') as f:\n",
        "    data = json.load(f)  # UnicodeDecodeError 발생 가능\n",
        "```\n",
        "\n",
        "### ✅ 안전한 방법들\n",
        "\n",
        "#### 방법 1: 바이너리 모드 (`json.load()`가 자동 처리)\n",
        "\n",
        "```python\n",
        "with open('korean.json', 'rb') as f:\n",
        "    data = json.load(f)\n",
        "```\n",
        "\n",
        "#### 방법 2: 텍스트 모드 + 명시적 인코딩\n",
        "\n",
        "```python\n",
        "with open('korean.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "```\n",
        "\n",
        "\n",
        "## 6. 🧪 실제 예제\n",
        "\n",
        "```python\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "        return squad_dict\n",
        "\n",
        "# 사용 예시\n",
        "data = read_squad('squad_data.json')\n",
        "print(type(data))  # <class 'dict'>\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 7. 파일 객체 `f`의 주요 메서드\n",
        "\n",
        "| 메서드            | 설명              |\n",
        "| -------------- | --------------- |\n",
        "| `f.read()`     | 전체 내용 읽기        |\n",
        "| `f.read(100)`  | 100바이트만 읽기      |\n",
        "| `f.readline()` | 한 줄씩 읽기         |\n",
        "| `f.seek(0)`    | 파일 포인터를 처음으로 이동 |\n",
        "| `f.tell()`     | 현재 포인터 위치 반환    |\n",
        "\n",
        "\n",
        "## ✅ 결론: JSON 읽을 때 추천 방식\n",
        "\n",
        "```python\n",
        "# 방법 1: 바이너리 모드\n",
        "with open(path, 'rb') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# 방법 2: 텍스트 모드 + UTF-8 인코딩\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "eLRiQistexfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_squad(path):\n",
        "  # 입력 받은 경로를 Path 객체로 변환\n",
        "  path = Path(path)\n",
        "  # with 문은 Context Manager를 사용해서 리소스를 안전하게 관리하는 구문\n",
        "  with open(path,'rb') as f:\n",
        "    squad_dict = json.load(f)\n",
        "    # 여기서 자동으로 f.close()가 호출됨!\n",
        "\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  for group in squad_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "      # context = 본문\n",
        "      context = passage['context']\n",
        "      # qa = 질문셋\n",
        "      for qa in passage['qas']:\n",
        "        question = qa['question']\n",
        "        for answer in qa['answers']:\n",
        "          contexts.append(context)\n",
        "          questions.append(question)\n",
        "          answers.append(answer)\n",
        "\n",
        "  return contexts, questions, answers\n",
        "\n",
        "train_contexts, train_questions, train_answers = read_squad('KorQuAD_v1.0_train.json')\n",
        "val_contexts, val_questions, val_answers = read_squad('KorQuAD_v1.0_dev.json')"
      ],
      "metadata": {
        "id": "8KkgrrFDbadG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qj8W8PqHvwpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 본문 개수 :', len(train_contexts))\n",
        "print('훈련 데이터의 질문 개수 :', len(train_questions))\n",
        "print('훈련 데이터의 답변 개수 :', len(train_answers))\n",
        "print('테스트 데이터의 본문 개수 :', len(val_contexts))\n",
        "print('테스트 데이터의 질문 개수 :', len(val_questions))\n",
        "print('테스트 데이터의 답변 개수 :', len(val_answers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou7K8gk2kxUl",
        "outputId": "4cdbdbb3-a27b-4ad1-d83b-02694e03af05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 본문 개수 : 60407\n",
            "훈련 데이터의 질문 개수 : 60407\n",
            "훈련 데이터의 답변 개수 : 60407\n",
            "테스트 데이터의 본문 개수 : 5774\n",
            "테스트 데이터의 질문 개수 : 5774\n",
            "테스트 데이터의 답변 개수 : 5774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('첫 번째 샘플의 본문')\n",
        "print('-'*20)\n",
        "print(train_contexts[0])"
      ],
      "metadata": {
        "id": "ywD1ZF9Qk6Ne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32a1eda-93d6-4b81-8e52-28b459e5241c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 샘플의 본문\n",
            "--------------------\n",
            "1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('첫 번째 샘플의 질문')\n",
        "print('-'*20)\n",
        "print(train_questions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIDwWTKRRi_W",
        "outputId": "05ebda3a-13c3-4986-ba38-fdee0f42d1dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 샘플의 질문\n",
            "--------------------\n",
            "바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# answer_start는 모델이 예측한 정답이 본문 어디서 시작하는지를 알려주는 위치 정보\n",
        "print('첫 번째 샘플의 답변')\n",
        "print('-'*20)\n",
        "print(train_answers[0])\n",
        "# answer_start만 있고 answer_end는 없지만, 일반적으로 이렇게 계산\n",
        "# answer_end = answer_start + len(answer_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9QY1W8tRrLU",
        "outputId": "23c2ec70-bf30-485e-d0f4-0803fd8af670"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 샘플의 답변\n",
            "--------------------\n",
            "{'text': '교향곡', 'answer_start': 54}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 종료 인덱스 추가\n",
        "def add_end_idx(answers, contexts):\n",
        "  for answer, context in zip(answers, contexts):\n",
        "    # answer뒤에 공백이 있으면 제거 -> 위에서 말한 식처럼 연산해야해서 빈칸까지 카운트 되면 문제가 생기기 때문\n",
        "    answer['text'] = answer['text'].rstrip()\n",
        "\n",
        "    # answer_end = answer_start + len(answer_text)\n",
        "    gold_text = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    end_idx = start_idx + len(gold_text)\n",
        "\n",
        "    assert context[start_idx:end_idx] == gold_text, \"end_index 계산에 에러 발생\"\n",
        "\n",
        "    answer['answer_end'] = end_idx\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "metadata": {
        "id": "WjZplQsgRzA5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다시 조회에서 수정 확인\n",
        "print('첫 번째 샘플의 답변')\n",
        "print('-'*20)\n",
        "print(train_answers[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWUGIg5mTvYI",
        "outputId": "4034c4e2-f31a-4252-83b5-6aabbc3fc193"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 샘플의 답변\n",
            "--------------------\n",
            "{'text': '교향곡', 'answer_start': 54, 'answer_end': 57}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제로 인덱스가 맞는지 확인\n",
        "train_contexts[0][54:57]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZcvLWS_WT52Z",
        "outputId": "0d608e46-b892-478d-8751-cffe7a0262c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'교향곡'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 본문과 질문 두 가지를 토크나이저의 입력으로 사용\n",
        "tokenizer = BertTokenizerFast.from_pretrained('klue/bert-base')\n",
        "# 예전에 Hugging Face transformers 라이브러리를 쓸 땐 보통 이렇게 직접 encode_plus()를 썼었다 -> 전에 실습 자료에서 인코딩 과정에서 encode_plus를 썻던 것이 기억나서 작성\n",
        "# 문장쌍 인코딩 과정\n",
        "# 여기서 토크나이저가 model_max_length = 512 이 뒤로 잘리니 정답 라벨이 512 뒤에 있으면 문제 발생\n",
        "# [CLS] 본문 [SEP] 질문 [SEP]\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHjoSPQCUEWR",
        "outputId": "2c87d7dc-1a84-4b5c-cfb5-576f6809bcb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ✅ `BatchEncoding` 구조\n",
        "\n",
        "```markdown\n",
        "BatchEncoding\n",
        "├── input_ids             → List[List[int]]\n",
        "├── attention_mask        → List[List[int]]\n",
        "├── token_type_ids        → List[List[int]] (문장쌍 있을 때만)\n",
        "├── offset_mapping        → List[List[Tuple[int, int]]] (옵션)\n",
        "├── special_tokens_mask   → List[List[int]] (옵션)\n",
        "├── overflow_to_sample_mapping → List[int] (옵션)\n",
        "├── ...                   → 기타 항목들\n",
        "└── [i]                   → EncodingFast 객체로 접근 가능\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### ✅ `EncodingFast` 구조 (`BatchEncoding[i]`)\n",
        "\n",
        "```markdown\n",
        "EncodingFast\n",
        "├── .tokens             → List[str]             # 토크나이저가 생성한 토큰들\n",
        "├── .ids                → List[int]             # 각 토큰의 ID\n",
        "├── .offsets           → List[Tuple[int, int]]  # 원문에서의 문자 위치 (start, end)\n",
        "├── .attention_mask     → List[int]             # 마스크 (패딩=0, 토큰=1)\n",
        "├── .type_ids           → List[int]             # segment ID (문장쌍 구분)\n",
        "└── ...                 → 기타 속성들\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "bVs6fmVRauNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('첫 번째 샘플의 토큰화 결과 :', train_encodings[0].tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j76w5-WsUrim",
        "outputId": "788fe638-334d-4049-edeb-bf90119f877f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 샘플의 토큰화 결과 : ['[CLS]', '183', '##9', '##년', '바그너', '##는', '괴테', '##의', '파우', '##스트', '##을', '처음', '읽', '##고', '그', '내용', '##에', '마음', '##이', '끌려', '이를', '소재', '##로', '해서', '하나', '##의', '교향곡', '##을', '쓰', '##려', '##는', '뜻', '##을', '갖', '##는', '##다', '.', '이', '시기', '바그너', '##는', '183', '##8', '##년', '##에', '빛', '독촉', '##으로', '산전', '##수', '##전', '##을', '다', '[UNK]', '상황', '##이', '##라', '좌절', '##과', '실망', '##에', '가득', '##했', '##으며', '메', '##피스', '##토', '##펠', '##레스', '##를', '만나', '##는', '파우', '##스트', '##의', '심경', '##에', '공감', '##했', '##다고', '한다', '.', '또한', '파리', '##에서', '아', '##브', '##네', '##크', '##의', '지휘', '##로', '파리', '음악', '##원', '관현', '##악단', '##이', '연주', '##하', '##는', '베토벤', '##의', '교향곡', '9', '##번', '##을', '듣', '##고', '깊', '##은', '감명', '##을', '받', '##았', '##는데', ',', '이것', '##이', '이듬해', '1', '##월', '##에', '파우', '##스트', '##의', '서', '##곡', '##으로', '쓰여진', '이', '작품', '##에', '조금', '##이', '##라도', '영향', '##을', '끼쳤', '##으리', '##라는', '것', '##은', '의심', '##할', '여지', '##가', '없', '##다', '.', '여기', '##의', '라', '##단', '##조', '조성', '##의', '경우', '##에도', '그', '##의', '전기', '##에', '적혀', '있', '##는', '것', '##처럼', '단순', '##한', '정신', '##적', '피로', '##나', '실', '##의', '##가', '반영', '##된', '것', '##이', '아니', '##라', '베토벤', '##의', '합창', '##교', '##향', '##곡', '조성', '##의', '영향', '##을', '받', '##은', '것', '##을', '볼', '수', '있', '##다', '.', '그렇게', '교향곡', '작곡', '##을', '183', '##9', '##년', '##부터', '40', '##년', '##에', '걸쳐', '파리', '##에서', '착수', '##했', '##으나', '1', '##악', '##장', '##을', '쓴', '뒤', '##에', '중단', '##했', '##다', '.', '또한', '작품', '##의', '완성', '##과', '동시', '##에', '그', '##는', '이', '서', '##곡', '(', '1', '##악', '##장', ')', '을', '파리', '음악', '##원', '##의', '연주회', '##에서', '연주', '##할', '파트', '##보', '##까', '##지', '준비', '##하', '##였', '##으나', ',', '실제로', '##는', '이루어지', '##지', '##는', '않', '##았', '##다', '.', '결국', '초연', '##은', '4', '##년', '반', '##이', '지난', '후', '##에', '드레스', '##덴', '##에서', '연주', '##되', '##었', '##고', '재연', '##도', '이루', '##어졌', '##지만', ',', '이후', '##에', '그대로', '방치', '##되', '##고', '말', '##았', '##다', '.', '그', '사이', '##에', '그', '##는', '리', '##엔', '##치', '##와', '방황', '##하', '##는', '네덜란드', '##인', '##을', '완성', '##하고', '탄', '##호', '##이', '##저', '##에도', '착수', '##하', '##는', '등', '분주', '##한', '시간', '##을', '보냈', '##는데', ',', '그런', '바쁜', '생활', '##이', '이', '곡', '##을', '잊', '##게', '한', '것', '##이', '아닌가', '하', '##는', '의견', '##도', '있', '##다', '.', '[SEP]', '바그너', '##는', '괴테', '##의', '파우', '##스트', '##를', '읽', '##고', '무엇', '##을', '쓰', '##고', '##자', '했', '##는', '##가', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('첫 번째 샘플의 길이 :', len(train_encodings[0].tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U1YZ2HeVdAw",
        "outputId": "b8c5ee5a-383b-4fd0-d98b-43a56715f597"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 샘플의 길이 : 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('첫 번째 샘플의 정수 인코딩 :', train_encodings[0].ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5_-C6JOW1vW",
        "outputId": "19d9254a-279a-4762-8faa-9bb80bd06acf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 샘플의 정수 인코딩 : [2, 13934, 2236, 2440, 27982, 2259, 21310, 2079, 11994, 3791, 2069, 3790, 1508, 2088, 636, 3800, 2170, 3717, 2052, 9001, 8345, 4642, 2200, 3689, 3657, 2079, 19282, 2069, 1363, 2370, 2259, 936, 2069, 554, 2259, 2062, 18, 1504, 4342, 27982, 2259, 13934, 2196, 2440, 2170, 1195, 23260, 6233, 17370, 2113, 2165, 2069, 809, 1, 3706, 2052, 2181, 8642, 2145, 7334, 2170, 4983, 2371, 4007, 1065, 5917, 2386, 2559, 4443, 2138, 4026, 2259, 11994, 3791, 2079, 15864, 2170, 5487, 2371, 4683, 3605, 18, 3819, 5986, 27135, 1376, 2645, 2203, 2292, 2079, 5872, 2200, 5986, 4152, 2252, 22835, 16706, 2052, 5485, 2205, 2259, 17087, 2079, 19282, 29, 2517, 2069, 881, 2088, 652, 2073, 23404, 2069, 1122, 2886, 13964, 16, 3982, 2052, 9944, 21, 2429, 2170, 11994, 3791, 2079, 1258, 2465, 6233, 24294, 1504, 3967, 2170, 4027, 2052, 5121, 3979, 2069, 18274, 21575, 23548, 575, 2073, 5292, 2085, 7251, 2116, 1415, 2062, 18, 3776, 2079, 942, 2286, 2446, 4196, 2079, 3640, 6509, 636, 2079, 4450, 2170, 10329, 1513, 2259, 575, 7925, 4488, 2470, 4006, 2125, 7874, 2075, 1329, 2079, 2116, 4523, 2897, 575, 2052, 3614, 2181, 17087, 2079, 10044, 2120, 2482, 2465, 4196, 2079, 3979, 2069, 1122, 2073, 575, 2069, 1164, 1295, 1513, 2062, 18, 3914, 19282, 8067, 2069, 13934, 2236, 2440, 3797, 4064, 2440, 2170, 5314, 5986, 27135, 7615, 2371, 4381, 21, 2376, 2121, 2069, 1365, 873, 2170, 4967, 2371, 2062, 18, 3819, 3967, 2079, 4976, 2145, 4213, 2170, 636, 2259, 1504, 1258, 2465, 12, 21, 2376, 2121, 13, 1498, 5986, 4152, 2252, 2079, 16385, 27135, 5485, 2085, 6237, 2178, 2299, 2118, 3864, 2205, 2507, 4381, 16, 4539, 2259, 6268, 2118, 2259, 1380, 2886, 2062, 18, 3983, 16087, 2073, 24, 2440, 1121, 2052, 3625, 1943, 2170, 9605, 2906, 27135, 5485, 2496, 2359, 2088, 17807, 2119, 4046, 6028, 3683, 16, 3719, 2170, 4311, 8095, 2496, 2088, 1041, 2886, 2062, 18, 636, 3734, 2170, 636, 2259, 1028, 2614, 2225, 2522, 14231, 2205, 2259, 8445, 2179, 2069, 4976, 19521, 1763, 2016, 2052, 2190, 6509, 7615, 2205, 2259, 886, 12100, 2470, 3641, 2069, 5755, 13964, 16, 3637, 9064, 3799, 2052, 1504, 595, 2069, 1515, 2318, 1891, 575, 2052, 5215, 1889, 2259, 4192, 2119, 1513, 2062, 18, 3, 27982, 2259, 21310, 2079, 11994, 3791, 2138, 1508, 2088, 3890, 2069, 1363, 2088, 2155, 1902, 2259, 2116, 35, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('첫 번째 샘플의 토큰화 결과 :', train_encodings[0].attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8wLfJAXW91-",
        "outputId": "95eab3a4-40e5-4988-859f-a22ff98c9a47"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 샘플의 토큰화 결과 : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    deleting_list = []\n",
        "\n",
        "    for i in tqdm(range(len(answers))):\n",
        "        # 토큰화 전 문자에서 인덱스(start,end)가 토큰화 후 달라질 것이기 때문에 찾아내야한다\n",
        "        # char_to_token은 주로 자연어처리(NLP)에서 사용되는 개념으로, 문자(character) 위치를 토큰(token) 위치로 변환하는 기능\n",
        "        # char_to_token(i, char_pos)\n",
        "        # i: 배치 내 문장 인덱스\n",
        "        # char_pos: 원본 텍스트에서 문자의 인덱스 (정수)\n",
        "        # 반환값: 해당 문자가 속한 토큰의 인덱스 (정수 또는 None)\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        # 파이썬 슬라이싱 규칙 - 1 해주기\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "\n",
        "        # 시작 인덱스가 비정상인 경우 -> 본문에 정답이 없는 경우\n",
        "        if start_positions[-1] is None:\n",
        "            # model_max_length -> 해당 토크나이저가 사용하는 모델의 최대 토큰 길이\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "            deleting_list.append(i)\n",
        "\n",
        "        # 종료 인덱스가 비정상인 경우 -> 본문에 정답이 없는 경우\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "            if i not in deleting_list:\n",
        "              deleting_list.append(i)\n",
        "\n",
        "    # update 메서드(dict의 메서드) -> 기존의 BatchEncoding에 새로운 키-값 쌍을 추가할 때 사용\n",
        "    # BatchEncoding은 transformers에서 제공하는 객체이지만, 내부적으로 딕셔너리처럼 다룰 수 있도록 설계돼 있어 update()를 사용할 수 있습니다.\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "    return deleting_list"
      ],
      "metadata": {
        "id": "HCDYucUHyQew"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleting_list_for_train = add_token_positions(train_encodings, train_answers)\n",
        "deleting_list_for_test= add_token_positions(val_encodings, val_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYLm-TLx5TtW",
        "outputId": "1ea0b9e6-3629-48c2-dfd4-b62e6ed73ab8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60407/60407 [00:00<00:00, 331378.00it/s]\n",
            "100%|██████████| 5774/5774 [00:00<00:00, 332198.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('삭제 예정인 훈련 샘플 :', deleting_list_for_train)\n",
        "print('삭제 예정인 테스트 샘플 :', deleting_list_for_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmHth6Pu9CRE",
        "outputId": "f939ad55-b4dd-46e0-b904-0757d10dd449"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "삭제 예정인 훈련 샘플 : [711, 726, 728, 729, 761, 765, 767, 768, 805, 2586, 2587, 2722, 2724, 2725, 2731, 3392, 3475, 3478, 3491, 3495, 3498, 3919, 4462, 4465, 4513, 4515, 4565, 4765, 4766, 4772, 4774, 4779, 5334, 6603, 6638, 6639, 6748, 6749, 6750, 6765, 6766, 6771, 6776, 6897, 6898, 6900, 7739, 7741, 9203, 9211, 10880, 11039, 11212, 11727, 11776, 11788, 11789, 11791, 12168, 13708, 13711, 13996, 14460, 14461, 14491, 14724, 14729, 14885, 15764, 15970, 15971, 15973, 15974, 15976, 15977, 15979, 15980, 16080, 17683, 17815, 17828, 18389, 18392, 19045, 19052, 19053, 19195, 19636, 19637, 19638, 19640, 19656, 19761, 19764, 19765, 20614, 20618, 21224, 21243, 21334, 21335, 21338, 21361, 21521, 21522, 22627, 22633, 24003, 24577, 24579, 24580, 24768, 25108, 25176, 25182, 25185, 25186, 25187, 25188, 25448, 25451, 25454, 25457, 25460, 27105, 27112, 27113, 27114, 27159, 27293, 27295, 27555, 27558, 28025, 28438, 28779, 29162, 29189, 29289, 29290, 29855, 31889, 31890, 31891, 31894, 31905, 32050, 32051, 32057, 32058, 32059, 32088, 32431, 32436, 32477, 33134, 33135, 33293, 33294, 33772, 33844, 34200, 34642, 34644, 35186, 35187, 35318, 36009, 36043, 36076, 36643, 36646, 36648, 36980, 36984, 37081, 37146, 37534, 37716, 37902, 37907, 37967, 38290, 38296, 38422, 38506, 39766, 39767, 40187, 40188, 40195, 40198, 41758, 42248, 42251, 42770, 43928, 44009, 44010, 44013, 44014, 44061, 44638, 44639, 44642, 45011, 45013, 45014, 45025, 45087, 45090, 45093, 45102, 45104, 45105, 45108, 45446, 46436, 47195, 47583, 47860, 47861, 48407, 48411, 48417, 48420, 48428, 48429, 48445, 48466, 48817, 48819, 49383, 50625, 50655, 50659, 50661, 50685, 51994, 52580, 52796, 53271, 54120, 54260, 54343, 54530, 54534, 56782, 56819, 56889, 56895, 57263, 58074, 58076, 58535, 59211, 59212, 59553, 59724, 59806, 59873, 59875, 59877, 59908, 59916, 59920, 59998]\n",
            "삭제 예정인 테스트 샘플 : [77, 583, 586, 881, 883, 953, 1090, 1093, 1097, 2572, 2576, 2876, 2879, 2882, 2979, 3054, 3502, 3505, 3507, 3509, 3512, 3515, 3529, 3530, 3533, 3535, 3538, 3544, 3546, 3586, 3589, 3590, 3591, 3645, 3871, 4246, 4292, 4295, 4296, 4380, 5130, 5134, 5139, 5186, 5189, 5191, 5193, 5194, 5195, 5196, 5599, 5600, 5603, 5604, 5605, 5669, 5672]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기서 정답이 있지만 삭제 예정 샘플에 찍히는 이유는 512에서 자르기 때문에 정답 라벨이 그 뒤에 있기 때문\n",
        "print('761번 샘플의 질문 :', train_questions[761])\n",
        "print('*'*80)\n",
        "print('761번 샘플의 본문 :', train_contexts[761])\n",
        "print('*'*80)\n",
        "print('761번 샘플의 정답 :', train_answers[761])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w_4TKBY9Irq",
        "outputId": "a5b30d1f-7674-403d-cdc0-61b18ec09009"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "761번 샘플의 질문 : 소치 팀추월 파이널D에서 여자 팀추월 대표팀의 최종 성적은?\n",
            "********************************************************************************\n",
            "761번 샘플의 본문 : 2014년 2월 9일 러시아 소치 아들레르 아레나서 열린 소치 동계올림픽 3000m 부문에서 김보름(21)은 4분12초08의 기록으로로 13위를 차지했다. 이날 3조로 경기를 치른 김보름은 21초05로 200m 구간을 통과한 후 2분31초34로 1800m 구간을 지났다. 이후 2200m 구간 통과 순간부터 스피드를 올리며 후반 들어 스퍼트를 올렸고, 결국 하위권이 아닌 중위권 기록을 남겼다. 김보름의 순위인 13위는 지난 2006 토리노 올림픽, 2010 밴쿠버 올림픽 당시 노선영(25-강원도청)이 기록한 19위를 넘어 한국 여자 3000m 부문의 가장 높은 순위다. 5조의 노선영은 4분19초02를 기록했다. 노선영은 200m 구간에서 21초32의 기록으로 지난 이후 속도를 올리지 못한 채로 결승점을 통과했다. 결국 노선영은 전체 26위의 성적을 남기며 경기를 마쳤다. 6조에서 경기를 소화한 양신영(24-전북도청)은 4분23초67을 기록해 이날 대회를 뛴 28명 중 최저의 기록을 남겼다. 한편 이날 대회는 4분00초34의 이레네 부스트(네덜란드)가 2006년 토리노 동계올림픽 이후 8년 만에 금메달을 다시 가져갔다. 대회 2연패를 노린 2위 마르티나 사블리코바(체코-4분01초95)와 3위 올가 그라프(러시아-4분03초47)에 앞선 기록이다. 16일 열린 1500m에서는 네덜란드의 요리엔 테르모르스가 1분53초51의 올림픽 기록으로 금메달을 차지했다. 은메달과 동메달도 네덜란드 선수들이 휩쓸었다. 은메달은 이레인 뷔스트(1분54초09)에게 돌아갔고 동메달은 하를로터 판바이크(1분54초54)가 주인이 됐다. 심지어 4위도 네덜란드 선수인 마리트 리엔스트라(1분56초40)가 차지했다. 김보름은 1분59초78로 21위에 올랐다. 노선영(25-강원도청)은 2분01초07로 29위, 양신영(24-전북도청)은 2분04초13으로 최하위인 36위에 그쳤다. 김보름은 무릎 통증이 있어 5000m를 기권했다. 22일 열린 팀추월 파이널D(7-8위 결정전)에서 김보름, 노선영(25, 강원도청), 양신영(24, 전북도청)이 나란히 달린 여자 팀추월 대표팀은 노르웨이에 져 8위로 대회를 마쳤다. 전날 8강전에서 일본에 져 준결승 진출에 실패한 여자 대표팀은 이날 3분11초54만에 마지막 주자가 결승선을 통과해 3분8초35를 기록한 노르웨이에 패했다.\n",
            "********************************************************************************\n",
            "761번 샘플의 정답 : {'text': '8위', 'answer_start': 1050, 'answer_end': 1052}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정답 라벨이 있는지 확인하기 위해서 정수 인코딩 한걸 다시 되돌려서 확인(761번 샘플)\n",
        "print('761번 샘플 전처리 후 :', tokenizer.decode(train_encodings['input_ids'][761]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnPtNinv9o1_",
        "outputId": "1c301778-2502-430c-b462-51b459eeff61"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "761번 샘플 전처리 후 : [CLS] 2014년 2월 9일 러시아 소치 아들레르 아레나서 열린 소치 동계올림픽 3000m 부문에서 김보름 ( 21 ) 은 4분12초08의 기록으로로 13위를 차지했다. 이날 3조로 경기를 치른 김보름은 21초05로 200m 구간을 통과한 후 2분31초34로 1800m 구간을 지났다. 이후 2200m 구간 통과 순간부터 스피드를 올리며 후반 들어 스퍼트를 올렸고, 결국 하위권이 아닌 중위권 기록을 남겼다. 김보름의 순위인 13위는 지난 2006 토리노 올림픽, 2010 밴쿠버 올림픽 당시 노선영 ( 25 - 강원도청 ) 이 기록한 19위를 넘어 한국 여자 3000m 부문의 가장 높은 순위다. 5조의 노선영은 4분19초02를 기록했다. 노선영은 200m 구간에서 21초32의 기록으로 지난 이후 속도를 올리지 못한 채로 결승점을 통과했다. 결국 노선영은 전체 26위의 성적을 남기며 경기를 마쳤다. 6조에서 경기를 소화한 양신영 ( 24 - 전북도청 ) 은 4분23초67을 기록해 이날 대회를 뛴 28명 중 최저의 기록을 남겼다. 한편 이날 대회는 4분00초34의 이레네 부스트 ( 네덜란드 ) 가 2006년 토리노 동계올림픽 이후 8년 만에 금메달을 다시 가져갔다. 대회 2연패를 노린 2위 마르티나 사블리코바 ( 체코 - 4분01초95 ) 와 3위 올가 그라프 ( 러시아 - 4분03초47 ) 에 앞선 기록이다. 16일 열린 1500m에서는 네덜란드의 요리엔 테르모르스가 1분53초51의 올림픽 기록으로 금메달을 차지했다. 은메달과 동메달도 네덜란드 선수들이 휩쓸었다. 은메달은 이레인 뷔스트 ( 1분54초09 ) 에게 돌아갔고 동메달은 하를로터 판바이크 ( 1분54초54 ) 가 주인이 됐다. 심지어 4위도 네덜란드 선수인 마리트 리엔스트라 ( 1분56초40 ) 가 차지했다. 김보름은 1분59초78로 21위에 올랐다. 노선영 ( 25 - 강원도청 ) 은 2분01초07로 29위, 양신영 ( 24 - 전북도청 ) [SEP] 소치 팀추월 파이널D에서 여자 팀추월 대표팀의 최종 성적은? [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 문제가 있는 샘플들은 제거\n",
        "def delete_samples(encodings, deleting_list):\n",
        "  # 정수인코딩된 데이터들을 numpy 배열로 바꾸고 -> deleting_list(삭제할 행의 인덱스를 답고 있는 리스트)에 있는 행들을 지운다\n",
        "  # np.delete(arr, obj, axis=None)\n",
        "  # arr\t삭제할 대상이 되는 배열\n",
        "  # obj\t삭제할 인덱스 또는 인덱스 리스트\n",
        "  # axis\t삭제할 축: 0이면 행(row), 1이면 열(column) / 생략시 1차원으로 펼쳐진 상태에서 삭제\n",
        "  input_ids = np.delete(np.array(encodings['input_ids']), deleting_list, axis=0)\n",
        "  # 똑같이 나머지 값들도\n",
        "  attention_masks = np.delete(np.array(encodings['attention_mask']), deleting_list, axis=0)\n",
        "  start_positions = np.delete(np.array(encodings['start_positions']), deleting_list, axis=0)\n",
        "  end_positions = np.delete(np.array(encodings['end_positions']), deleting_list, axis=0)\n",
        "\n",
        "  X_data = [input_ids, attention_masks]\n",
        "  y_data = [start_positions, end_positions]\n",
        "\n",
        "  return X_data, y_data"
      ],
      "metadata": {
        "id": "f6Lm7QxZMuIc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정제 -> max_length 초과 샘플들 삭제\n",
        "X_train, y_train = delete_samples(train_encodings, deleting_list_for_train)\n",
        "X_test, y_test = delete_samples(val_encodings,deleting_list_for_test)"
      ],
      "metadata": {
        "id": "6PwjVU9oUEup"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('------------삭제 전------------')\n",
        "print('훈련 데이터의 샘플의 개수 :', len(train_contexts))\n",
        "print('테스트 데이터의 샘플의 개수 :', len(val_contexts))\n",
        "\n",
        "print('------------삭제 후------------')\n",
        "print('훈련 데이터의 샘플의 개수 :', len(X_train[0]))\n",
        "print('테스트 데이터의 샘플의 개수 :', len(X_test[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxvkfLBCUwVG",
        "outputId": "9b042d27-2bda-4794-81d0-0fecf9cd833a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------삭제 전------------\n",
            "훈련 데이터의 샘플의 개수 : 60407\n",
            "테스트 데이터의 샘플의 개수 : 5774\n",
            "------------삭제 후------------\n",
            "훈련 데이터의 샘플의 개수 : 60140\n",
            "테스트 데이터의 샘플의 개수 : 5717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존의 모델 정의 했던 모델 변형\n",
        "from transformers import TFBertModel\n",
        "\n",
        "class TFBertForQuestionAnswering(tf.keras.Model):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
        "        # 최종결과 768(Bert의 히든) -> 2(시작 점수, 종료 점수)\n",
        "        self.qa_outputs = tf.keras.layers.Dense(2,\n",
        "                                               kernel_initializer=tf.keras.\n",
        "                                               initializers.TruncatedNormal\n",
        "                                               (0.02),\n",
        "                                               name='qa_outputs')\n",
        "        self.softmax = tf.keras.layers.Activation(tf.keras.activations.softmax)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # BERT가 토큰마다 점수를 출력하고, squeeze로 불필요한 1차원 제거 후, softmax로 확률로 바꿔서 어느 토큰이 정답인지 판단한다.\n",
        "        # BERT의 마지막 층의 모든 토큰들\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        # 768 -> 2\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "\n",
        "        # 사용할 출력층은 총 2개, 각각 시작 인덱스 예측과 종료 인덱스 예측에 사용\n",
        "        # axis=-1이므로 마지막 차원을 기준으로 두 개의 텐서로 나눔\n",
        "        # 결과:\n",
        "        # start_logits.shape == (batch_size, seq_len, 1)\n",
        "        # end_logits.shape == (batch_size, seq_len, 1)\n",
        "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
        "\n",
        "        # start_logits = (batch_size, sequence_length,)\n",
        "        # end_logits = (batch_size, sequence_length,)\n",
        "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
        "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
        "\n",
        "        # 시작 인덱스에 대한 다음 클래스 분류 문제\n",
        "        start_probs = self.softmax(start_logits)\n",
        "\n",
        "        # 종료 인덱스에 대한 다중 클래스 분류 문제\n",
        "        end_probs = self.softmax(end_logits)\n",
        "\n",
        "        return start_probs, end_probs"
      ],
      "metadata": {
        "id": "pKwaNMR7dx9S"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성 및 컴파일\n",
        "model = TFBertForQuestionAnswering(\"klue/bert-base\")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "# softmax를 통과해서 왔으니 from_logits=False\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "# loss=[loss, loss]는 모델 출력이 2개일 때, 각 출력에 대응되는 loss를 지정해주는 것\n",
        "model.compile(optimizer=optimizer, loss=[loss, loss])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs2tW_7RlNru",
        "outputId": "172b489f-da6d-4cde-c894-e1f685d51895"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=3,\n",
        "    verbose=1,\n",
        "    batch_size=16,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfYHI5VnnM22",
        "outputId": "04ce5426-9b67-4af5-8b51-23ad78d80b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test_data_by_idx(idx):\n",
        "  # 예시) \"나는 사과를 좋아해요. [SEP] 무엇을 좋아하나요?\"\n",
        "  # split -> [\"나는 사과를 좋아해요.\", \"무엇을 좋아하나요?\"]\n",
        "  # 0 -> 본문\n",
        "  # 1 -> 질문\n",
        "  context = tokenizer.decode(X_test[0][idx]).split('[SEP] ')[0]\n",
        "  question = tokenizer.decode(X_test[0][idx]).split('[SEP] ')[1]\n",
        "  print('본문 :', context)\n",
        "  print('질문 :', question)\n",
        "  # 파이썬 슬라이싱에 의해 뺏던 1다시 더하기\n",
        "  # 정답 인코딩을 슬라이싱으로 가져온다\n",
        "  answer_encoded = X_test[0][idx][y_test[0][idx]:y_test[1][idx]+1]\n",
        "  print('정답 :',tokenizer.decode(answer_encoded))\n",
        "  # tf.constant(...) -> 리스트/넘파이 배열 -> Tensorflow Tensor로 변환\n",
        "  # 차원을 하나 늘려서 배치(batch) 차원을 추가하는 표현\n",
        "  # 예시)\n",
        "  # tf.constant([101, 1234, 3456, 102, 0, 0, 0]).shape\n",
        "  # (7,) ← 단일 문장, 1D 텐서\n",
        "  # tf.constant(...)[None, :].shape\n",
        "  # (1, 7) ← 배치 크기 1인 2D 텐서로 바뀜\n",
        "  # 왜 차원을 늘려야 할까?\n",
        "  # TensorFlow 모델은 입력을 항상 (batch_size, sequence_length) 형태로 기대하기 때문\n",
        "  output = model([tf.constant(X_test[0][idx])[None, :], tf.constant(X_test[1][idx])[None, :]])\n",
        "  # squeeze는 텐서의 불필요한 차원(크기가 1인 차원)을 제거하는 함수\n",
        "  # softmax를 통과하고 나온게 각각의 결과 -> start_probs = [0], end_probs = [1] -> shape = (batch_size, sequence_length,)\n",
        "  # 여기서 squeeze를 통해 sequencelength 만 뽑아낸다.\n",
        "  # 그 중 제일 값이 큰 것을 시작 토큰/ 종료 토큰으로 각각 받아온다\n",
        "  # 여기서 end 에서의 +1은 파이썬 슬라이싱 때문이다\n",
        "  start = tf.math.argmax(tf.squeeze(output[0]))\n",
        "  end = tf.math.argmax(tf.squeeze(output[1]))+1\n",
        "  # 이 start랑 end 값이 정수 인코딩된 값 기준이기 때문에 -> 그 값 기준으로 해당하는 라벨 토큰을 가져오고 다시 decode하는 과정을 거친다.\n",
        "  answer_encoded = X_test[0][idx][start:end]\n",
        "  print('예측 :',tokenizer.decode(answer_encoded))\n",
        "  print('----------------------------------------')"
      ],
      "metadata": {
        "id": "tGDjjRspp19p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 100):\n",
        "  predict_test_data_by_idx(i)"
      ],
      "metadata": {
        "id": "5g9ndKwBXAzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}