{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1YhT33M1wfeYDgCp8-22YvdzJGjtJeyof",
      "authorship_tag": "ABX9TyMepYYmC8K0dFarutvYwehx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhwan01/DeepDive/blob/main/Bert_%EB%84%A4%EC%9D%B4%EB%B2%84_%EC%98%81%ED%99%94_%EB%A6%AC%EB%B7%B0_%EB%B6%84%EB%A5%98(%EC%A7%81%EC%A0%91_%EA%B5%AC%ED%98%84).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NGG1dOyYcWj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel"
      ],
      "metadata": {
        "id": "zX03Gf0rkcoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네이버 영화 리뷰 데이터 학습을 위해 훈련 데이터와 테스트 데이터를 다운로드합니다.\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ],
      "metadata": {
        "id": "XahSFz2dlUtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')"
      ],
      "metadata": {
        "id": "irTltITmoU_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
        "print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "ExoeptgBoej8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.drop_duplicates(subset=['document'], inplace = True)\n",
        "train_data = train_data.dropna(how = 'any')\n",
        "print('훈련 데이터의 리뷰 수:',len(train_data))"
      ],
      "metadata": {
        "id": "di5y5Qi1osqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.dropna(how = 'any')\n",
        "print('테스트 데이터의 리뷰 수:',len(test_data))"
      ],
      "metadata": {
        "id": "8bTIDY4upKkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nnRs0eI_pa_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')"
      ],
      "metadata": {
        "id": "anNGPHrDpatX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
      ],
      "metadata": {
        "id": "e7yxdK3lr2oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
      ],
      "metadata": {
        "id": "gb-YdEJisH_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코딩하면서 특수 토큰이 들어가고 다시 디코딩 하니 특수 토큰이 기존의 문장에서 추가되서 나옴(문장 복원 과정)\n",
        "tokenizer.decode(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
      ],
      "metadata": {
        "id": "p46snFGCsSGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수 토큰 이름과 ID 출력\n",
        "print(tokenizer.cls_token,':', tokenizer.cls_token_id)\n",
        "print(tokenizer.sep_token,':', tokenizer.sep_token_id)\n",
        "print(tokenizer.pad_token,':', tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "cZgEUYcishrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "\n",
        "# 버전 차이로 인한 함수 변화\n",
        "# encoded_result = tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\",max_seq_length=max_seq_length, pad_to_max_length=True)\n",
        "# max_sequence_length -> max_length // pad_to_max_length=True -> padding = 'max_length'\n",
        "encoded_result = tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\",max_length = max_length, padding='max_length')\n",
        "print(encoded_result)\n",
        "print('길이 :', len(encoded_result))"
      ],
      "metadata": {
        "id": "DV4bz_QTvYVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([0]*max_length)"
      ],
      "metadata": {
        "id": "EgTNiPEzv3he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어텐션 마스크 인코딩\n",
        "valid_num = len(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n",
        "print(valid_num * [1] + (max_length - valid_num) * [0])"
      ],
      "metadata": {
        "id": "uOWGPGYrxT5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples, labels, max_length, tokenizer):\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
        "    # tqdm은 여기서 진행률 표시줄을 추가하기 위해서 사용한다\n",
        "    # (examples, labels)로 묶고, examples의 길이만큼 반복한다 그걸 zip 객체를 감싸 진행바 표시\n",
        "    # !!! truncation=True 옵션 추가 128 넘으면 자동으로 잘라 -> 원본 코드 돌려보니 128을 넘는 문장이 있기 때문\n",
        "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
        "        input_id = tokenizer.encode(example, max_length=max_length, padding='max_length',truncation=True)\n",
        "\n",
        "        # 단어 위치하면 1, 패딩은 0으로\n",
        "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
        "        attention_mask = [1] * (max_length - padding_count) + [0] * padding_count\n",
        "\n",
        "        # 세그먼트 인코딩 = 입력 [CLS] 나는 학생이다 [SEP] 너는 선생님이다 [SEP] ->\n",
        "        #            Segment ID:   0     0     0       0    1       1        1\n",
        "        token_type_id = [0] * max_length\n",
        "\n",
        "        # assert -> 조건 만족하지 않으면 에러 발생시키는 디버킹 도구\n",
        "        # input_id의 길이가 최대 시퀀스 길이와 같은지 확인\n",
        "        # 만약 다르면 에러 메시지와 함께 실제 길이와 기대 길이를 출력\n",
        "        assert len(input_id) == max_length, \"Error with input length {} vs {}\".format(len(input_id), max_length)\n",
        "\n",
        "        # attention_mask의 길이가 최대 시퀀스 길이와 같은지 확인\n",
        "        # attention_mask는 어떤 토큰이 실제 데이터이고 어떤 토큰이 패딩인지 표시\n",
        "        assert len(attention_mask) == max_length, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_length)\n",
        "\n",
        "        # token_type_id의 길이가 최대 시퀀스 길이와 같은지 확인\n",
        "        # token_type_id는 BERT에서 첫 번째/두 번째 문장을 구분하는 데 사용 (0 또는 1)\n",
        "        assert len(token_type_id) == max_length, \"Error with token type length {} vs {}\".format(len(token_type_id), max_length)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(label)\n",
        "\n",
        "    # 리스트를 그대로 쓰면 파이썬 내장 자료구조라 연산 느리고 벡터연산도 불가능\n",
        "    # -> 리스트+리스트 연결만되기 때문\n",
        "    # 따라서 배열끼리 더하고 곱하는 것도 자연스러운 numpy 배열로 바꿔서 숫자를 효율적으로 처리하려고 변환을 한다\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "    # 라벨을 정수 배열로 변환해서 분류 문제에 적합하게 만듬\n",
        "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), data_labels\n"
      ],
      "metadata": {
        "id": "V_PUGL-IxoDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_y = convert_examples_to_features(train_data['document'],train_data['label'],max_length=max_length,tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "rUtuwPFz0M-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_X, test_y = convert_examples_to_features(test_data['document'],test_data['label'],max_length=max_length,tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "FBee6q0PknaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_id = train_X[0][0]\n",
        "attention_mask = train_X[1][0]\n",
        "token_type_id = train_X[2][0]\n",
        "label = train_y[0]\n",
        "\n",
        "print('단어에 대한 정수 인코딩 :', input_id)\n",
        "print('어텐션 마스크 :', attention_mask)\n",
        "print('세그먼트 인코딩 :', token_type_id)\n",
        "print('각 인코딩의 길이 :', len(input_id))\n",
        "print('정수 인코딩 복원 :', tokenizer.decode(input_id))\n",
        "print('레이블 :', label)\n"
      ],
      "metadata": {
        "id": "4zFiK3OPm1Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch 형식으로 저장된 'klue/bert-base' 모델을\n",
        "# TensorFlow 형식으로 변환해서 불러오기 위해 from_pt=True 옵션 사용\n",
        "model = TFBertModel.from_pretrained(\"klue/bert-base\",from_pt=True)"
      ],
      "metadata": {
        "id": "vrMHSHAZngcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "\n",
        "# 입력 토큰 ID들을 받는 Input 레이어 (배열 길이는 max_length, 데이터 타입은 32비트 정수)\n",
        "input_ids_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
        "\n",
        "# 어텐션 마스크를 받는 Input 레이어 (패딩 등에서 어느 토큰에 집중할지 표시, max_length 길이, int32)\n",
        "attention_masks_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
        "\n",
        "# 토큰 타입 ID를 받는 Input 레이어 (문장 A, B 구분용, max_length 길이, int32)\n",
        "token_type_ids_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
        "\n",
        "# BERT 모델에 정석 방식으로 입력-> 원본 입력은 첫번째 입력이 [input_ids_layer]로 감싸져 있어야함\n",
        "outputs = model(\n",
        "    input_ids=input_ids_layer,\n",
        "    attention_mask=attention_masks_layer,\n",
        "    token_type_ids=token_type_ids_layer\n",
        ")"
      ],
      "metadata": {
        "id": "74M-Px43n2TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 토큰의 출력 shape: (batch_size, sequence_length, hidden_size) → 예: (32, 128, 768)\n",
        "# 보면 128개의 출력이 나오는게 된다 -> 각출력 768차원 벡터 -> many to many\n",
        "print(outputs[0])"
      ],
      "metadata": {
        "id": "ba_DD8fJo_cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 768차원짜리 출력 하나 나옴 -> many to one이 된다.\n",
        "print(outputs[1])"
      ],
      "metadata": {
        "id": "Y6z6bNp-WFFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.model을 상속받아 사용 -> 상속받은 클래스의 compile,fit 같은 메소드를 사용할 수 있다.\n",
        "class TFBertForSequenceClassification(tf.keras.Model):\n",
        "    def __init__(self, model_name):\n",
        "        # 파이썬3부터는 이렇게 간추려 쓸 수 있음 super(TFBertForSequenceClassification, self).__init__() -> super().__init__()\n",
        "        super().__init__()\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
        "        # 출력층을 하나 만들어서 classifier에 저장\n",
        "        # 출력 뉴런 하나로 이진 분류 예측\n",
        "        self.classifier = tf.keras.layers.Dense(1,\n",
        "                                                # 가중치 초기화 방식\n",
        "                                                # 정규분포에서 값을 뽑되 극단값은 버리는 버전\n",
        "                                                # BERT 논문에 따르면, 모든 Dense 레이어는 TruncatedNormal(std=0.02)로 초기화해야 학습이 안정적이기 때문\n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
        "                                                # 이진분류 활성화 함수\n",
        "                                                activation='sigmoid',\n",
        "                                                name='classifier')\n",
        "    def call(self, inputs):\n",
        "        # 입력 받은 3개의 튜플 요소\n",
        "        input_ids, attention_mask, token_type_ids = inputs\n",
        "        # 모델에 입력값 전달\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        # 이진분류 값을 받을거니 [1]번 선택\n",
        "        # 문장의 대표 벡터인 [CLS] 토큰의 벡터 가져오기\n",
        "        cls_token = outputs[1]\n",
        "        # [CLS] 벡터를 Dense 레이어에 넣어서 최종 결과 예측\n",
        "        # sigmoid이기 때문에 0~1사이의 확률\n",
        "        prediction = self.classifier(cls_token)\n",
        "\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "qVHn5bjpHNia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "6cEtAZXmH-3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(5e-5)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PulRVVl3YJwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_X, train_y, epochs=2, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "zjMUHU7amrja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_X,test_y, batch_size=1024)\n",
        "print(\"test loss, test acc: \", results)"
      ],
      "metadata": {
        "id": "vX98-4qIFwuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  input_id = tokenizer.encode(new_sentence, max_length=max_length, padding='max_length',truncation=True)\n",
        "  padding_count = input_id.count(tokenizer.pad_token_id)\n",
        "  attention_mask = [1]*(max_length-padding_count) + [0]*padding_count\n",
        "  token_type_id = [0]*max_length\n",
        "\n",
        "  # 파이썬 리스트 구조 -> 처리 빠른 넘파이 배열 구조로\n",
        "  input_ids = np.array([input_id])\n",
        "  attention_masks = np.array([attention_mask])\n",
        "  token_type_ids = np.array([token_type_id])\n",
        "\n",
        "  encoded_input = [input_ids,attention_masks,token_type_ids]\n",
        "  # model.predict는 배치 단위로 예측 결과를 반환함\n",
        "  # many-to-one 문제라면 출력 형태는 (batch_size, output_dim)\n",
        "  # many-to-many 문제라면 (batch_size, seq_len, output_dim) 형태임\n",
        "\n",
        "  # 따라서 output[0]은 배치 첫 번째 샘플의 결과이고\n",
        "  # output[0][0]은 (many-to-one일 때) 첫 번째 출력값 또는\n",
        "  # (many-to-many일 때) 첫 번째 토큰의 출력값임\n",
        "\n",
        "  # 이진 분류에서는 output_dim=1 이므로 output[0][0]으로 확률 값을 얻음\n",
        "  # output은 (배치 크기, 시퀀스 길이, 출력 차원)의 3차원 배열임\n",
        "  # output[0]은 배치의 첫 번째 샘플에 해당하는 시퀀스 전체 출력이고,\n",
        "  # output[0][0]은 그 샘플 시퀀스 중 첫 번째 토큰에 대한 출력 벡터임\n",
        "  # 마찬가지로 output[1]은 두 번째 샘플의 전체 시퀀스 출력임\n",
        "  score = model.predict(encoded_input)[0][0]\n",
        "\n",
        "  if score > 0.5 :\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
      ],
      "metadata": {
        "id": "PhOkC3pRdXGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
      ],
      "metadata": {
        "id": "_Bpa48EkgS0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
      ],
      "metadata": {
        "id": "1aSIHJyFjNjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7dHlNpQRjQE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}