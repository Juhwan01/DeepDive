{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae18ba8-f950-4e87-8f3c-6e5f0ab02b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install -q tensorboard wandb \n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install -q --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\"\n",
    "\n",
    "%pip install \"Pillow>=9.4.0\"\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02234de3-f5a3-4a94-93e0-c18846a01089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c22022-1417-4c2a-ad0e-6aa7612222dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c7a06-fce8-4514-8c8e-7dbbe6652f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템(assistant)에게 주어진 역할\n",
    "system_message = \"당신은 이미지와 제품명(name)으로부터 패션/스타일 정보를 추론하는 분류 모델입니다.\"\n",
    "\n",
    "# 실제로 사용자 입력 -> 모델이 답해야 하는 프롬프트\n",
    "prompt = \"\"\"입력 정보:\n",
    "- name: {name}\n",
    "- image: [image]\n",
    "\n",
    "위 정보를 바탕으로, 아래 7가지 key에 대한 값을 JSON 형태로 추론해 주세요:\n",
    "1) gender\n",
    "2) masterCategory\n",
    "3) subCategory\n",
    "4) season\n",
    "5) usage\n",
    "6) baseColour\n",
    "7) articleType\n",
    "\n",
    "출력 시 **아래 JSON 예시 형태**를 반드시 지키세요:\n",
    "{{\n",
    "  \"gender\": \"예시값\",\n",
    "  \"masterCategory\": \"예시값\",\n",
    "  \"subCategory\": \"예시값\",\n",
    "  \"season\": \"예시값\",\n",
    "  \"usage\": \"예시값\",\n",
    "  \"baseColour\": \"예시값\",\n",
    "  \"articleType\": \"예시값\"\n",
    "}}\n",
    "\n",
    "# 예시\n",
    "{{\n",
    "  \"gender\": \"Men\",\n",
    "  \"masterCategory\": \"Accessories\",\n",
    "  \"subCategory\": \"Eyewear\",\n",
    "  \"season\": \"Winter\",\n",
    "  \"usage\": \"Casual\",\n",
    "  \"baseColour\": \"Blue\",\n",
    "  \"articleType\": \"Sunglasses\"\n",
    "}}\n",
    "\n",
    "# 주의\n",
    "- 7개 항목 이외의 정보(텍스트, 문장 등)는 절대 포함하지 마세요.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7eb2f-24f0-4e4b-b47e-d5e1bf680104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cols_to_label(example):\n",
    "    # 실제 컬럼명에 맞게 수정\n",
    "    label_dict = {\n",
    "        \"gender\": example[\"gender\"],\n",
    "        \"masterCategory\": example[\"masterCategory\"],\n",
    "        \"subCategory\": example[\"subCategory\"],\n",
    "        \"season\": example[\"season\"],\n",
    "        \"usage\": example[\"usage\"],\n",
    "        \"baseColour\": example[\"baseColour\"],\n",
    "        \"articleType\": example[\"articleType\"],\n",
    "    }\n",
    "    example[\"label\"] = json.dumps(label_dict, ensure_ascii=False)\n",
    "    return example\n",
    "\n",
    "def format_data(sample):\n",
    "   # Image.Image를 PngImageFile로 변환\n",
    "   buffer = io.BytesIO()\n",
    "   sample[\"image\"].save(buffer, format='PNG')\n",
    "   buffer.seek(0)\n",
    "   image = Image.open(buffer)\n",
    "   \n",
    "   return {\n",
    "       \"messages\": [\n",
    "           {\n",
    "               \"role\": \"system\",\n",
    "               \"content\": [\n",
    "                   {\n",
    "                       \"type\": \"text\",\n",
    "                       \"text\": system_message\n",
    "                   }\n",
    "               ],\n",
    "           },\n",
    "           {\n",
    "               \"role\": \"user\",\n",
    "               \"content\": [\n",
    "                   {\n",
    "                       \"type\": \"text\",\n",
    "                       \"text\": prompt.format(name=sample[\"productDisplayName\"]),\n",
    "                   },\n",
    "                   {\n",
    "                       \"type\": \"image\",\n",
    "                       \"image\": image,\n",
    "                   }\n",
    "               ],\n",
    "           },\n",
    "           {\n",
    "               \"role\": \"assistant\",\n",
    "               \"content\": [\n",
    "                   {\n",
    "                       \"type\": \"text\",\n",
    "                       \"text\": sample[\"label\"],\n",
    "                   }\n",
    "               ],\n",
    "           },\n",
    "       ],\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9b30f-2665-4d47-a290-f9f94c47ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ashraq/fashion-product-images-small\", split=\"train\")\n",
    "dataset_add_label = dataset.map(combine_cols_to_label)\n",
    "dataset_add_label = dataset_add_label.shuffle(seed=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e348d3-38a3-4223-b0dc-4587dd4cacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_add_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80992b0-3352-482f-866f-57f60fb2b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = [format_data(row) for row in dataset_add_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74a091-8bbf-4dfa-a169-b7a84f611976",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7b51c-9b4d-47b5-8811-23aa1d3e41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size=0.9로 설정하여 전체 데이터의 90%를 테스트 세트로 분리\n",
    "train_dataset, test_dataset = train_test_split(formatted_dataset,\n",
    "                                             test_size=0.9,\n",
    "                                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f870de25-a37a-4e5b-a1b4-fef198d72315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('학습 데이터의 개수:', len(train_dataset))\n",
    "print('테스트 데이터의 개수:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0fbdb-5db4-41a8-9ee3-883d8f5dfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "   model_id,\n",
    "   device_map=\"auto\",                            # GPU 메모리에 자동 할당\n",
    "   torch_dtype=torch.bfloat16,                   # bfloat16 정밀도 사용\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)  # 텍스트/이미지 전처리기 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ca1ea-30a7-4250-800f-cc6592c0d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182a4db-d71e-48f1-9de6-fe88d602af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "       # 모델 가중치에 LoRA 업데이트를 적용하는 정도를 조절하는 스케일링 계수\n",
    "       lora_alpha=128,\n",
    "       # 과적합을 방지하기 위한 드롭아웃 비율 설정\n",
    "       lora_dropout=0.05,\n",
    "       # LoRA의 순위(rank) - 저차원 행렬의 차원을 결정\n",
    "       r=256,\n",
    "       # 편향(bias) 업데이트 여부 - 'none'은 편향을 업데이트하지 않음\n",
    "       bias=\"none\",\n",
    "       # LoRA를 적용할 대상 모듈들 - 트랜스포머 모델의 주요 투영 레이어들\n",
    "       target_modules=[\n",
    "           \"q_proj\",    # Query 투영 레이어\n",
    "           \"up_proj\",   # FFN 상향 투영 레이어\n",
    "           \"o_proj\",    # Output 투영 레이어\n",
    "           \"k_proj\",    # Key 투영 레이어\n",
    "           \"down_proj\", # FFN 하향 투영 레이어\n",
    "           \"gate_proj\", # FFN 게이트 투영 레이어\n",
    "           \"v_proj\"     # Value 투영 레이어\n",
    "       ],\n",
    "       # 작업 유형 지정 - 인과적 언어 모델링(다음 토큰 예측)\n",
    "       task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0fb20-9c67-4a76-8882-9f81724e602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"output_dir\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=2,                      # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=16,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=8,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196baaa-f31e-4514-891d-ce5fc302d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "   \"\"\"\n",
    "   텍스트와 이미지가 포함된 대화 데이터를 모델 학습에 적합한 형태로 변환하는 함수\n",
    "   \n",
    "   Args:\n",
    "       examples: 각각 \"messages\" 키를 가진 딕셔너리들의 리스트\n",
    "                messages는 role(system/user/assistant)과 content를 포함하는 대화 형태\n",
    "   \n",
    "   Returns:\n",
    "       batch: 모델 학습에 사용할 수 있는 토큰화된 텍스트, 이미지, 라벨이 포함된 배치\n",
    "   \"\"\"\n",
    "   \n",
    "   # 1단계: 텍스트 전처리 - 채팅 템플릿 적용\n",
    "   # 각 예제의 messages를 모델 고유의 채팅 형식으로 변환\n",
    "   # 모델마다 다른 특수 토큰과 형식을 사용 (예: <|im_start|>, <|system|> 등)\n",
    "   texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "   \n",
    "   # 2단계: 이미지 데이터 추출 및 전처리\n",
    "   # messages에서 이미지 정보를 추출하여 모델이 처리할 수 있는 형태로 변환\n",
    "   # process_vision_info()는 messages에서 이미지를 찾아 적절한 형태로 변환해주는 함수\n",
    "   image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "   # 3단계: 텍스트 토크나이징 + 이미지 인코딩\n",
    "   # 텍스트를 토큰으로 변환하고 이미지를 인코딩하여 하나의 배치로 묶음\n",
    "   # return_tensors=\"pt\": PyTorch 텐서 형태로 반환\n",
    "   # padding=True: 배치 내 모든 시퀀스를 같은 길이로 맞춤 (짧은 것은 패딩 토큰으로 채움)\n",
    "   batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "   # 4단계: 라벨 생성 (손실 계산용)\n",
    "   # input_ids를 복사하여 라벨로 사용 (다음 토큰 예측 학습을 위함)\n",
    "   labels = batch[\"input_ids\"].clone()\n",
    "   \n",
    "   # 5단계: 패딩 토큰 손실 계산에서 제외\n",
    "   # 패딩된 부분은 실제 데이터가 아니므로 손실 계산에서 제외\n",
    "   # -100으로 설정하면 CrossEntropyLoss에서 자동으로 무시됨\n",
    "   labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "   # 6단계: 이미지 토큰 손실 계산에서 제외\n",
    "   # 이미지 토큰은 예측 대상이 아니므로 손실 계산에서 제외\n",
    "   if isinstance(processor, Qwen2VLProcessor):\n",
    "       # Qwen2VL 모델에서 사용하는 특수 이미지 토큰들의 ID\n",
    "       # 151652: 이미지 시작 토큰, 151653: 이미지 종료 토큰, 151655: 이미지 패치 토큰\n",
    "       image_tokens = [151652, 151653, 151655]\n",
    "   else:\n",
    "       # 다른 비전-언어 모델의 이미지 토큰 ID 추출\n",
    "       image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "   \n",
    "   # 이미지 토큰들을 손실 계산에서 제외 (-100으로 설정)\n",
    "   for image_token_id in image_tokens:\n",
    "       labels[labels == image_token_id] = -100\n",
    "\n",
    "   # 7단계: 최종 배치에 라벨 추가\n",
    "   # 모델 학습 시 손실 계산에 사용될 라벨을 배치에 추가\n",
    "   batch[\"labels\"] = labels\n",
    "\n",
    "   return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a62ef-06b4-41d8-97ae-5aa2f2a3c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 예시 확인\n",
    "example = train_dataset[0]  # 데이터셋의 첫 번째 아이템\n",
    "print(\"단일 예시 데이터:\")\n",
    "print(example)\n",
    "\n",
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"이미지 픽셀 형태:\", batch[\"pixel_values\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5a42f-b871-4d7b-b2db-a61b85d1a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ab87a-7417-4c5b-b3b5-2733565bfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671c6c0-ca49-467d-b427-df6f9c0cd8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 디코딩 예시 (입력 텍스트가 어떻게 변환되었는지 확인)\n",
    "decoded_text = processor.tokenizer.decode(batch[\"input_ids\"][0])\n",
    "print(\"\\n디코딩된 텍스트:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5e71b-4db7-4343-b80d-5ffeb839a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18243c0b-f767-47fd-a84c-d9923145686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
