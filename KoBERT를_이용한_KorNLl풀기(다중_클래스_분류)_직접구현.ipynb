{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOemLf//qgF/VNysIOVukpd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhwan01/DeepDive/blob/main/KoBERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_KorNLl%ED%92%80%EA%B8%B0(%EB%8B%A4%EC%A4%91_%ED%81%B4%EB%9E%98%EC%8A%A4_%EB%B6%84%EB%A5%98)_%EC%A7%81%EC%A0%91%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XZlU0wXAkH4"
      },
      "outputs": [],
      "source": [
        "# 이전과 똑같이 레거시 모드로\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "kcvifoPvAqIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터 다운로드\n",
        "# MultiNLI 한국어 훈련 데이터\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/multinli.train.ko.tsv\",\n",
        "                          filename=\"multinli.train.ko.tsv\")\n",
        "\n",
        "# SNLI 1.0 한국어 훈련 데이터\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/snli_1.0_train.ko.tsv\",\n",
        "                          filename=\"snli_1.0_train.ko.tsv\")\n",
        "\n",
        "# 검증 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.dev.ko.tsv\",\n",
        "                          filename=\"xnli.dev.ko.tsv\")\n",
        "\n",
        "# 테스트 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.test.ko.tsv\",\n",
        "                          filename=\"xnli.test.ko.tsv\")"
      ],
      "metadata": {
        "id": "ucpYI7W3BHmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tsv -> 탭(\\t)으로 구분된다 <=> csv는 기본적으로 쉼표(,))로 구분된다\n",
        "# quoting=3 (QUOTE_NONE)으로 읽을 때\n",
        "# 따옴표가 제거되지 않고 그대로 유지됨\n",
        "# \"Hello\"도, \"Quoted text\"도 따옴표 포함 그대로 읽음 -> 따옴표를 그대로 보존해야 할 때 사용\n",
        "train_snli = pd.read_csv(\"snli_1.0_train.ko.tsv\", sep='\\t', quoting=3)\n",
        "train_xnli = pd.read_csv(\"multinli.train.ko.tsv\", sep='\\t', quoting=3)\n",
        "val_data = pd.read_csv(\"xnli.dev.ko.tsv\", sep='\\t', quoting=3)\n",
        "test_data = pd.read_csv(\"xnli.test.ko.tsv\", sep='\\t', quoting=3)"
      ],
      "metadata": {
        "id": "De_odCp5BZP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에서 읽어온 데이터는 DataFrame형태 이 두개의 객체를 세로(행 방향)로 붙인다.\n",
        "# pandas 2.0부터 DataFrame.append() 메서드가 삭제 되었습니다. -> concat 메서드 사용\n",
        "#ignore_index=True는 기존 인덱스를 무시하고 새로운 연속적인 인덱스(0, 1, 2, ...)로 다시 만들어주는 옵션입니다.\n",
        "train_data = pd.concat([train_snli,train_xnli])\n",
        "# 결합 후 섞기\n",
        "# sample() 함수는 DataFrame에서 무작위 샘플을 추출하는 함수\n",
        "# frac=1 은 전체 데이터(frac=1 즉 100%)를 무작위로 섞어 반환한다는 뜻\n",
        "# frac\t데이터에서 뽑을 비율(0~1 사이 실수)\tfrac=0.5 → 50% 무작위 추출\n",
        "# 원본 인덱스는 유지되기 때문에, 만약 인덱스도 재정렬하고 싶으면 reset_index(drop=True)를 추가로 써야 합니다.\n",
        "train_data = train_data.sample(frac=1)"
      ],
      "metadata": {
        "id": "eX5EqIQWDUZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "gk6q6Ue4FkrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.head()"
      ],
      "metadata": {
        "id": "Vz9kcpMJGkDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "GlGZmlOzHIRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_na_and_duplciates(df):\n",
        "  df = df.dropna() # how='any'가 기본값\n",
        "  df = df.drop_duplicates()\n",
        "  # drop=True를 쓰면 기존 인덱스를 새로운 컬럼으로 추가하지 않고 그냥 버립니다. 만약 drop=False면, 기존 인덱스가 컬럼으로 남는다\n",
        "  df = df.reset_index(drop=True) # DataFrame의 인덱스를 초기화\n",
        "  return df"
      ],
      "metadata": {
        "id": "1ekmdYXwHLqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "train_data = drop_na_and_duplciates(train_data)\n",
        "val_data = drop_na_and_duplciates(val_data)\n",
        "test_data = drop_na_and_duplciates(test_data)"
      ],
      "metadata": {
        "id": "J2r4vxhgIA4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 샘플 개수 :', len(train_data))\n",
        "print('검증용 샘플 개수 :', len(val_data))\n",
        "print('테스트용 샘플 개수 :', len(test_data))"
      ],
      "metadata": {
        "id": "TdtuB0EbFVCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "tlhtMlnTFhfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 128\n",
        "# iloc -> 데이터프레임에서 위치 기반으로 행을 가져온다 -> iloc[0]은 첫 번째 행\n",
        "# 왜 [0] 사용안하느냐?\n",
        "# 인덱스가 재설정되지 않았거나 섞인 경우에는 iloc[0]이 더 안전하고 정확\n",
        "sent1 = train_data['sentence1'].iloc[0]\n",
        "sent2 = train_data['sentence2'].iloc[0]\n",
        "\n",
        "print('문장1 :',sent1)\n",
        "print('문장2 :',sent2)"
      ],
      "metadata": {
        "id": "cjy7QdQcGGpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode_plus 메서드는 문장 쌍 관계 처리를 위해 만들어졌다.\n",
        "# 이렇게 한번에 인코딩하여 두 문장을 이어서 모델에 넣는다.\n",
        "# padding='max_length' -> 0으로 max_length까지 패딩\n",
        "# BertTokenizerFast랑은 다르게 작동하는듯\n",
        "encoding_result = tokenizer.encode_plus(sent1,sent2,max_length=max_seq_len,padding='max_length',truncation=True)"
      ],
      "metadata": {
        "id": "5zDJVOi7HqbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoding_result['attention_mask'])"
      ],
      "metadata": {
        "id": "RtKQ1wE5Llqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에서한 전처리를 모든 데이터에 적용\n",
        "def convert_examples_to_features(sent_list1,sent_list2,max_seq_len,tokenizer):\n",
        "  input_ids, attention_masks, token_type_ids = [], [], []\n",
        "  # tqdm -> 진행도\n",
        "  # zip(sent1,sent2) -> zip()으로 묶으면 두 리스트를 쌍으로 묶어서 튜플을 만든다\n",
        "  # 예시 -> [('안녕', '하세요'), ('좋은', '아침'), ('나쁜', '날씨')]\n",
        "  for sent1,sent2 in tqdm(zip(sent_list1,sent_list2),total=len(sent_list1)):\n",
        "    encoding_result = tokenizer.encode_plus(sent1,sent2,max_length=max_seq_len,padding='max_length',truncation=True)\n",
        "    # 모델에 넣을 입력들\n",
        "    input_ids.append(encoding_result['input_ids'])\n",
        "    attention_masks.append(encoding_result['attention_mask'])\n",
        "    token_type_ids.append(encoding_result['token_type_ids'])\n",
        "\n",
        "  # 최종 모델 입력을 위해 리스트를 numpy 배열(int 타입)로 변환\n",
        "  # 모델 입력은 일반적으로 numpy 배열이나 Tensor 형태여야 한다.(이유->이전 실습 노트에 작성)\n",
        "  input_ids = np.array(input_ids, dtype=int)\n",
        "  attention_masks = np.array(attention_masks, dtype=int)\n",
        "  token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "\n",
        "  return (input_ids,attention_masks,token_type_ids)"
      ],
      "metadata": {
        "id": "mvx36c7hMcSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = convert_examples_to_features(train_data['sentence1'],train_data['sentence2'],max_seq_len,tokenizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dYuqcO0gPgN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_id = X_train[0][0]\n",
        "attention_mask = X_train[1][0]\n",
        "token_type_id = X_train[2][0]\n",
        "\n",
        "print('단어에 대한 정수 인코딩 :', input_id)\n",
        "print('어텐션 마스크 :', attention_mask)\n",
        "print('세그먼트 인코딩 :', token_type_id)\n",
        "print('각 인코딩의 길이 :', len(input_id))\n",
        "print('정수 인코딩 복원 :', tokenizer.decode(input_id))"
      ],
      "metadata": {
        "id": "Emu_yLQaRUU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증 데이터도 똑같이 진행\n",
        "X_val = convert_examples_to_features(val_data['sentence1'],val_data['sentence2'],max_seq_len,tokenizer)"
      ],
      "metadata": {
        "id": "trhL6Z6aTSaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_id = X_val[0][0]\n",
        "attention_mask = X_val[1][0]\n",
        "token_type_id = X_val[2][0]\n",
        "\n",
        "print('단어에 대한 정수 인코딩 :', input_id)\n",
        "print('어텐션 마스크 :', attention_mask)\n",
        "print('세그먼트 인코딩 :', token_type_id)\n",
        "print('각 인코딩의 길이 :', len(input_id))\n",
        "print('정수 인코딩 복원 :', tokenizer.decode(input_id))"
      ],
      "metadata": {
        "id": "IdweOwhjTd3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터도 똑같이 진행\n",
        "X_test = convert_examples_to_features(test_data['sentence1'],test_data['sentence2'],max_seq_len,tokenizer)"
      ],
      "metadata": {
        "id": "WGIq81qOTiil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_id = X_test[0][0]\n",
        "attention_mask = X_test[1][0]\n",
        "token_type_id = X_test[2][0]\n",
        "\n",
        "print('단어에 대한 정수 인코딩 :', input_id)\n",
        "print('어텐션 마스크 :', attention_mask)\n",
        "print('세그먼트 인코딩 :', token_type_id)\n",
        "print('각 인코딩의 길이 :', len(input_id))\n",
        "print('정수 인코딩 복원 :', tokenizer.decode(input_id))"
      ],
      "metadata": {
        "id": "EkZVMz87T1qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열로 구성된 레이블 정수 인코딩 진행\n",
        "train_label = train_data['gold_label'].tolist()\n",
        "val_label = val_data['gold_label'].tolist()\n",
        "test_label = test_data['gold_label'].tolist()\n",
        "\n",
        "# LabelEncoder는 문자열 라벨을 숫자로 변환해주는 도구\n",
        "#fit(): 카테고리 종류 학습\n",
        "#transform(): 숫자로 변환\n",
        "#fit_transform(): fit + transform 한번에\n",
        "#inverse_transform(): 숫자 -> 원본으로 역변환\n",
        "idx_encode = preprocessing.LabelEncoder()\n",
        "idx_encode.fit(train_label)\n",
        "\n",
        "# 정수 변환\n",
        "y_train = idx_encode.transform(train_label)\n",
        "y_val = idx_encode.transform(val_label)\n",
        "y_test = idx_encode.transform(test_label)\n",
        "# idx_encode.classes_\n",
        "# array(['contradiction', 'entailment', 'neutral'], dtype=object)\n",
        "# label_idx = dict(zip(...))\n",
        "# dict(zip(['contradiction', 'entailment', 'neutral'], [0, 1, 2]))\n",
        "# 결과 적으로 -> 'contradiction': 0\n",
        "label_idx = dict(zip((list(idx_encode.classes_)),idx_encode.transform(list(idx_encode.classes_))))\n",
        "idx_label = {value:key for key,value in label_idx.items()}\n",
        "print('각 레이블과 정수 :',label_idx)"
      ],
      "metadata": {
        "id": "gmM6s_PVT5p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('변환 전 :', train_label[:5])\n",
        "print('변환 후 :', y_train[:5])"
      ],
      "metadata": {
        "id": "NzxOa957WRE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전에 정의해둔 클래스 복붙(앞에서 한 내용 bert 네이버 영화 리뷰 분류) -> 차이점은(다중분류)\n",
        "#\n",
        "class TFBertForSequenceClassification(tf.keras.Model):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
        "        # 출력층을 하나 만들어서 classifier에 저장\n",
        "        # 출력 뉴런 하나로 이진 분류 예측 -> 출력 뉴런 num_labels수 만큼 = 다중 분류 문제\n",
        "        self.classifier = tf.keras.layers.Dense(num_labels,\n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
        "                                                # softmax사용 -> 다중 분류\n",
        "                                                activation='softmax',\n",
        "                                                name='classifier')\n",
        "    def call(self, inputs):\n",
        "      input_ids, attention_mask, token_type_ids = inputs\n",
        "\n",
        "      # BERT 모델을 통과시켜 모든 토큰에 대한 출력 및 [CLS] 벡터 얻기\n",
        "      outputs = self.bert(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          token_type_ids=token_type_ids)\n",
        "\n",
        "      # 문장의 대표 벡터인 [CLS] 토큰의 벡터 가져오기 (pooled_output)\n",
        "      cls_token = outputs[1]\n",
        "\n",
        "      # [CLS] 벡터를 Dense 레이어에 넣어서 다중 클래스 확률 예측\n",
        "      # softmax를 사용하므로 각 클래스에 대한 확률 분포가 나옴\n",
        "      prediction = self.classifier(cls_token)\n",
        "\n",
        "      return prediction"
      ],
      "metadata": {
        "id": "1-7vh-cKZlph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞이랑 다르게 label이 3개이기 때문에 num_labels=3\n",
        "model = TFBertForSequenceClassification(\"klue/bert-base\",num_labels=3)"
      ],
      "metadata": {
        "id": "SRKGUkYUc4ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(5e-5)\n",
        "# SparseCategoricalCrossentropy -> 레이블 형식: 정수 인코딩된 레이블 (예: 2, 0, 1, ...)\n",
        "# 다중 클래스 분류에서 레이블이 정수 인코딩 상태이면 SparseCategoricalCrossentropy 를 쓰고,\n",
        "# 레이블이 원-핫 인코딩 상태이면 CategoricalCrossentropy 를 씁니다.\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ND02QOrAdH3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0.001,\n",
        "    patience=2,\n",
        ")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "metadata": {
        "id": "8RXi-pIZdRSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [0] 은 손실값\n",
        "# [1] 은 첫 번째 평가 지표 (보통 accuracy)\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test, batch_size=1024)[1]))"
      ],
      "metadata": {
        "id": "nWJcMYnueR-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}