{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e0da7a-e33e-48e0-818e-4c9fa01bed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4336804c-677b-4481-95ed-dff63c9668cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터의 type 분포:\n",
      "mrc_question_with_1_to_4_negative: 296\n",
      "paraphrased_question: 196\n",
      "synthetic_question: 497\n",
      "mrc_question: 491\n",
      "no_answer: 404\n",
      "\n",
      "전체 데이터 분할 결과: Train 380개, Test 1504개\n",
      "\n",
      "학습 데이터의 type 분포:\n",
      "mrc_question_with_1_to_4_negative: 60\n",
      "paraphrased_question: 40\n",
      "synthetic_question: 100\n",
      "mrc_question: 99\n",
      "no_answer: 81\n",
      "\n",
      "테스트 데이터의 type 분포:\n",
      "mrc_question_with_1_to_4_negative: 236\n",
      "paraphrased_question: 156\n",
      "synthetic_question: 397\n",
      "mrc_question: 392\n",
      "no_answer: 323\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"iamjoon/klue-mrc-ko-rag-dataset\", split=\"train\")\n",
    "\n",
    "system_message = \"\"\"당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
    "\n",
    "다음의 지시사항을 따르십시오.\n",
    "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
    "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
    "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
    "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
    "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
    "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
    "\n",
    "검색 결과:\n",
    "-----\n",
    "{search_result}\"\"\"\n",
    "\n",
    "print(\"원본 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    print(f\"{type_name}: {dataset['type'].count(type_name)}\")\n",
    "\n",
    "test_ratio = 0.8\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for type_name in set(dataset['type']):\n",
    "    curr_type_data = [i for i in range(len(dataset)) if dataset[i]['type'] == type_name]\n",
    "    \n",
    "    test_size = int(len(curr_type_data) * test_ratio)\n",
    "    \n",
    "    test_data.extend(curr_type_data[:test_size])\n",
    "    train_data.extend(curr_type_data[test_size:])\n",
    "\n",
    "def format_data(sample):\n",
    "    search_result = \"\\n-----\\n\".join([f\"문서{idx + 1}: {result}\" for idx, result in enumerate(sample[\"search_result\"])])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message.format(search_result=search_result),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"question\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"answer\"]\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]\n",
    "\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")\n",
    "\n",
    "print(\"\\n학습 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in train_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")\n",
    "\n",
    "print(\"\\n테스트 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in test_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699d8946-1487-4220-b1ee-aa1aeab98214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: “우버(UBER)는 사람들이 이동하는 수단을 발전시켰습니다. 승객과 기사를 실시간으로 연결해 승객에게는 편리함을, 기사에게는 더 많은 효율성과 수익을 가져다 주었습니다. 2010년 6월 미국 샌프란시스코에서 처음 서비스를 시작한 뒤 현재 세계 140여개 도시에 진출했습니다. 우버는 앞으로도 계속 사람과 도시를 가깝게 이어줄 것입니다.”샌프란시스코 우버 본사에서 최근 만난 나이리 후다지안 우버 글로벌커뮤니케이션 부문장은 이렇게 강조했다. 우버는 일종의 ‘차량 예약 서비스’를 제공하는 글로벌 정보기술(IT) 기업이다. 스마트폰 애플리케이션(앱·응용프로그램·사진)을 통해 승객과 차량을 연결해 주는 사업을 하고 있다.승객을 일반택시와 연결해 주는 ‘우버택시’, 일반인이 자신의 차량으로 운송 서비스를 할 수 있도록 도와주는 ‘우버엑스’, 일종의 고급 콜택시인 ‘우버블랙’ 등의 서비스를 갖췄다. 한국에서는 지난해 8월 우버코리아가 설립돼 우버블랙 사업을 펼치고 있다.후다지안 부문장은 “우리는 더 나은 교통 서비스 위해 플랫폼을 제공하는 회사”라고 설명했다. 그는 “택시를 잡기 어려운 지역에서도 우버를 통해 사람들에게 편리한 운송 서비스를 제공할 수 있다”며 “우리는 사람들이 좀 더 신뢰할 수 있고 빠른 교통 서비스를 원한다는 사실을 확인할 수 있었다”고 말했다. 우버는 기본적으로 스마트폰 앱을 통해 차량을 부르고 요금을 내는 구조이기 때문에 매우 빠르고 편리하다는 게 그의 설명이다.그러나 우버는 세계 여러 도시에서 법적 논란도 일으키고 있다. 이달 초에는 런던 파리 베를린 로마 등 유럽 주요 대도시에서 택시기사들이 ‘우버 반대’ 시위를 잇달아 벌이기도 했다. 한국에서도 불법 논란이 확산되는 중이다. 정식 택시회사로 등록돼 있지 않고 렌터카(고급 외제차 등) 등을 이용해 우버블랙 서비스가 이뤄지고 있기 때문이다.하지만 우버 측은 차량과 승객만 연결해 주기 때문에 문제가 없다는 입장이다. 후다지안 부문장은 “새로운 서비스가 나올 때마다 논란은 있게 마련”이라며 “분명한 것은 택시기사들도 우버 서비스를 통할 때 더 많은 수익을 내고 있다는 사실”이라고 강조했다. 샌프란시스코=안정락 기자\\n-----\\n문서2: 차량 공유업체 우버를 통해 승객을 태우는 운전자는 자영업자가 아니라 우버의 피고용인으로 봐야 한다는 심결이 미국 캘리포니아주 노동위원회에서 나왔다. 강제성은 없지만 이를 계기로 우버에 대한 집단소송이 캘리포니아에서 일어날 수 있다. 기름값, 통행료 등 각종 비용은 운전자에게 부담시키고 운전자와 승객을 연결해주며 수수료만 받는 우버의 사업 모델이 흔들릴 수 있다는 전망이다.17일(현지시간) 뉴욕타임스 등에 따르면 캘리포니아주 노동위는 지난해 8주 동안 우버 운전기사로 일한 바버라 앤 버윅에게 우버는 4152달러20센트를 지급하라고 명령했다. 버윅은 지난해 7~9월 1주일에 60~80시간을 일하고 1만1000달러를 벌었다. 하지만 비용과 세금을 제하면 최저임금도 받지 못한 셈이라며 지난해 9월 노동위에 문제를 제기했다. 노동위는 “우버는 단순히 승객과 운전자를 연결하는 중립적인 기술 플랫폼일 뿐이라고 항변했지만, 실제론 서비스가 이뤄지는 모든 부분에 개입했다”고 밝혔다.지난 3일 결정된 이번 심결은 16일 우버가 법원에 소송을 제기하면서 알려졌다. 심결은 강제성이 없으며 버윅 개인에게만 한정된다. 우버는 조지아, 펜실베이니아, 텍사스 등 5개 주는 이미 우버 운전자를 독립 계약자로 분류하고 있다며 이번 심결의 부당함을 호소했다. 반면 플로리다주는 종업원으로 분류해야 한다고 결정해 주마다 의견이 엇갈린다. 다만 캘리포니아주는 우버 본사가 있는 지역인 데다 세계 정보기술(IT)의 중심지여서 각 지역 규제 당국이 이번 심결을 참고할 것으로 전망된다.\\n-----\\n문서3: “우버 사태는 시작일 뿐이에요. 새로운 기술에 바탕을 둔 서비스의 등장으로 기존 법·제도와 마찰을 겪는 일은 앞으로 계속 일어날 것입니다.”자가용 콜택시 알선 서비스인 우버(Uber)에 대해 서울시가 차단 방침을 밝힌 것을 계기로 새로운 혁신 서비스와 기술이 현행 법 등과 충돌하는 문제가 도마에 올랐다. 국내 정보기술(IT) 전문가들은 우버와 같은 혁신 서비스를 기존 제도와 법규 틀 안에서 규제해서는 안 된다고 지적한다. 무인 자동차와 무인 비행체, 공유 숙박, 온라인 부동산 중개 서비스 등 현행 법으로는 수용하기 힘든 서비스들이 계속해서 나타나고 있기 때문이다.이를 무조건 불법으로 규정해 막거나 서울시처럼 공공서비스로 대체해 시장을 빼앗는 것은 민간의 창의와 혁신을 짓밟는 행위라는 주장이다. 오히려 우버 사태를 계기로 혁신 기술과 서비스를 탄력적으로 수용할 수 있도록 법과 제도를 바꿔나가는 논의를 시작해야 한다고 강조했다.○법이 기술 변화 못 따라가최근 우버 논란에 대해 김진형 소프트웨어정책연구소장은 “지금 당장은 우버를 금지할 수 있지만 언제까지 계속 금지할 수는 없을 것”이라고 말했다. 자동차가 처음 등장했을 때도 우마차 업자들이 반대해 자동차도 빨간 깃대를 꽂고 다니도록 한 법을 1865년 제정한 적이 있다는 것이다.윤종수 법무법인 세종 변호사는 “우버는 기존 운송 체계와 다른 비즈니스가 등장한 것인데, 합법이냐 불법이냐보다 그것이 사회적으로 효용이나 가치를 더 많이 주는지 아니면 피해나 문제를 더 많이 발생시키는지 먼저 따져야 한다”고 말했다. 그는 “인터넷이 처음 등장했을 때나 인터넷으로 음악을 들을 수 있게 됐을 때도 마찬가지였다”며 “새로운 가치를 제공해 많은 사람이 받아들이면 결국 규제가 바뀌게 된다”고 덧붙였다.조신 연세대 미래융합기술연구원장은 “어느 나라나 규제가 기술의 변화를 못 좇아가는 게 일반적이지만, 한국은 패러다임 변화에 맞춰 법과 제도가 바뀌는 속도가 너무 느린 감이 있다”며 “특히 제조업에 비해 발전이 늦은 서비스산업을 키우기 위해서라도 새로운 기술에 맞춰 다양한 서비스가 활성화될 수 있는 터를 마련해줄 필요가 있다”고 지적했다. 그는 미국 캘리포니아와 네바다주가 무인 자동차 출현에 대비해 이를 합법화한 게 좋은 사례라고 소개했다.○택시기사에게도 선택권 줘야서울시의 우버 앱 서비스 차단 방침에 대해서는 승객은 물론 택시기사에게 선택권을 줘야 한다는 의견도 있다. 임정욱 스타트업얼라이언스 센터장은 “우버는 택시기사가 아니라 택시회사를 위협하는 서비스”라며 “오히려 택시기사들이 우버를 비롯해 다양한 교통 서비스 회사를 통해 일할 수 있도록 선택권을 보장해야 한다”고 말했다.한상기 소셜컴퓨팅연구소 대표는 “많은 택시운전사의 피해가 예상되는 만큼 우버에 반대하는 의견도 이해가 간다”면서 “우버와 택시 업계, 정책 결정자들이 머리를 맞대고 논의해 모두에게 혜택이 돌아갈 수 있도록 조율하는 과정이 필요하다”고 말했다.김진형 소장은 “산업혁명기에 기계를 부수는 러다이트 운동이 일어났지만 결국 기술의 발전을 막을 수 없었다”며 “국가의 역할은 새로운 기술에 빠르게 적응하도록 직업교육을 강화하는 것이지 무조건 막는 게 아니다”고 강조했다.\\n-----\\n문서4: 우버는 금일부터 우버택시 서비스에 앱 결제 시스템을 적용한다고 발표했다. 기존에 우버블랙에 탑재되어 시행되고 있는 앱 결제 시스템이 모든 우버택시에 확대 적용되는 것으로, 우버는 앱 결제 시스템을 기반으로 더욱 편리한 서비스를 제공해 이용객 편의를 강화한다는 방침이다. 신규 결제 시스템 적용으로 우버택시 이용객은 탑승에 앞서 예상 요금을 확인할 수 있으며, 앱에 등록된 카드로만 우버택시 이용 요금을 결제할 수 있다. 특히, 코로나로 안전에 대한 경각심이 높아진 가운데, 앱 결제 시스템 상에서 비대면으로 결제가 진행되므로 승객들은 안심하고 우버택시를 이용할 수 있다. 드라이버와 승객 간에 현금을 주고받고, 거스름돈을 받는 등 결제를 위한 대기 시간이 사라지고, 통행요금은 자동으로 합산되며, 앱을 통해 탑승 및 운임 내역 등을 언제 어디서나 손쉽게 확인 가능하다. 앱 결제 시스템을 이용하기 위해서는 우버 앱 상에 국내 발급 카드 등록이 필요하다. 카드 등록 절차는 간단하다. 먼저 우버 앱에 접속해 화면 좌측 상단의 메뉴 버튼을 눌러 결제[Wallet]를 선택한다. 결제 수단 아래 [결제 수단 추가] 버튼을 거쳐 [신용카드 또는 체크카드] 버튼을 선택, 카드 정보를 입력하면 등록 절차가 완료된다. 앱 결제 시스템은 우버블랙을 포함, 국내 시장에서 이미 널리 사용되는 결제 방법이지만, 해당 시스템이 익숙하지 않은 이들도 원활하게 서비스를 이용할 수 있도록 우버는 앱 내 ‘고객 지원’ 문의 기능을 적극 활용해 도움이 필요한 이들을 신속하게 지원할 예정이다. 우버는 신규 시스템으로 이용객 편의를 제고하는 동시에 다양한 안전 기능을 통해 서비스 안전 강화 정책을 펼치고 있다. 우버는 지난 4월, 스마트폰에 탑재된 GPS 및 센서 기술을 바탕으로 국내 서비스에 ‘운행 상황 확인’ 기능을 도입했다. 해당 기능은 우버 차량에 예기치 못한 정차가 발생했을 경우 혹은 예정 경로에서 벗어나거나 사고가 의심되는 상황 등을 감지해 앱 상에서 드라이버와 승객 모두에게 안전 여부를 확인하는 메시지를 전송한다. 문제가 발생했을 경우, 우버 이용객들은 긴급 버튼을 눌러 도움을 요청할 수 있으며, 우버 안전 관리부(Safety Line)에 문제가 생겼다고 신고할 수 있다. 또한, 운행 상황 확인 기능 도입과 함께 기존의 피드백 기능을 크게 개선해 우버 이용객들이 단순히 별점으로 서비스 평가로 매기는 것이 아니라, 소음, 운전, 운행 경로 등에 대해 상세한 의견을 남길 수 있도록 설계했다. 이 외에도 우버는 최대 장점인 기술력을 적극 활용해 안전에 중점을 둔 다수의 기능을 선보이며 안전 강화 행보를 이어나가고 있다. 눈여겨볼 주요 안전 기능으로는 호출 차량이 맞는지 확인을 도와주는 ‘당신의 여정이 맞는지 확인하세요(Check Your Ride)’ 인앱 기능, 4자리 핀(PIN) 인증 시스템인 ‘요청 차량 확인’ 기능, ‘색상으로 승객 찾기(Spotlight)’ 기능 등이 있다. 우버코리아 톰 화이트(Tom White) 한국 총괄은 “우버는 안전을 최우선 과제로 삼고, 지속되는 글로벌 위기와 변화에 빠르게 대응하기 위해 전염병 전문학자를 비롯 최고 전문가들에게 지속적으로 자문을 구하고 공조하고 있다”라며, “앞서 선보인 우버만의 기술력을 활용한 다양한 기능과 함께 한국에서도 더욱 편리하게 앱을 이용할 수 있도록 앞으로도 최선을 다하겠다”라고 덧붙였다.\\n-----\\n문서5: 우버는 연말연시 안전한 우버 모빌리티 서비스 이용을 위해 꼭 알아두어야 할 여러가지 안전 기능을 소개했다. 우선 우버는 작년 처음 선보인 112로 전화가 연결되는 \\'112 지원 버튼\\' 옵션에 문자 기능을 새롭게 추가한다고 밝혔다. 위급상황 발생시 문자 기능을 탑재한 ‘112 지원 버튼’을 누르면 우버 탑승자 휴대폰 내에 차량의 종류, 모델, 번호판 등 주요 정보를 담은 문자가 자동 생성되며, 전송을 누르면 112 상황실로 문자가 전송된다. 이번 추가 기능을 토대로 우버는 보다 더 안전성을 강화해 나갈 예정이다. 우버는 2018년 앱 상에서 드라이버와 탑승자 모두가 이용할 수 있는 안전 기능으로 ‘112 지원 버튼’을 도입한 바 있으며, 이용자 안전성 및 편의성을 강화하기 위한 노력에 맞춰 앱 상에 지속적으로 다양한 기능들을 추가하고 있다. 그간 우버가 선보인바 있거나 도입하기 위해 추진하고 있는 기능으로는 ‘안심 연락처’, ‘안전 도구 툴킷(Safety Toolkit)’, ‘색상으로 승객 찾기(Spotlight)’, ‘PIN 인증 시스템’, ‘드라이버 신원 확인 시스템’ 강화 등이 있다. 먼저, 작년 ‘112 지원 버튼’의 도입과 함께 적용된 ‘안심 연락처’ 기능은 우버 이용자가 최대 5명까지 지인의 연락처를 미리 앱에 등록해 차량 탑승시 등록되어 있는 연락처로 예상 도착 시간을 포함, 실시간 위치 정보를 공유할 수 있도록 지원한다. 아울러 해당 기능은 이용자 의사에 따라 야간에만 활성화할 수 있는 별도 옵션과 함께 제공된다. 나아가, 탑승자들은 안전에 관한 다양한 정보를 한눈에 보여주는 안전 도구 툴킷 메뉴를 이용해 경찰 당국의 지원으로 제공되는 안전 팁 외에도 드라이버 경력 조회, 보험 보장 정보 등 각종 커뮤니티 가이드라인을 확인할 수 있다. 그 밖에 우버 탑승자들은 ‘색상으로 승객 찾기’ 기능을 통해서 앱에서 차량을 호출한 뒤 해당 버튼을 눌러 핸드폰 전체 화면을 지정된 밝은 색으로 변경할 수 있다. 드라이버에게는 색상 정보를 제공하는 인앱 메시지가 전송되어 탑승자들이 핸드폰 화면을 도로 쪽으로 보이게 들면 드라이버가 탑승자를 쉽게 찾을 수 있다. 이에 멈추지 않고 우버는 4자리 PIN 인증 시스템 도입을 추진하고 있다. 이는 탑승자가 잘못된 차량에 탑승하는 일이 없도록 설계되었다. 탑승자들은 원할시 차량 탑승 전 4자리 고유 핀 번호를 부여받을 수 있다. 드라이버는 탑승자에게 번호를 전달받아 앱 상에 입력할 경우에만 출발이 가능하다. 이와 더불어 우버는 초음파를 사용해 PIN 인증 없이도 올바른 차량 확인이 가능한 기술을 개발 중에 있다. 우버 운전기사 신원확인 시스템 또한 개선되었다. 기존 방식에서는 운전자가 주기적으로 셀카를 찍어 등록된 사진과 동일 여부를 검증했지만 앞으로는 셀카를 찍으면서도 머리를 움직이고 미소까지 지어야 인증이 완료된다. 한층 강화된 보안 기능으로 승객의 안전성을 최대한 보장하겠다는 의도가 담긴 발전이다. 마지막으로, 우버는 휴일에 공항, 기차역 등으로 몰려드는 많은 여행객들로 인해 교통이 혼잡할 것을 대비해 우버 이용객들에게 ‘당신의 여정이 맞는지 확인하세요(Check Your Ride)’ 인앱 기능 사용을 권장하고 있다. 해당 기능을 통해 우버 이용객들은 배정된 차량 및 드라이버가 맞는지 확인하기 위해 필요한 드라이버 사진, 차량 번호판, 차량 종류 및 모델을 비롯한 세부 사항을 안내받을 수 있다.'},\n",
       " {'role': 'user',\n",
       "  'content': '캘리포니아주 외에 독립 계약자로 우버 운전자를 분류하는 다른 3개 주의 구체적인 이유는 무엇인가요?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '검색 결과에는 캘리포니아주 외에 우버 운전자를 독립 계약자로 분류하는 다른 3개 주와 그 이유에 대한 구체적인 정보를 찾을 수 없습니다.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647f5ca0-9bbd-4ace-94e7-c1959a1a33ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a15123f-65f7-4d44-a295-1f7a3fa7c7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ba55521127426fa02dadd204dfe4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset.save_to_disk(\"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59eca2a6-099b-4bbf-a039-860d8326dfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: “박근혜 대통령의 방중을 통해 북핵문제에 대해 한·미·중 삼각협력체제가 형성될 수 있는 기반이 마련된 것은 외교적 업적이 분명하다.” 서진영 고려대 명예교수(71·사진)는 30일 “박 대통령의 방중이 상당히 만족할 만한 효과를 거뒀다”고 평가하며 이같이 말했다. 서 교수는 중국 정치와 미·중관계 등을 연구해온 원로 중국전문가다. 김영삼 정부에서 대통령 자문 정책기획위원장을, 이명박 정부에서 한·중 전문가 공동연구위원회 한국 측 위원장을 역임했다. 서 교수는 한·미·중 정상이 연쇄적으로 회동하면서 북한 핵문제에 대해 일치된 목소리를 낸 것에 가장 큰 의미를 부여했다. 그는 “이번 한·중 정상회담에서 ‘북한 핵을 절대 용납할 수 없다’는 원칙을 ‘한·중 미래비전 공동선언’에 담지는 못했지만 박 대통령이 기자회견에서 직접 언급했다는 점에서 중국이 간접적으로 인정했다고 볼 수 있다”며 “과거보다 중국이 북핵문제에 대해 훨씬 전향적인 자세를 보인 것”이라고 말했다.특히 중국 측이 박 대통령에게 보인 환대를 주목했다. 서 교수는 “시진핑 주석은 박 대통령과 7, 8시간을 같이 보냈고 부인인 펑리위안 여사와도 함께 만나는 등 ‘패밀리 미팅’ 같은 분위기까지 만들어내려 했다”며 상당한 성의를 보였다고 평가했다. 중국 내 권력서열 2, 3위인 리커창 총리와 장더장 전인대 상무위원장이 박 대통령과 면담하고 ‘한국 주도의 평화통일’ 등 정치적으로 미묘한 사안에 대해서도 긍정적인 입장을 보인 것은 한국의 전략적 요구에 접근하려고 노력한 것이라고 서 교수는 분석했다. 서 교수는 박 대통령이 취임 이후 일본보다 중국을 먼저 방문한 점에 대해 “사실상 지금까지 우리 외교가 미·일에 지나치게 집중된 부분이 있었던 만큼 중국을 중요시 여기는 것은 당연하다”며 “우리 외교가 나아갈 방향의 일부를 보여준 것”이라고 평가했다.서 교수는 다만 “중국은 북한 핵문제에 대해 과거 ‘협상용’으로 보던 입장에서 3차 핵실험을 거치면서 ‘핵무장용’이라는 의심을 갖게 된 상태인 것은 맞다”며 “중국으로서는 북핵을 저지하면서도 북한 체제를 붕괴시키지 않아야 한다는 딜레마를 안고 있다”고 분석했다. 이어 “과거처럼 중국이 일방적으로 북한을 옹호하던 데서 벗어나 북핵문제에 대해 압박을 가할 정도로 변화한 점은 우리에게 유리한 국면인 것이 사실인 만큼 앞으로도 중국과 긴밀한 협력이 중요하다”고 지적했다.\\n-----\\n문서2: 중화인민공화국과 조선민주주의인민공화국은 군사, 문화, 교육, 과학기술, 체육 등 모든 영역에서 밀접한 관계를 맺고 있다. 양국은 경제 문화 협조에 관한 협정 우호, 협조 및 호상원조에 관한 조약 , 통상항해협정, 항공운수협정, 영사협정 등 여러 가지 조약과 협정을 체결하였으며 또한 매년 교육, 과학기술, 체육 등 부문의 교류 계획을 체결하고 있다. 중국은 북조선 함경북도 청진시에, 북조선은 중국 랴오닝성 선양 시, 그리고 홍콩에 각각 총영사관을 두고 있다. 중국 랴오닝성은 북조선의 평안북도, 헤이룽장성은 함경북도, 장쑤성은 강원도, 상하이 시는 함경남도 함흥시와 친선 관계를 맺기도 하였다.\\n\\n김정은 북한 노동당 위원장이 지난 25일부터 나흘간 중국을 비공식 방문했다고 중국과 북한이2018년3월28일 관영매체들을 통해 발표했다. 북한 조선중앙통신, 중국 중앙(CC)TV와 신화통신 등에 따르면 김 위원장은 부인 리설주와 함께 특별열차 편으로 26일 중국 베이징(北京)에 도착했다. 중국은 김 위원장의 첫 방문을 대대적으로 환영했다. 왕후닝(王호<삼수변+扈>寧) 중국공산당 정치국 상무위원, 딩쉐샹(丁薛祥) 중국공산당 중앙판공청 주임, 쑹타오(宋濤) 중국공산당 대외연락부장, 리진쥔(李進軍) 북한 주재 중국대사 등이 베이징역에서 김 위원장 일행을 영접했다. 김 위원장과 시 주석의 첫 정상회담에서는 한반도 문제가 주요 의제로 올랐다. 아울러 두 정상은 냉랭해진 북중 관계를 복원하자는 취지의 다짐을 하며 화해의 손을 내밀기도 했다. 다만 중국 매체들은 한반도 비핵화 논의에 관한 두 정상의 발언을 상세히 전한 반면, 북한 매체는 북중 정상회담 자체와 북중 간 전통적 우호관계에만 집중하는 차이를 보였다.\\n-----\\n문서3: 김남구 한국투자금융지주 부회장(52)의 여권엔 중국 출입국 도장 수십여개가 찍혀 있다. 칭화대가 중국뿐 아니라 해외 고위급 인사를 대상으로 개설한 E-MBA 과정을 수강하기 위해 지난해 6월부터 베이징 서우두공항을 수시로 드나들어서다. 그는 한 달에 한 번 1박2일 또는 2박3일씩 중국에 머물면서 중국 기업 사장, 금융회사 임직원 등으로 구성된 ‘퉁쉐(同學·동기생)’들과 친분을 쌓고 있다.○중국 MBA 떠나는 CEO들김 부회장처럼 중국 명문대 경영학석사(MBA)나 최고경영자 과정에 등록, 한 달에 한 번씩 베이징이나 상하이를 방문해 공부하는 ‘차이나 셔틀’ 유학파가 늘고 있다. 중국의 금융 엘리트들과 두터운 인맥을 형성함과 동시에 현지의 살아 있는 정보를 얻을 수 있어서다. ‘셔틀 동문’들은 금융투자업계에서 내로라하는 인물들이다. 베이징 인민대에서 최고경영자 과정을 마친 정유신 한국벤처투자 사장, 상하이교통대 E-MBA 출신인 조용준 하나대투증권 전무, 칭화대와 푸단대에서 석·박사 학위를 받은 전병서 전 대우증권 전무 등이 핵심 멤버로 꼽힌다.강방천 에셋플러스자산운용 회장도 지난 3월부터 상하이 푸단대의 최고경영자 과정에 등록해 한 달에 3~7일씩 상하이에 머물고 있다. 정식 학위 과정은 아니지만 100% 중국어로 수업이 진행되는 제대로 된 과정이다. 강 회장은 이를 위해 2013년부터 틈나는 대로 중국어를 배웠다는 후문이다. 부장 이하급 간부들이 ‘중국 셔틀’에 합류하는 사례도 늘고 있다. 최근 칭화대의 ‘사모펀드와 고급자본투자전략’ 과정을 수료한 김혜원 한국투자신탁운용 채널영업본부 부장이 대표적인 사례다.‘차이나 셔틀’ 유학파들은 기존 중국 전문가들과 함께 국내에서 중국 연구모임을 만들고 활발히 활동하며 ‘중국 자본시장 진출의 첨병’ 역할을 하고 있다. 현재 국내 자본시장에서 ‘중국통(通)’이라고 불리는 전문가들은 대부분 중국자본시장연구회 소속이다. 연구회는 중국 사회과학원에서 석·박사 학위를 받은 임병익 금융투자협회 조사연구실장이 주축이 돼 2009년 발족됐다. 금융위원회 출신으로 대표적인 ‘국제통’으로 불린 유재훈 한국예탁결제원 사장이 연구회 회장을 맡고 있으며 회원은 50여명이다. 매달 열리는 조찬 모임엔 꾸준히 20명 이상 참석한다.최근엔 중국 사회과학원 경제연구소장, 푸단대 교수 등 중국 경제 관련 저명인사들이 한국을 찾으면 중국자본시장연구회가 주최하는 비공개 세미나에 참석할 만큼, 중국 내에서도 전문성을 인정받고 있다. 중국 관련 연구 용역을 의뢰하는 기업도 늘어나는 추세다.○“중국 알지 못하면 성공 못 해”금융투자업계 종사자들이 중국에 관심을 쏟는 이유는 중국의 영향력이 자본시장으로 확대되고 있어서다. 위안화 국제화, 중국 자본시장 개방 등의 조치가 가시화되면 한국과 중국 자본시장의 관계가 지금보다 밀접해질 것이라는 설명이다.중국 금융 엘리트들과의 인맥 형성도 자본시장 종사자들이 ‘차이나 셔틀 유학’을 결심하게 하는 이유다. 강 회장은 “중국은 한국의 미래에 중대한 영향을 주는 국가”라며 “중국 관련 펀드를 잘 운용하기 위해서라도 현지 전문가들의 ‘관시(關係·관계)’가 필요하다”고 말했다.‘차이나 셔틀 유학’ 성과는 금융상품에도 영향을 주고 있다. 자본시장연구회 주요 멤버인 조용준 전무는 작년 중국 주식시장의 내수업종 1등주에 투자하는 ‘중국 1등주 신탁’을 내놓았다. 한국투자금융지주 계열사인 한국투신운용이 업계 최초로 중국 본토 주식시장에 투자하는 상장지수펀드(ETF)를 내놓은 것도 김남구 부회장의 후원 덕분이라는 후문이다.',\n",
       "   'role': 'system'},\n",
       "  {'content': '김 위원장이 중국을 처음 방문할 때 도착했던 곳은?', 'role': 'user'},\n",
       "  {'content': '김정은 북한 노동당 위원장이 중국을 처음 방문했을 때 도착한 곳은 베이징입니다. 그는 부인 리설주와 함께 특별열차를 타고 2018년 3월 26일 베이징에 도착했습니다. [[ref2]]',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e8121b-9fa7-47b8-a43a-2e22dcfc3079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84abe116f5f4a82903d99d635a20674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2-7B-Instruct\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855ab294-db13-4217-8cfe-a8e1f89ff6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: “박근혜 대통령의 방중을 통해 북핵문제에 대해 한·미·중 삼각협력체제가 형성될 수 있는 기반이 마련된 것은 외교적 업적이 분명하다.” 서진영 고려대 명예교수(71·사진)는 30일 “박 대통령의 방중이 상당히 만족할 만한 효과를 거뒀다”고 평가하며 이같이 말했다. 서 교수는 중국 정치와 미·중관계 등을 연구해온 원로 중국전문가다. 김영삼 정부에서 대통령 자문 정책기획위원장을, 이명박 정부에서 한·중 전문가 공동연구위원회 한국 측 위원장을 역임했다. 서 교수는 한·미·중 정상이 연쇄적으로 회동하면서 북한 핵문제에 대해 일치된 목소리를 낸 것에 가장 큰 의미를 부여했다. 그는 “이번 한·중 정상회담에서 ‘북한 핵을 절대 용납할 수 없다’는 원칙을 ‘한·중 미래비전 공동선언’에 담지는 못했지만 박 대통령이 기자회견에서 직접 언급했다는 점에서 중국이 간접적으로 인정했다고 볼 수 있다”며 “과거보다 중국이 북핵문제에 대해 훨씬 전향적인 자세를 보인 것”이라고 말했다.특히 중국 측이 박 대통령에게 보인 환대를 주목했다. 서 교수는 “시진핑 주석은 박 대통령과 7, 8시간을 같이 보냈고 부인인 펑리위안 여사와도 함께 만나는 등 ‘패밀리 미팅’ 같은 분위기까지 만들어내려 했다”며 상당한 성의를 보였다고 평가했다. 중국 내 권력서열 2, 3위인 리커창 총리와 장더장 전인대 상무위원장이 박 대통령과 면담하고 ‘한국 주도의 평화통일’ 등 정치적으로 미묘한 사안에 대해서도 긍정적인 입장을 보인 것은 한국의 전략적 요구에 접근하려고 노력한 것이라고 서 교수는 분석했다. 서 교수는 박 대통령이 취임 이후 일본보다 중국을 먼저 방문한 점에 대해 “사실상 지금까지 우리 외교가 미·일에 지나치게 집중된 부분이 있었던 만큼 중국을 중요시 여기는 것은 당연하다”며 “우리 외교가 나아갈 방향의 일부를 보여준 것”이라고 평가했다.서 교수는 다만 “중국은 북한 핵문제에 대해 과거 ‘협상용’으로 보던 입장에서 3차 핵실험을 거치면서 ‘핵무장용’이라는 의심을 갖게 된 상태인 것은 맞다”며 “중국으로서는 북핵을 저지하면서도 북한 체제를 붕괴시키지 않아야 한다는 딜레마를 안고 있다”고 분석했다. 이어 “과거처럼 중국이 일방적으로 북한을 옹호하던 데서 벗어나 북핵문제에 대해 압박을 가할 정도로 변화한 점은 우리에게 유리한 국면인 것이 사실인 만큼 앞으로도 중국과 긴밀한 협력이 중요하다”고 지적했다.\n",
      "-----\n",
      "문서2: 중화인민공화국과 조선민주주의인민공화국은 군사, 문화, 교육, 과학기술, 체육 등 모든 영역에서 밀접한 관계를 맺고 있다. 양국은 경제 문화 협조에 관한 협정 우호, 협조 및 호상원조에 관한 조약 , 통상항해협정, 항공운수협정, 영사협정 등 여러 가지 조약과 협정을 체결하였으며 또한 매년 교육, 과학기술, 체육 등 부문의 교류 계획을 체결하고 있다. 중국은 북조선 함경북도 청진시에, 북조선은 중국 랴오닝성 선양 시, 그리고 홍콩에 각각 총영사관을 두고 있다. 중국 랴오닝성은 북조선의 평안북도, 헤이룽장성은 함경북도, 장쑤성은 강원도, 상하이 시는 함경남도 함흥시와 친선 관계를 맺기도 하였다.\n",
      "\n",
      "김정은 북한 노동당 위원장이 지난 25일부터 나흘간 중국을 비공식 방문했다고 중국과 북한이2018년3월28일 관영매체들을 통해 발표했다. 북한 조선중앙통신, 중국 중앙(CC)TV와 신화통신 등에 따르면 김 위원장은 부인 리설주와 함께 특별열차 편으로 26일 중국 베이징(北京)에 도착했다. 중국은 김 위원장의 첫 방문을 대대적으로 환영했다. 왕후닝(王호<삼수변+扈>寧) 중국공산당 정치국 상무위원, 딩쉐샹(丁薛祥) 중국공산당 중앙판공청 주임, 쑹타오(宋濤) 중국공산당 대외연락부장, 리진쥔(李進軍) 북한 주재 중국대사 등이 베이징역에서 김 위원장 일행을 영접했다. 김 위원장과 시 주석의 첫 정상회담에서는 한반도 문제가 주요 의제로 올랐다. 아울러 두 정상은 냉랭해진 북중 관계를 복원하자는 취지의 다짐을 하며 화해의 손을 내밀기도 했다. 다만 중국 매체들은 한반도 비핵화 논의에 관한 두 정상의 발언을 상세히 전한 반면, 북한 매체는 북중 정상회담 자체와 북중 간 전통적 우호관계에만 집중하는 차이를 보였다.\n",
      "-----\n",
      "문서3: 김남구 한국투자금융지주 부회장(52)의 여권엔 중국 출입국 도장 수십여개가 찍혀 있다. 칭화대가 중국뿐 아니라 해외 고위급 인사를 대상으로 개설한 E-MBA 과정을 수강하기 위해 지난해 6월부터 베이징 서우두공항을 수시로 드나들어서다. 그는 한 달에 한 번 1박2일 또는 2박3일씩 중국에 머물면서 중국 기업 사장, 금융회사 임직원 등으로 구성된 ‘퉁쉐(同學·동기생)’들과 친분을 쌓고 있다.○중국 MBA 떠나는 CEO들김 부회장처럼 중국 명문대 경영학석사(MBA)나 최고경영자 과정에 등록, 한 달에 한 번씩 베이징이나 상하이를 방문해 공부하는 ‘차이나 셔틀’ 유학파가 늘고 있다. 중국의 금융 엘리트들과 두터운 인맥을 형성함과 동시에 현지의 살아 있는 정보를 얻을 수 있어서다. ‘셔틀 동문’들은 금융투자업계에서 내로라하는 인물들이다. 베이징 인민대에서 최고경영자 과정을 마친 정유신 한국벤처투자 사장, 상하이교통대 E-MBA 출신인 조용준 하나대투증권 전무, 칭화대와 푸단대에서 석·박사 학위를 받은 전병서 전 대우증권 전무 등이 핵심 멤버로 꼽힌다.강방천 에셋플러스자산운용 회장도 지난 3월부터 상하이 푸단대의 최고경영자 과정에 등록해 한 달에 3~7일씩 상하이에 머물고 있다. 정식 학위 과정은 아니지만 100% 중국어로 수업이 진행되는 제대로 된 과정이다. 강 회장은 이를 위해 2013년부터 틈나는 대로 중국어를 배웠다는 후문이다. 부장 이하급 간부들이 ‘중국 셔틀’에 합류하는 사례도 늘고 있다. 최근 칭화대의 ‘사모펀드와 고급자본투자전략’ 과정을 수료한 김혜원 한국투자신탁운용 채널영업본부 부장이 대표적인 사례다.‘차이나 셔틀’ 유학파들은 기존 중국 전문가들과 함께 국내에서 중국 연구모임을 만들고 활발히 활동하며 ‘중국 자본시장 진출의 첨병’ 역할을 하고 있다. 현재 국내 자본시장에서 ‘중국통(通)’이라고 불리는 전문가들은 대부분 중국자본시장연구회 소속이다. 연구회는 중국 사회과학원에서 석·박사 학위를 받은 임병익 금융투자협회 조사연구실장이 주축이 돼 2009년 발족됐다. 금융위원회 출신으로 대표적인 ‘국제통’으로 불린 유재훈 한국예탁결제원 사장이 연구회 회장을 맡고 있으며 회원은 50여명이다. 매달 열리는 조찬 모임엔 꾸준히 20명 이상 참석한다.최근엔 중국 사회과학원 경제연구소장, 푸단대 교수 등 중국 경제 관련 저명인사들이 한국을 찾으면 중국자본시장연구회가 주최하는 비공개 세미나에 참석할 만큼, 중국 내에서도 전문성을 인정받고 있다. 중국 관련 연구 용역을 의뢰하는 기업도 늘어나는 추세다.○“중국 알지 못하면 성공 못 해”금융투자업계 종사자들이 중국에 관심을 쏟는 이유는 중국의 영향력이 자본시장으로 확대되고 있어서다. 위안화 국제화, 중국 자본시장 개방 등의 조치가 가시화되면 한국과 중국 자본시장의 관계가 지금보다 밀접해질 것이라는 설명이다.중국 금융 엘리트들과의 인맥 형성도 자본시장 종사자들이 ‘차이나 셔틀 유학’을 결심하게 하는 이유다. 강 회장은 “중국은 한국의 미래에 중대한 영향을 주는 국가”라며 “중국 관련 펀드를 잘 운용하기 위해서라도 현지 전문가들의 ‘관시(關係·관계)’가 필요하다”고 말했다.‘차이나 셔틀 유학’ 성과는 금융상품에도 영향을 주고 있다. 자본시장연구회 주요 멤버인 조용준 전무는 작년 중국 주식시장의 내수업종 1등주에 투자하는 ‘중국 1등주 신탁’을 내놓았다. 한국투자금융지주 계열사인 한국투신운용이 업계 최초로 중국 본토 주식시장에 투자하는 상장지수펀드(ETF)를 내놓은 것도 김남구 부회장의 후원 덕분이라는 후문이다.<|im_end|>\n",
      "<|im_start|>user\n",
      "김 위원장이 중국을 처음 방문할 때 도착했던 곳은?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "김정은 북한 노동당 위원장이 중국을 처음 방문했을 때 도착한 곳은 베이징입니다. 그는 부인 리설주와 함께 특별열차를 타고 2018년 3월 26일 베이징에 도착했습니다. [[ref2]]<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5824e560-3b64-49ee-88dc-48223c164af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ab0ded-743c-4986-a79e-fe65a6621d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-rag-ko\",\n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    bf16=True,\n",
    "    learning_rate=1e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cadeafd6-9d89-4d7a-8319-e65fa040c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2de5f0f-5df5-4bc2-a146-e25d41e2db96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 2998])\n",
      "어텐션 마스크 형태: torch.Size([1, 2998])\n",
      "레이블 형태: torch.Size([1, 2998])\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=8192\n",
    "\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db09c1b9-7127-4df5-b4a6-c25f18fe26e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[151644, 8948, 198, 64795, 82528, 33704, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 138520, 19391, 143604, 129264, 130650, 382, 13146, 48431, 20401, 66790, 29326, 131193, 17877, 125686, 125548, 139713, 624, 16, 13, 138520, 53680, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 143604, 16186, 139713, 624, 17, 13, 85322, 77226, 98801, 19391, 130768, 130213, 17877, 143604, 16186, 125476, 34395, 53900, 21329, 95577, 139713, 624, 18, 13, 138520, 19391, 128605, 143603, 12802, 85322, 77226, 98801, 19391, 130671, 32290, 85322, 77226, 98801, 126377, 330, 33883, 64795, 138520, 93, 19391, 128605, 130213, 12802, 136673, 1189, 5140, 45881, 34395, 143604, 16186, 139713, 624, 19, 13, 143604, 47836, 53618, 142976, 139236, 18411, 142616, 82190, 53435, 40853, 129549, 53435, 125068, 17877, 140174, 128836, 32290, 5140, 240, 97, 19391, 36330, 250, 125746, 16560, 23084, 126402, 83634, 17380, 94613, 139236, 84621, 47324, 18411, 129624, 20487, 139713, 13, 95617, 18411, 129901, 26698, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 56475, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 5053, 130939, 54116, 57132, 16186, 139713, 624, 20, 13, 95617, 18411, 129901, 26698, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 80573, 220, 20, 42044, 139236, 56475, 143409, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 20492, 4318, 1097, 20, 5053, 130939, 54116, 57132, 16186, 139713, 624, 21, 13, 81173, 66845, 23573, 49367, 23259, 20401, 139236, 18411, 58677, 26699, 82190, 143604, 16186, 139713, 382, 129845, 77226, 98801, 510, 34764, 51588, 26698, 16, 25, 1036, 129471, 125722, 135508, 136774, 20401, 74808, 126402, 17877, 131582, 139963, 138014, 51588, 37087, 19391, 131978, 61298, 13935, 56039, 13935, 126402, 127165, 120, 126317, 129640, 28754, 49543, 37087, 19969, 141965, 32831, 131396, 28733, 64521, 54116, 38177, 62618, 140175, 52300, 129274, 74884, 116, 124546, 80968, 24485, 227, 133864, 128618, 79632, 129330, 1987, 89860, 85251, 125144, 126429, 125476, 66845, 130345, 127027, 124546, 23259, 7, 22, 16, 13935, 55054, 85251, 8, 16560, 220, 18, 15, 32077, 1036, 129471, 136774, 20401, 74808, 126402, 12802, 58034, 64795, 125511, 62107, 129704, 47836, 62107, 23573, 141541, 18411, 126352, 145102, 13146, 854, 34395, 69441, 231, 19969, 130705, 23084, 131380, 12802, 126254, 128836, 13, 89860, 142220, 16560, 138373, 136994, 80573, 125714, 13935, 126402, 130891, 134454, 131698, 33883, 130000, 129093, 17380, 138373, 133081, 19969, 13146, 13, 130508, 125144, 134088, 36055, 63089, 56475, 136774, 64577, 51588, 36055, 126712, 20487, 127324, 125826, 137471, 11, 23084, 79632, 129471, 36055, 63089, 56475, 61298, 13935, 126402, 56419, 51588, 19969, 140429, 137650, 133373, 130092, 3315, 116, 94, 143128, 137471, 127864, 93701, 128836, 13, 89860, 142220, 16560, 61298, 13935, 56039, 13935, 126402, 36055, 55902, 12802, 77353, 144165, 128552, 98005, 57089, 132537, 139964, 20136, 113, 51588, 37087, 19391, 131978, 83556, 59698, 52300, 134952, 43590, 133886, 37195, 116, 71108, 19391, 130887, 132182, 132989, 18411, 85403, 57026, 128836, 13, 54825, 16560, 1036, 12802, 42044, 61298, 13935, 126402, 36055, 55902, 61741, 125786, 56475, 3369, 131226, 23573, 20136, 113, 17877, 18585, 230, 66845, 65722, 102, 139798, 47836, 28733, 130671, 527, 16560, 129093, 135092, 17877, 3369, 23573, 13935, 126402, 143005, 70582, 65865, 140429, 125519, 129709, 527, 19391, 34143, 112, 130974, 129293, 125580, 125590, 22042, 243, 136774, 12802, 54116, 25715, 61741, 126110, 56475, 137352, 139957, 128911, 128836, 16560, 18585, 238, 56475, 138373, 12802, 16778, 226, 127302, 128552, 139180, 128836, 34395, 30520, 120, 28733, 90686, 854, 124905, 1036, 53680, 92192, 129885, 138373, 12802, 139963, 138014, 51588, 37087, 19391, 131978, 10764, 249, 101, 144915, 56419, 129321, 128533, 64577, 41429, 18411, 63332, 31328, 71108, 854, 130939, 126254, 128836, 13, 126952, 125511, 138373, 3315, 116, 94, 12802, 22042, 243, 136774, 126327, 63332, 31328, 46832, 246, 66845, 18411, 55673, 87608, 128836, 13, 89860, 142220, 16560, 1036, 29326, 85251, 144007, 55673, 129150, 33704, 22042, 243, 136774, 53680, 220, 22, 11, 220, 23, 134745, 17877, 131050, 63332, 144107, 34395, 85403, 31328, 31328, 10764, 236, 239, 28002, 80901, 126246, 83518, 55054, 80573, 47985, 129676, 142353, 16560, 77002, 3369, 133087, 134372, 28002, 125714, 138694, 527, 129381, 128618, 80901, 20487, 128878, 134108, 95218, 125476, 10764, 244, 19946, 854, 124905, 58034, 64795, 23573, 128677, 20401, 18411, 63332, 124982, 131042, 69441, 231, 19969, 128836, 13, 138373, 66136, 142452, 28754, 26698, 53955, 220, 17, 11, 220, 18, 80901, 31328, 56983, 131973, 130095, 3315, 112, 251, 28002, 80573, 129359, 125932, 40853, 56419, 31328, 66845, 58034, 125054, 139748, 12802, 22042, 243, 136774, 53680, 48108, 112, 125786, 126204, 3369, 23573, 124785, 55673, 47985, 20401, 69441, 231, 56290, 125160, 32077, 527, 77002, 136994, 128552, 125714, 144149, 23573, 32129, 126246, 19391, 130869, 47985, 40771, 235, 29281, 128533, 38150, 137471, 63332, 31328, 129274, 130092, 20401, 56419, 138279, 80968, 135797, 19391, 143189, 16186, 125476, 34395, 136111, 23573, 71108, 130939, 89860, 142220, 16560, 128618, 129150, 128836, 13, 89860, 142220, 16560, 22042, 243, 136774, 12802, 131565, 93701, 136331, 136293, 129885, 138373, 17877, 137769, 141875, 23573, 18585, 238, 19391, 131978, 1036, 55054, 125086, 55902, 129082, 128878, 124830, 74884, 116, 124546, 19969, 125714, 13935, 32077, 19391, 66790, 60315, 59698, 57801, 130263, 126402, 52300, 129881, 12802, 132236, 125615, 62107, 127908, 138373, 17877, 127840, 29326, 131180, 16560, 129274, 125834, 125568, 129330, 854, 124905, 1036, 40281, 28002, 74884, 116, 124546, 19969, 73518, 52959, 131498, 74808, 129321, 20401, 136605, 18411, 63332, 57026, 129044, 71108, 854, 130939, 69441, 231, 19969, 128836, 13, 26698, 142220, 16560, 139293, 1036, 126402, 124785, 33704, 139964, 20136, 113, 51588, 37087, 19391, 131978, 45130, 120, 92192, 3369, 129640, 55902, 26699, 527, 42039, 63332, 125615, 140585, 56475, 220, 18, 125625, 20136, 113, 21530, 44680, 245, 246, 17877, 126352, 59698, 131611, 3369, 138014, 125054, 40853, 26699, 527, 131411, 124970, 125512, 17877, 143143, 57801, 130722, 137619, 31328, 129274, 131417, 13146, 854, 124905, 1036, 126402, 124785, 42039, 26698, 16560, 139963, 138014, 17877, 125569, 21329, 132537, 47985, 139964, 48364, 112, 37087, 18411, 62740, 243, 142194, 135298, 21329, 50696, 52959, 89659, 129112, 16560, 76497, 250, 126673, 125544, 18411, 95170, 34395, 90686, 854, 34395, 128618, 129150, 128836, 13, 23084, 31079, 1036, 53680, 92192, 131137, 138373, 12802, 83556, 126321, 128552, 139964, 17877, 38523, 117, 47324, 16186, 125615, 5140, 41902, 26698, 47665, 245, 31079, 60315, 139963, 138014, 51588, 37087, 19391, 131978, 23872, 243, 129471, 17877, 35509, 47836, 131219, 17380, 137525, 23573, 18585, 238, 33704, 124830, 126327, 126310, 28002, 23573, 124973, 32290, 31328, 128584, 130037, 31328, 62107, 127908, 139275, 47985, 138373, 53680, 40771, 112, 134372, 23573, 47455, 239, 28754, 12802, 127840, 129330, 854, 34395, 66790, 80968, 128836, 624, 34764, 51588, 26698, 17, 25, 70943, 56290, 31328, 125496, 78125, 56290, 124785, 53680, 65510, 125519, 125496, 54330, 130788, 31328, 125496, 78125, 56290, 124785, 33704, 124459, 108, 55054, 11, 53435, 56290, 11, 127048, 126596, 11, 45130, 120, 124632, 131040, 11, 48364, 112, 126596, 77002, 129304, 126440, 126346, 56475, 22042, 222, 127302, 23573, 92751, 124781, 18411, 32985, 118, 34395, 90686, 13, 79302, 239, 124785, 33704, 43115, 37087, 53435, 56290, 47455, 239, 92817, 19391, 130207, 47455, 239, 29281, 124657, 47324, 11, 47455, 239, 92817, 128355, 91043, 55902, 54321, 92817, 19391, 130207, 65510, 125535, 1154, 125206, 55902, 126524, 33883, 129640, 29281, 11, 142654, 78125, 93672, 23259, 129640, 29281, 11, 126440, 55054, 129640, 29281, 77002, 127296, 127154, 65510, 125535, 53680, 47455, 239, 29281, 17877, 48364, 112, 88781, 126063, 127378, 130005, 126932, 126216, 127048, 126596, 11, 45130, 120, 124632, 131040, 11, 48364, 112, 126596, 77002, 85403, 51588, 20401, 127048, 97929, 94203, 127324, 17877, 48364, 112, 88781, 126204, 90686, 13, 138373, 33704, 139963, 92817, 125519, 50972, 65306, 131226, 47985, 48364, 255, 85251, 29326, 19391, 11, 139963, 92817, 125519, 33704, 138373, 5140, 252, 112, 57268, 144262, 32831, 129296, 126345, 44518, 11, 128719, 46832, 235, 144400, 19391, 126804, 126317, 3315, 112, 251, 125144, 55054, 124780, 17877, 129419, 34395, 90686, 13, 138373, 5140, 252, 112, 57268, 144262, 32831, 33704, 139963, 92817, 125519, 20401, 69441, 231, 126246, 131226, 47985, 11, 10764, 245, 97, 12802, 145873, 40853, 32831, 33704, 50972, 65306, 131226, 47985, 11, 129359, 145163, 32831, 33704, 129413, 54321, 47985, 11, 58034, 16186, 12802, 44518, 16560, 50972, 65306, 131793, 47985, 50972, 138609, 29326, 80573, 90711, 250, 125519, 92751, 124781, 18411, 32985, 118, 130898, 53900, 139836, 382, 138329, 29281, 33704, 139964, 127042, 57089, 64795, 143128, 40853, 12802, 133146, 220, 17, 20, 32077, 126558, 73518, 144136, 62275, 138373, 17877, 73986, 78125, 76337, 141875, 128836, 34395, 138373, 53680, 139964, 12802, 17, 15, 16, 23, 126216, 18, 128514, 17, 23, 32077, 92751, 125144, 129865, 49543, 129125, 131582, 142234, 128836, 13, 139964, 65510, 125519, 142702, 139269, 11, 138373, 70943, 128205, 74071, 8, 15653, 80573, 128753, 56290, 139269, 77002, 19391, 125686, 125548, 32290, 130508, 143128, 40853, 33704, 85403, 31328, 56983, 125624, 54330, 80573, 129676, 127820, 126591, 53955, 125625, 10764, 236, 116, 42039, 220, 17, 21, 32077, 138373, 47665, 254, 12802, 135946, 7, 68990, 8, 19391, 129392, 135375, 128836, 13, 138373, 33704, 130508, 143128, 40853, 20401, 48364, 104, 141875, 17877, 60960, 66845, 128552, 46832, 246, 125144, 128836, 13, 74884, 243, 127033, 144262, 7, 99445, 47324, 27, 134088, 23259, 126667, 10, 120799, 29, 112085, 8, 138373, 78125, 85057, 64795, 136994, 124785, 58034, 125054, 125826, 11, 76497, 102, 144838, 145531, 7, 101142, 106929, 102197, 8, 138373, 78125, 85057, 64795, 70943, 128205, 129382, 78125, 125118, 55673, 93701, 11, 3315, 239, 117, 125166, 57268, 7, 73038, 99275, 97, 8, 138373, 78125, 85057, 64795, 60960, 128792, 125568, 132772, 43752, 54470, 11, 56983, 85251, 145451, 7, 100339, 100814, 105343, 8, 139964, 55673, 57132, 138373, 66845, 55054, 77002, 12802, 47665, 254, 12802, 135946, 126346, 56475, 130508, 143128, 40853, 83556, 124528, 17877, 126440, 127302, 128836, 13, 130508, 143128, 40853, 53680, 44518, 55673, 129150, 20401, 48364, 104, 36055, 55902, 61741, 125786, 129889, 61298, 126641, 47985, 137903, 55673, 35711, 124970, 37087, 17380, 38523, 105, 144110, 13146, 13, 48408, 126893, 60294, 129419, 36055, 55902, 33704, 5140, 225, 231, 144710, 33883, 85251, 139963, 126402, 92751, 124781, 18411, 30520, 113, 54321, 16186, 132343, 131565, 21329, 20401, 49367, 144042, 17877, 53900, 124905, 46832, 242, 33883, 20401, 77596, 238, 17877, 66136, 134372, 130898, 10764, 244, 19946, 13, 139293, 138373, 126932, 49543, 128901, 61298, 126641, 47985, 73986, 138014, 56290, 127041, 120, 20401, 19391, 130207, 129419, 36055, 55902, 20401, 95996, 129709, 17877, 58034, 41429, 125511, 56419, 23573, 63757, 32290, 11, 139964, 126932, 49543, 16560, 139963, 126402, 36055, 55902, 61741, 125786, 136751, 80573, 139963, 126402, 16778, 226, 56419, 125160, 80968, 124657, 47324, 130891, 19391, 72553, 130263, 126402, 42905, 129882, 12802, 18411, 63332, 139836, 624, 34764, 51588, 26698, 18, 25, 130508, 131793, 88259, 130092, 129924, 25715, 138870, 21329, 54330, 85403, 61741, 40853, 7, 20, 17, 8, 20401, 83518, 128739, 136233, 138373, 36330, 250, 43866, 124785, 129392, 40853, 28733, 127421, 57026, 59761, 19969, 62099, 235, 128173, 90686, 13, 90711, 255, 56290, 66845, 19969, 138373, 127871, 130651, 60716, 128792, 126429, 80901, 128911, 58677, 134445, 60960, 55902, 42039, 73523, 125624, 23573, 468, 5251, 7064, 45130, 120, 29281, 17877, 28733, 130262, 66425, 130039, 133146, 33883, 220, 21, 128514, 126558, 47665, 254, 12802, 135946, 89860, 40281, 126923, 78125, 126524, 17877, 28733, 29326, 17380, 55838, 250, 60315, 64850, 136108, 13146, 13, 54825, 16560, 61298, 34143, 105, 19391, 61298, 84621, 220, 16, 129471, 17, 32077, 129549, 220, 17, 129471, 18, 32077, 141906, 138373, 19391, 137767, 116, 126251, 131611, 138373, 54116, 124517, 32129, 40853, 11, 40771, 230, 128024, 131110, 16235, 226, 125545, 54321, 77002, 42039, 136239, 52300, 3369, 144975, 144838, 7, 116184, 13935, 57089, 20487, 76435, 8, 527, 134771, 90711, 250, 79716, 17877, 3315, 234, 241, 34395, 90686, 13, 136277, 126402, 124785, 53454, 5140, 244, 254, 60315, 16560, 12156, 64850, 138329, 85403, 61741, 40853, 131137, 138373, 130345, 51588, 66845, 43115, 125144, 124632, 129150, 55054, 3189, 7064, 8, 60315, 81173, 34395, 142408, 25715, 45130, 120, 29281, 19391, 77002, 49664, 11, 61298, 34143, 105, 19391, 61298, 84621, 141906, 47665, 254, 12802, 135946, 129835, 58034, 16186, 12802, 18411, 141875, 33883, 125466, 63089, 42905, 3369, 125625, 129835, 3315, 227, 242, 139982, 527, 126310, 124632, 126793, 19969, 143861, 246, 34395, 90686, 13, 138373, 20401, 40771, 230, 128024, 24485, 246, 28002, 28626, 134771, 129419, 33861, 93672, 58677, 144026, 17877, 141965, 32831, 77953, 53680, 143409, 141526, 21329, 20401, 137879, 64521, 60039, 18411, 79302, 119, 17877, 28733, 136633, 13146, 13, 3369, 135607, 139982, 126322, 51588, 527, 128901, 40771, 230, 128024, 129924, 25715, 124517, 124781, 56475, 66136, 17380, 50340, 42905, 58677, 126251, 64850, 125489, 13, 47665, 254, 12802, 135946, 58677, 125496, 66845, 56475, 81173, 34395, 142408, 25715, 45130, 120, 29281, 17877, 95577, 131344, 36055, 125522, 82528, 130092, 144145, 125746, 129924, 25715, 32129, 40853, 11, 58034, 16186, 12802, 142438, 66845, 468, 5251, 7064, 36330, 250, 82528, 31328, 65510, 26699, 129044, 125703, 66845, 129924, 128844, 128739, 56419, 125054, 11, 90711, 255, 56290, 66845, 80573, 10764, 239, 116, 125068, 66845, 56475, 27767, 251, 13935, 129471, 55054, 20136, 247, 80901, 18411, 83596, 33704, 56419, 127148, 26698, 56419, 60960, 40281, 128844, 128739, 56419, 125054, 77002, 12802, 20136, 113, 125512, 48108, 97, 79004, 17380, 8620, 120, 121, 144190, 13146, 13, 130262, 126321, 129034, 90486, 144245, 131587, 60294, 24897, 25715, 85057, 93672, 26699, 98005, 40853, 47985, 133146, 220, 18, 128514, 126558, 58034, 16186, 12802, 10764, 239, 116, 125068, 66845, 20401, 81173, 34395, 142408, 25715, 45130, 120, 29281, 19391, 77002, 49664, 33883, 61298, 34143, 105, 19391, 220, 18, 93, 22, 32077, 141906, 58034, 16186, 12802, 19391, 137767, 116, 126251, 34395, 90686, 13, 36055, 76337, 20136, 247, 80901, 45130, 120, 29281, 33704, 126523, 125590, 220, 16, 15, 15, 4, 138373, 31079, 17380, 28733, 124517, 12802, 132876, 128841, 62071, 129923, 130722, 45130, 120, 29281, 125489, 13, 129413, 98005, 40853, 33704, 136342, 130039, 220, 17, 15, 16, 18, 126216, 126558, 10764, 233, 230, 60315, 16560, 60960, 17380, 138373, 31079, 18411, 73669, 144035, 130822, 94315, 51588, 125489, 13, 62740, 54470, 23084, 16186, 128911, 16778, 226, 63089, 126253, 3369, 126402, 124785, 3315, 227, 242, 139982, 527, 19391, 20136, 102, 97929, 42905, 32129, 131000, 47985, 143861, 246, 34395, 90686, 13, 139465, 90711, 255, 56290, 66845, 20401, 3369, 55054, 129439, 144419, 29346, 80573, 126429, 128911, 25715, 125822, 129924, 25715, 65865, 138279, 527, 45130, 120, 29281, 17877, 28733, 63256, 23573, 130508, 135508, 54321, 130092, 129924, 25715, 21530, 57160, 36109, 93672, 26699, 3315, 109, 226, 139287, 125144, 124517, 125822, 63089, 62740, 54470, 12802, 60960, 126414, 128533, 32129, 131000, 13146, 13, 14009, 125625, 129835, 3315, 227, 242, 139982, 527, 126310, 124632, 126793, 128901, 54116, 130999, 138373, 56419, 51588, 19969, 134771, 129676, 141185, 56475, 138373, 131698, 129439, 93701, 17877, 127579, 34395, 140968, 126835, 125511, 140968, 57089, 130705, 3369, 126402, 124785, 64577, 125822, 133627, 126616, 69923, 20401, 48364, 101, 127148, 527, 127864, 47836, 17877, 130127, 90686, 13, 132270, 141185, 64577, 125822, 133627, 56475, 3369, 126402, 124785, 125160, 7, 31935, 8, 527, 130939, 126488, 132920, 56419, 51588, 19969, 128901, 140094, 138373, 25715, 125822, 133627, 137650, 61741, 126291, 126299, 125489, 13, 131698, 61741, 16560, 138373, 130592, 136581, 54321, 56475, 27767, 251, 13935, 129471, 55054, 20136, 247, 80901, 18411, 83596, 33704, 16235, 226, 127148, 131870, 40771, 230, 128024, 129924, 25715, 129640, 61741, 65510, 55054, 137650, 125086, 40853, 12802, 55673, 126970, 12802, 64805, 120, 220, 17, 15, 15, 24, 126216, 95996, 129704, 134521, 13146, 13, 40771, 230, 128024, 133373, 36330, 250, 82528, 42039, 60960, 126414, 128533, 3369, 124785, 37087, 125160, 527, 42039, 126488, 129807, 126310, 57132, 138108, 130092, 127027, 133376, 88781, 37087, 54321, 32129, 40853, 12802, 131698, 61741, 98005, 137471, 32985, 94, 34395, 132931, 133662, 33704, 220, 20, 15, 57026, 79632, 125489, 13, 126932, 129062, 130847, 132920, 65510, 138143, 54070, 93701, 136233, 8620, 122, 116, 129044, 125511, 220, 17, 15, 79632, 130408, 127969, 129150, 51876, 13, 128215, 125722, 136233, 138373, 130592, 136581, 54321, 43115, 37087, 137650, 43590, 40853, 11, 10764, 239, 116, 125068, 66845, 142220, 77002, 138373, 43115, 37087, 129985, 125569, 79632, 31328, 55054, 126253, 130092, 17877, 138037, 89940, 138373, 25715, 125822, 133627, 137650, 61741, 19969, 55673, 128215, 42905, 73986, 78125, 59761, 125674, 56039, 60315, 19391, 127969, 129150, 47836, 62107, 127908, 11, 138373, 66136, 136448, 56419, 51588, 132818, 139180, 132872, 34395, 90686, 13, 138373, 129985, 131698, 65722, 102, 126346, 17877, 124970, 144013, 42905, 54116, 124517, 47985, 143861, 246, 31079, 60315, 16560, 57835, 41429, 13146, 13, 136277, 2073, 126402, 124785, 125214, 21329, 129293, 126559, 128677, 78125, 129293, 60716, 854, 138870, 129924, 25715, 124517, 124781, 98358, 55054, 25715, 126253, 138373, 19391, 138449, 17877, 3315, 237, 253, 16560, 132819, 16560, 138373, 20401, 126440, 129321, 28754, 12802, 64577, 125822, 133627, 42039, 130729, 66845, 130357, 136633, 13146, 13, 45710, 126246, 56290, 138249, 56290, 11, 138373, 64577, 125822, 133627, 73523, 126321, 136357, 65510, 59698, 19969, 35509, 29326, 56290, 135229, 130092, 53680, 138373, 64577, 125822, 133627, 20401, 92751, 124781, 19969, 129082, 129885, 22042, 222, 127302, 33883, 128732, 71108, 131411, 133828, 125489, 13, 126402, 124785, 40771, 230, 128024, 24485, 246, 28002, 28626, 134771, 20401, 58677, 144026, 141965, 32831, 47985, 64577, 125822, 133627, 98358, 55054, 25715, 126253, 3369, 125625, 129835, 3315, 227, 242, 139982, 126310, 124632, 527, 17877, 82619, 125512, 128555, 128956, 132819, 13146, 13, 129413, 98005, 40853, 33704, 1036, 126402, 124785, 33704, 130092, 20401, 143005, 19391, 70943, 66845, 23573, 126440, 129321, 17877, 55673, 16560, 133152, 854, 50340, 124905, 1036, 126402, 124785, 129985, 10764, 236, 222, 29346, 18411, 126720, 132028, 26699, 66425, 133083, 127937, 141526, 21329, 56419, 51588, 19969, 129360, 3369, 124780, 29326, 7, 107287, 13935, 130891, 8, 527, 19969, 126871, 129330, 854, 34395, 126254, 128836, 13, 14009, 125625, 129835, 3315, 227, 242, 139982, 126310, 124632, 527, 128677, 53680, 16560, 40771, 230, 128024, 55902, 125678, 130612, 126440, 129321, 17877, 55673, 34395, 90686, 13, 64577, 125822, 133627, 137650, 61741, 55673, 35711, 48108, 97, 79004, 31328, 65510, 26699, 129044, 56419, 125054, 16560, 68232, 126216, 138373, 55673, 76337, 133627, 20401, 66136, 23259, 124517, 126337, 220, 16, 129510, 54330, 19391, 10764, 230, 105, 25715, 42905, 3369, 126402, 124785, 220, 16, 129510, 54330, 28927, 57160, 36109, 527, 17877, 66136, 133019, 137750, 13, 130092, 129924, 25715, 138870, 21329, 54330, 94203, 53955, 55054, 31328, 130092, 129924, 82528, 93672, 26699, 12802, 24485, 227, 124781, 81173, 132618, 17380, 138373, 129238, 129283, 55673, 76337, 133627, 19391, 10764, 230, 105, 25715, 42905, 58034, 40853, 21329, 23259, 144419, 29346, 7, 42239, 8, 18411, 66136, 133019, 33704, 137638, 130508, 131793, 88259, 85403, 61741, 40853, 20401, 94315, 54321, 126365, 243, 79716, 131411, 94315, 51588, 125489, 13, 151645, 198, 151644, 872, 198, 138329, 143128, 40853, 12802, 138373, 17877, 136065, 141875, 47836, 53618, 129392, 135375, 134965, 45130, 111, 33704, 30, 151645, 198, 151644, 77091, 198, 138329, 29281, 33704, 139964, 127042, 57089, 64795, 143128, 40853, 12802, 138373, 17877, 136065, 141875, 125580, 17877, 53618, 129392, 135375, 23573, 45130, 111, 33704, 47665, 254, 12802, 135946, 78952, 13, 54825, 16560, 85403, 31328, 56983, 125624, 54330, 80573, 129676, 127820, 126591, 53955, 125625, 18411, 74361, 222, 34395, 220, 17, 15, 16, 23, 126216, 220, 18, 128514, 220, 17, 21, 32077, 47665, 254, 12802, 135946, 19391, 129392, 135375, 133240, 13, 4318, 1097, 17, 5053, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b8400f7-e443-4d84-bf2b-100536882114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 138329, 29281, 33704, 139964, 127042, 57089, 64795, 143128, 40853, 12802, 138373, 17877, 136065, 141875, 125580, 17877, 53618, 129392, 135375, 23573, 45130, 111, 33704, 47665, 254, 12802, 135946, 78952, 13, 54825, 16560, 85403, 31328, 56983, 125624, 54330, 80573, 129676, 127820, 126591, 53955, 125625, 18411, 74361, 222, 34395, 220, 17, 15, 16, 23, 126216, 220, 18, 128514, 220, 17, 21, 32077, 47665, 254, 12802, 135946, 19391, 129392, 135375, 133240, 13, 4318, 1097, 17, 5053, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ed1406-e846-4886-8f88-57e4914afdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd7fdfd-02b6-4cf3-9f82-36e3db5e5de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 30:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.555300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.468300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.358800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.419800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.397900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.294500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcb8a743-19d1-4051-89bc-096c39a08d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5ba0b49-615c-406b-8898-8cc31d662d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: “업사이클링은 새로운 디자인 패러다임으로 자리매김할 것입니다.”이태용 한국디자인진흥원장(사진)은 업사이클링에 대해 “투자 대비 부가가치가 높고 환경문제를 해결하기 위한 촉매제 역할을 할 것”이라며 이같이 말했다. 이 원장은 “꾸준한 연구와 시범사업 등을 통해 업사이클링 산업을 육성해 나가겠다”고 덧붙였다.현재 국내 업체들은 업사이클링 제품의 대량 생산과 유통망 확보에 큰 어려움을 겪고 있다. 이 원장은 “업사이클링 분야에서는 1인 기업이 많아 대량 생산에 한계가 있다”며 “오프라인 매장을 두지 못하고 온라인을 통해서만 판매를 하는 업체들이 많다”고 설명했다.이 같은 문제점을 해결하기 위해 한국디자인진흥원은 작년부터 ‘업사이클 디자인 사업’을 진행하고 있다. 지난 10월 열린 ‘디자인코리아 2013’에서 업사이클디자인관을 마련해 홍보를 도왔다. 전문인력 양성을 위한 세미나도 개최했다. 국내 인프라 구축을 위해 다양한 연구 활동도 하고 있다. 이 원장은 “업사이클링 산업이 발전하면 새로운 시장과 일자리가 창출될 수 있을 것”이라며 “마케팅 부문 등에서 실질적인 도움을 주는 데 주력하고 업사이클링에 대한 인식 개선을 위해서도 노력하겠다”고 강조했다.국내 업사이클 산업 규모가 커지면 해외처럼 유명 업사이클링 업체도 등장할 수 있다는 게 그의 생각이다. 스위스 ‘프라이탁’은 세계에 350개 매장을 두고 600억원의 매출을 올리고 있는 업사이클링 전문회사다.\n",
      "-----\n",
      "문서2: 삼성이 그룹 차원에서 창조경제 활성화에 본격적으로 뛰어든다. 정보통신기술(ICT) 화학 건설 등 여러 업종의 계열사가 융합형 서비스·상품을 만들어 해외에 진출하는 방안을 추진하고, 유휴 특허를 중소기업과 나눠 새 기술을 개발하기로 했다. 박근혜 정부의 창조경제 활성화와 대·중소기업 상생 기조에 동참한다는 취지에서다. 삼성은 20일 서울 서초동 삼성전자 사옥에서 사장단회의를 갖고 이 같은 방안을 논의했다. 정기영 삼성경제연구소장이 강연자로 나서 사장들에게 창조경제의 개념 및 삼성의 과제에 대해 설명했다. 정 소장은 토지, 노동 등 기존 생산요소보다 기술 아이디어 등을 중요시하는 경제 패러다임을 창조경제로 정의했다. 이를 발전시키기 위해 삼성이 중소기업에 유휴특허를 빌려주고 기술을 지도해 새 기술을 개발할 수 있게 해야 한다고 강조했다. 유휴특허란 상품화하지 않았거나 특허료를 받고 있지 않은 특허를 말한다. 이인용 삼성 미래전략실 커뮤니케이션팀장(사장)은 “창조경제 확산을 위해 삼성이 가진 기술이 중소기업에 전파될 수 있도록 하겠다”며 “삼성전자 등 각 계열사 사장들이 구체적 방법을 검토할 것”이라고 설명했다. 정 소장은 창조적 인재 육성도 강조했다. 삼성은 올해 공채부터 인문계 전공자를 뽑아 소프트웨어를 교육시켜 채용하는 프로그램인 ‘삼성 컨버전스 소프트웨어 아카데미(SCSA)’ 제도를 도입했다. 여러 분야를 아우르는 통섭형 인재를 키우기 위해서다. 그는 또 ICT를 통한 인프라·산업 고도화, 계열사 간 이종 산업 융합을 통한 세계시장 개척 등을 추진해야 한다고 했다. 구체적으로는 건설과 화학, IT서비스를 결합해 신흥국에 진출하는 방법을 찾을 것을 제시했다. 박근혜 정부는 미래창조과학부를 신설하고 세계 최고 수준의 기술과 인프라를 갖춘 ICT 기술을 산업 전반에 적용해 성장동력과 일자리를 창출하는 것을 경제 정책의 기조로 삼고 있다.\n",
      "-----\n",
      "문서3: 소비자에게 무료로 제품을 나눠주는 경품추첨 서비스가 나왔다. 정보기술(IT) 벤처기업 ‘오션스피이플’은 무료 경품 추첨 ‘오픈프라이즈’ 서비스를 시작한다고 14일 발표했다. 소비자들은 스마트폰 애플리케이션(앱·응용프로그램)을 내려받아 관심있는 신제품이나 서비스에 응모해 직접 이용해볼 수 있다. 기업은 이를 통해 마케팅 효과를 거둘 수 있다.경품에 응모하려면 앱을 내려받아 회원 가입을 한 뒤 지급받은 포인트인 ‘큐브’를 사용하면 된다. 다양한 신상품과 서비스에 중복 응모할 수 있으며 큐브는 상품 후기를 달거나 설문에 답하는 등 앱 내에서 특정 활동을 하면 적립할 수 있다. 각 상품마다 응모가 마감되기 전까지 타이머가 작동하는 등 게임 요소도 가미했다.오션스피이플은 자사 상품을 알리려는 기업이 이 서비스를 마케팅 수단으로 사용할 수 있다고 설명했다. 신제품 출시 직후 짧은 기간 내에 다수의 소비자에게 제품을 노출할 수 있으며 현물 투자 방식이기 때문에 비용을 절감할 수 있다는 것이다. 한 가지 상품이나 서비스를 8주간 노출할 수 있다.김상훈 오션스피이플 대표는 “기존 소셜커머스는 과도한 할인 가격에 상품을 제공해 소비자 만족도가 떨어지고 판매자의 이미지도 동반 추락하는 단점이 있었다”며 “소비자에게 무료로 제품을 제공해 만족도를 끌어올리는 한편 기업은 신상품 출시 때 효율적인 마케팅 수단으로 이용할 수 있다”고 소개했다.\n",
      "-----\n",
      "문서4: 국내 스타트업(신생 벤처기업)이 개발한 폐쇄형 소셜네트워크서비스(SNS)가 미국 실리콘밸리 정보기술(IT)업계 종사자들의 정보공유 수단으로 활용된다.팀블라인드는 구글 페이스북 넷플릭스 우버 등 실리콘밸리에 있는 40개 IT 기업 직원들을 대상으로 익명 기반의 폐쇄형 SNS 앱(응용프로그램)인 ‘테크 라운지’ 서비스를 시작했다고 11일 발표했다. 테크 라운지는 특정 기업 직원에 국한되지 않고 동종업계 종사자끼리 정보와 의견을 교환할 수 있는 서비스다.이 회사는 특정 기업 임직원들이 익명으로 의견을 공유하는 익명 게시판인 ‘블라인드’를 국내는 물론 해외에서도 서비스하고 있다. 지난 7월엔 미국 최대 전자상거래기업인 아마존이, 10월엔 마이크로소프트(MS)가 블라인드를 사용하기 시작했다. 블라인드는 전용 앱을 내려받아 사전 등록된 기업에 소속된 직원들만 기업별·업종별 익명 게시판을 이용할 수 있는 서비스다. 소속 기업을 등록하고 싶으면 이용자 개인이 신청할 수 있고, 신청자가 많은 기업 순으로 기업별 익명게시판을 개설해주고 있다.2013년 말 서비스를 시작한 블라인드는 현재 네이버 카카오 넥슨 엔씨소프트 등 국내 IT 기업뿐 아니라 현대자동차 삼성물산 등 대기업에 이르기까지 814개 기업이 익명게시판으로 활용하고 있다. 국내에서도 서비스하고 있는 테크 라운지는 업종별로 업계 상황에 대해 대화할 수 있는 공간으로 업무 강도, 연봉 등 지인을 통한 사적 네트워크로만 접할 수 있던 알짜 정보를 얻을 수 있는 것이 장점이라는 게 회사 측 설명이다.정영준 블라인드 대표는 “같은 회사, 동종 업계 사람들이 모여 직장 내 불만뿐 아니라 전문적인 의견까지 교류하고 있다”고 말했다.<|im_end|>\n",
      "<|im_start|>user\n",
      "기업에서 오픈프라이즈를 활용할 수 있는 분야는?<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "893f9c81-b983-45e5-959d-a68127cff3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "기업에서 오픈프라이즈를 활용할 수 있는 분야는 주로 마케팅과 신제품 홍보입니다. 오픈프라이즈는 소비자에게 무료로 제품을 나눠주는 경품추첨 서비스를 제공하여, 기업이 신제품을 짧은 기간 내에 다수의 소비자에게 노출시킬 수 있는 효과적인 마케팅 수단으로 활용될 수 있습니다. 이를 통해 기업은 현물 투자 방식으로 비용을 절감하면서도 소비자 만족도를 높일 수 있습니다. 또한, 소비자들이 제품 후기를 작성하거나 설문에 응답하는 등의 활동을 통해 추가적인 마케팅 데이터를 수집할 수 있습니다 [[ref3]].<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b990a60-0d6f-4e75-a060-b75c8b479a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92599407-22c6-4559-af3d-353d00e4f4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8b9193cca44c8981122f6a672da669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-285\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa6ef1b5-7db9-438a-969e-ffcf95fb1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0f0292b-826f-42a6-9b4f-68672b8da70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3930cf2d-18c5-4a6b-a503-99a5e0aa17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "청춘밴드는 사연 신청자의 부모님을 위해 특별한 선물을 생각했습니다. 사연 신청자는 부모님을 위해 특별한 선물을 하고 싶어 했고, 그 부모님은 비닐 공장을 운영하는 사람입니다. '청춘밴드'는 부모님의 취향을 저격하는 즉석 노래들을 선보이며, 그들의 마음을 사로잡았습니다. 또한, '청춘밴드'는 부모님의 취향을 저격하는 즉석 노래들을 선보이며, 그들의 마음을 사로잡았습니다. 이에 '청춘밴드'는 남다른 추억을 선물해주기 위해 '리마인드 웨딩 이벤트'를 진행했습니다. 이 이벤트는 부모님에게 특별한 추억을 선물하는 것으로, '청춘밴드'는 이 일로 시청자들에게 감동을 선사했습니다 [[ref5]].\n",
      "    label:\n",
      "\n",
      "'청춘밴드'는 사연 신청자의 부모님을 위해 특별한 선물로 리마인드 웨딩 이벤트를 준비했습니다. 사연 신청자의 부모님은 한평생 일하느라 청춘을 잊고 지내셨고, 흔한 결혼사진 한 장 없이 40여 년의 세월을 함께 걸어온 부부였습니다. '청춘밴드'는 이 부부에게 남다른 추억을 선물해주기 위해 리마인드 웨딩 이벤트를 진행하며, 부모님의 취향을 저격하는 즉석 노래들을 선보여 마음을 사로잡았습니다. 이 이벤트는 부모님에게 특별한 추억을 만들어주고, 가족들과 함께 웃음과 눈물을 나누는 감동적인 시간이 되었습니다 [[ref5]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "GS 신임 임원 과정의 마무리 행사가 지난 1일부터 6박7일간 진행되었으며, 이는 6일에 열린 신임 임원 만찬으로 마무리되었습니다 [[ref2]].\n",
      "    label:\n",
      "\n",
      "GS 신임 임원 과정의 마무리 행사는 지난 6일에 제주시 엘리시안 제주리조트에서 열렸습니다. 이 행사는 1일부터 6박 7일간 진행된 GS 신임 임원 과정의 마지막 행사로, 신임 임원 20여명 등 30여명이 참석했습니다 [[ref2]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "삼성페이의 결제 방식 중 애플과 구글이 적용하지 않는 것은 마그네틱 보안 전송(MST)입니다. MST는 기존의 마그네틱 단말기로 결제가 가능하도록 하는 기술로, 애플의 애플페이와 구글의 구글 월렛은 NFC 방식을 주로 사용합니다. 삼성페이는 NFC와 MST 두 가지 방식을 모두 지원하여 다양한 결제 환경에서 사용할 수 있습니다 [[ref1]], [[ref2]].\n",
      "    label:\n",
      "\n",
      "애플과 구글이 적용하지 않는 삼성페이의 결제 방식은 \"마그네틱 보안 전송(MST)\"입니다. 삼성페이는 근접무선통신(NFC) 방식과 함께 MST 방식을 지원하여, 기존의 마그네틱 신용카드 결제 단말기를 그대로 사용할 수 있게 합니다. 이로 인해 삼성페이는 더 많은 매장에서 결제가 가능하며, 이는 애플페이와 구글 월렛이 NFC 방식에만 국한된 것과 차별화되는 점입니다 [[ref1]], [[ref5]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "커스터드 중 진한 크림의 제형은 \"크렘 파티시에레\"입니다. 이는 에클레르를 채울 때 주로 사용됩니다. 또한, 이 진한 크림은 보통 후식 또는 소스로 제공되며, 키슈나 다른 음식을 만들 때도 유용하게 사용됩니다. [[ref4]]\n",
      "    label:\n",
      "\n",
      "진한 크림 제형의 커스터드는 주로 다양한 디저트와 페이스트리에 사용됩니다. 특히 \"크렘 파티시에레\"라는 진한 크림 형태의 커스터드는 에클레르와 같은 페이스트리를 채우는 데 주로 사용됩니다. 이 외에도 커스터드는 후식이나 소스로 제공되며, 설탕이나 바닐라가 곁들여진 레시피도 존재합니다. 커스터드는 또한 키슈와 같은 다른 요리를 만들 때도 유용하게 사용됩니다 [[ref4]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "삼성생명은 작년 태국 합작법인인 시암삼성에 40억원을 출자했습니다 [[ref4]].\n",
      "    label:\n",
      "\n",
      "삼성생명은 작년 태국 법인인 시암삼성에 40억원을 출자한 데 이어, 올 상반기 추가로 173억원을 투입할 계획을 세웠습니다. 이로 인해 시암삼성의 삼성생명 지분은 종전 49%에서 66.4%로 높아질 예정입니다 [[ref4]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9909d-6ea1-46e8-a5a2-dbe3cc05814b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
