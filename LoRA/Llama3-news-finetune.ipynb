{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79562d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76868a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54e0348138043fe9d4b986ec3ce3001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/781 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c75964d4dc14856ba737d432f1c3d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6301353cc94bd09fab5b88cd0061be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"iamjoon/finance_news_summarizer\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37052a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 크기: 991\n"
     ]
    }
   ],
   "source": [
    "print(\"전체 데이터 크기:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dd88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.5\n",
    "\n",
    "train_data = []\n",
    "test_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736ff350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 인덱스를 0부터 데이터 개수-1까지 리스트로 만든다\n",
    "data_indices = list(range(len(dataset)))\n",
    "# 테스트 사이즈를 구한다\n",
    "test_size = int(len(dataset) * test_ratio)\n",
    "\n",
    "# 데이터셋 인덱스 지정\n",
    "test_data = data_indices[:test_size]\n",
    "train_data = data_indices[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa96d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai format으로 데이터 변환\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\":[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sample[\"system_prompt\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"user_prompt\"]\n",
    "            },\n",
    "            # 정답 예시 제공 json타입이기 때문에 str타입으로 변환 후 제공\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": str(sample[\"assistant\"])\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca78b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 OpenAI format으로 변환\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393b8061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기: 496\n",
      "테스트 데이터 크기: 495\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 크기 출력\n",
    "print(\"훈련 데이터 크기:\", len(train_dataset))\n",
    "print(\"테스트 데이터 크기:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e28b8494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\\n두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 \\'없음\\' 등을 적어서는 안 됩니다.\\n\\n만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": False,\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\\n\\n만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": True,\\n\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}'},\n",
       " {'role': 'user',\n",
       "  'content': '중소기업 하반기 경기전망 작년보다 악화…원자잿값 상승 우려\\n서울 연합뉴스 신선미 기자 중소기업의 올해 하반기 경기전망 지수가 지난해 같은 기간보다 하락한 것으로 나타났다. 5일 중소기업중앙회에 따르면 지난달 15∼24일 중소기업 500곳을 대상으로 실시한 중소기업 경영애로 및 2022년 하반기 경기전망조사 결과 하반기 경기전망지수 SBHI 는 87.6으로 지난해 하반기 91.6 보다 4.0포인트 p 하락했다. 이 지수가 100 이상이면 경기가 개선될 것으로 보는 응답자가 더 많고 100 미만이면 그 반대라는 의미다. 중기중앙회 중기중앙회 제공 하반기 SBHI를 업종별로 보면 제조업의 경우 펄프·종이 및 종이제품업 54.2 섬유제품업 54.2 전기장비업 68.2 은 경기가 악화될 것으로 내다봤고 기타 운송장비업 127.3 가죽·가방 및 신발업 104.6 은 경기가 호전될 것으로 전망했다. 서비스업에서는 부동산업 및 임대업 60.0 도매 및 소매업 84.0 은 경기가 악화될 것으로 봤지만 예술·스포츠 및 여가 관련 서비스업 112.0 은 업황 개선을 전망했다. 하반기 예상되는 애로 요인 복수응답 은 원자재 가격 상승 58.8% 내수 부진 31.2% 인력 수급난 29.8% 금리상승 28.4% 최저임금 상승 19.4% 등의 순이었다. 또 상반기 겪은 애로 요인으로는 원자재가격 상승 62.6% 내수부진 35.2% 인력 수급난 29.8% 금리상승 25.2% 최저임금 상승 22.8% 등의 순으로 응답률이 높았다. 소상공인·중소기업의 경기 개선을 위해 필요한 정부 정책 복수응답 으로는 세금 및 각종 부담금 인하 61.4% 금융지원 45.0% 인력난 해소 34.6% 원자재 수급 안정화 28.6% 근로시간 유연화 20.0% 순으로 꼽혔다. 코로나19 이전 수준의 경영실적 회복 예상 시기에 대해서는 응답자의 27.0%가 2024년 이후 라고 답했고 이어 2023년 상반기 와 2023년 하반기 각 23.0% 2022년 하반기 14.8% 등이었다.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"{'is_stock_related': True, 'negative_impact_stocks': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'negative_keywords': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'positive_impact_stocks': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'positive_keywords': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'reason_for_negative_impact': '펄프·종이 및 종이제품업, 섬유제품업, 전기장비업, 부동산업 및 임대업, 도매 및 소매업은 하반기 경기 전망에서 경기가 악화될 것으로 예측되고 있다.', 'reason_for_positive_impact': '기타 운송장비업, 가죽·가방 및 신발업, 예술·스포츠 및 여가 관련 서비스업은 하반기 경기 전망에서 경기가 호전될 것으로 예측되고 있다.', 'summary': '중소기업중앙회 조사 결과, 올해 하반기 중소기업 경기 전망 지수가 작년보다 하락했다. 제조업에서는 펄프, 종이, 섬유, 전기장비업이 경기 악화를 예상했고, 운송장비와 가죽, 신발업이 호전을 전망했다. 원자재 가격 상승과 내수 부진 등이 주요 애로 요인으로, 경기 개선을 위해 세금 및 부담금 인하와 금융지원 등이 필요하다고 응답됐다.'}\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08573068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ab8bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55368e214254f419386ae2a2ffaba1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab86630291ee4bd38f066809abc8ab87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ac7945efe643c2b61d4586fabbd2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a6cb791ceb4d6aac51a3458bb85ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee0f0311e9a4980b846324accdd0a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ebcf512fde4ed28d438fa050b220dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f975beb66af04f79bbf092639e533a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577b8c7d10eb4e67aca12dc177c92c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437297d016854b5ba41d889e86589eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4e3cb80851466a9291d7fdaae45723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5561bdfac62d4d8c87b153c9987859a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"NCSoft/Llama-VARCO-8B-Instruct\"\n",
    "\n",
    "# device_map=\"auto\": 자동으로 적절한 디바이스(GPU/CPU)에 배치\n",
    "# torch_dtype=torch.bfloat16: 메모리 효율성을 위해 bfloat16 타입 사용\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    # load_in_8bit=True, \n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# model id 바탕으로 사전학습 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436cc7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\n",
      "금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65665ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,           # LoRA 스케일링 파라미터\n",
    "    lora_dropout=0.1,        # 드롭아웃 비율\n",
    "    r=8,                     # rank (저차원 분해의 차원)\n",
    "    bias=\"none\",             # 바이어스 학습 여부\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA를 적용할 모듈들\n",
    "    task_type=\"CAUSAL_LM\",   # 태스크 타입 (언어모델링)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1df21f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"llama3-8b-summarizer-ko\",          # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=3,                            # 훈련 에포크 수 \n",
    "    per_device_train_batch_size=2,                 # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,                 # 그래디언트 누적 스텝\n",
    "    gradient_checkpointing=True,                   # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",                     # 최적화기\n",
    "    logging_steps=10,                              # 로깅 주기\n",
    "    save_strategy=\"steps\",                         # 저장 전략\n",
    "    save_steps=50,                                 # 저장 주기\n",
    "    bf16=True,                                     # bfloat16 사용\n",
    "    learning_rate=1e-4,                            # 학습률\n",
    "    max_grad_norm=0.3,                             # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                             # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",                  # 스케줄러 유형\n",
    "    push_to_hub=False,                             # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,                   # 사용하지 않는 컬럼 제거 안 함\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}, # 데이터셋 전처리 스킵\n",
    "    report_to=None                                 # 로깅 툴 사용 안 함\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec0060a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    for example in batch:\n",
    "        messages = example[\"messages\"]\n",
    "\n",
    "        prompt = \"<|begin_of_text|>\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"].strip()\n",
    "            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "\n",
    "        text = prompt.strip()\n",
    "\n",
    "        # 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = [-100] * len(input_ids)\n",
    "\n",
    "        # assistant 응답의 시작 위치 찾기\n",
    "        assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "        eot_token = \"<|eot_id|>\"\n",
    "        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\n",
    "\n",
    "        # 레이블 범위 지정\n",
    "        i = 0\n",
    "        while i <= len(input_ids) - len(assistant_tokens):\n",
    "            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens:\n",
    "                start = i + len(assistant_tokens)\n",
    "                end = start\n",
    "                while end <= len(input_ids) - len(eot_tokens):\n",
    "                    if input_ids[end:end + len(eot_tokens)] == eot_tokens:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start, end):\n",
    "                    labels[j] = input_ids[j]\n",
    "                for j in range(end, end + len(eot_tokens)):\n",
    "                    labels[j] = input_ids[j]  # <|eot_id|> 토큰도 포함\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        pad_len = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * pad_len)\n",
    "        new_batch[\"labels\"][i].extend([-100] * pad_len)\n",
    "\n",
    "    for k in new_batch:\n",
    "        new_batch[k] = torch.tensor(new_batch[k])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d07d0edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 2884])\n",
      "어텐션 마스크 형태: torch.Size([1, 2884])\n",
      "레이블 형태: torch.Size([1, 2884])\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=8192\n",
    "\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b314f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[128000, 128006, 9125, 128007, 198, 65895, 83628, 34804, 56773, 125441, 111068, 25941, 123103, 99458, 88708, 19954, 126652, 56773, 16969, 111068, 25941, 117469, 106478, 102517, 44005, 104193, 123061, 111068, 25941, 106478, 102517, 21121, 80052, 627, 103097, 109521, 111964, 127491, 122625, 119097, 108859, 101480, 93917, 101868, 56069, 13094, 168, 235, 105, 21028, 11240, 106612, 77437, 43139, 114839, 16582, 119978, 627, 65993, 108, 103386, 36092, 112, 102260, 109055, 19954, 105642, 103386, 36092, 112, 102260, 105880, 103607, 34609, 113348, 45618, 49085, 88525, 96677, 119978, 13, 127063, 11240, 56069, 113890, 18359, 62085, 99742, 102893, 105365, 102467, 112215, 124005, 13, 106725, 27796, 56773, 21028, 16582, 119978, 627, 54059, 54542, 11240, 57575, 106603, 907, 16969, 67890, 30426, 115790, 19954, 95713, 61938, 13, 67890, 56154, 115790, 18359, 106725, 103607, 22035, 100711, 119978, 13, 95713, 67890, 30426, 115790, 19954, 106725, 103607, 104834, 24486, 907, 18918, 104965, 103430, 76242, 96, 34609, 119978, 627, 34983, 65895, 115790, 13094, 47782, 115300, 122292, 81021, 55055, 108520, 122292, 84734, 17835, 103607, 32179, 90759, 109670, 13, 105813, 21028, 17835, 364, 123467, 6, 120908, 103607, 108503, 16969, 96270, 124005, 382, 73653, 103168, 95713, 111068, 122625, 103966, 30381, 99458, 88708, 7, 127702, 125543, 105797, 102662, 119222, 51796, 109745, 11, 103966, 30381, 99458, 88708, 7, 127702, 8, 81673, 111304, 103045, 78453, 101106, 13094, 108838, 111068, 25941, 33177, 50152, 102772, 116100, 81673, 109583, 114839, 61938, 382, 109659, 104449, 512, 5018, 285, 31641, 54356, 794, 3641, 345, 1, 1743, 794, 330, 58126, 21121, 102772, 95713, 111068, 120155, 87097, 103168, 97237, 87097, 103168, 123926, 114839, 16582, 119978, 64259, 73653, 103168, 95713, 111068, 122625, 103966, 30381, 99458, 88708, 7, 127702, 8, 115467, 78453, 101106, 109791, 109745, 11, 103966, 30381, 99458, 88708, 7, 127702, 8, 54780, 111304, 103045, 78453, 101106, 13094, 108838, 111068, 25941, 33177, 50152, 102772, 116100, 81673, 109583, 114839, 61938, 382, 109659, 104449, 512, 5018, 285, 31641, 54356, 794, 3082, 345, 1, 31587, 37888, 533, 1284, 26246, 794, 4482, 101508, 13094, 168, 235, 105, 81021, 55055, 84734, 21028, 106612, 87472, 17835, 23955, 111068, 122625, 41871, 235, 30381, 103684, 126652, 109720, 104349, 43139, 58935, 30381, 107205, 99458, 88708, 106001, 87134, 18359, 114839, 16582, 119978, 13, 106943, 26799, 17835, 103607, 109745, 110192, 80732, 43139, 103607, 22035, 100711, 119978, 13, 99458, 88708, 80732, 43139, 58935, 30381, 107205, 62398, 84391, 126546, 103607, 34609, 119978, 13, 111068, 25941, 123103, 58935, 30381, 48936, 29833, 65621, 127923, 24486, 115382, 101886, 94801, 43139, 103607, 34609, 119978, 13, 63207, 103168, 11, 119097, 88525, 110661, 115300, 122292, 84734, 17835, 114839, 16582, 119978, 1210, 1282, 1, 20489, 5595, 55260, 37888, 533, 794, 330, 82001, 21028, 99458, 88708, 102823, 95713, 111068, 25941, 123103, 41871, 235, 30381, 103684, 126652, 84696, 18359, 111590, 58935, 30381, 24486, 111436, 18918, 84618, 109509, 113631, 114839, 16582, 119978, 761, 1, 31587, 52454, 794, 4482, 18202, 235, 30381, 103684, 126652, 109720, 111590, 58935, 30381, 107205, 99458, 88708, 102823, 119097, 101528, 33390, 84618, 109509, 41871, 235, 30381, 103684, 126652, 56773, 103170, 106589, 93292, 20565, 120789, 101954, 120138, 24486, 104167, 56154, 108652, 103430, 30446, 105880, 56069, 13094, 168, 235, 105, 81021, 55055, 84734, 106612, 87472, 17835, 114839, 16582, 119978, 13, 113094, 80732, 11, 127798, 80732, 120908, 109580, 110097, 61938, 13, 107067, 100660, 104167, 56154, 112887, 108785, 27797, 61938, 13, 47782, 115300, 122292, 84734, 17835, 114839, 100660, 115106, 1210, 1282, 1, 43324, 37888, 533, 1284, 26246, 794, 4482, 101508, 13094, 168, 235, 105, 81021, 55055, 84734, 21028, 106612, 87472, 17835, 23955, 111068, 122625, 41871, 235, 30381, 103684, 126652, 109720, 104349, 43139, 58935, 30381, 107205, 99458, 88708, 105880, 114839, 16582, 119978, 13, 106943, 26799, 17835, 103607, 109745, 110192, 80732, 43139, 103607, 22035, 100711, 119978, 13, 99458, 88708, 80732, 43139, 58935, 30381, 107205, 62398, 84391, 126546, 103607, 34609, 119978, 13, 111068, 25941, 123103, 58935, 30381, 48936, 29833, 65621, 127923, 24486, 115382, 101886, 94801, 43139, 103607, 34609, 119978, 13, 63207, 103168, 11, 119097, 88525, 110661, 115300, 122292, 84734, 17835, 114839, 16582, 119978, 1210, 1282, 1, 20489, 5595, 54965, 37888, 533, 794, 330, 82001, 21028, 99458, 88708, 102823, 95713, 111068, 25941, 123103, 41871, 235, 30381, 103684, 126652, 84696, 18359, 111590, 58935, 30381, 24486, 111436, 18918, 84618, 109509, 113631, 114839, 16582, 119978, 761, 1, 43324, 52454, 794, 4482, 64189, 30381, 103684, 126652, 109720, 111590, 58935, 30381, 107205, 99458, 88708, 102823, 119097, 101528, 33390, 84618, 109509, 86503, 30381, 103684, 126652, 56773, 103170, 106589, 93292, 20565, 120789, 101954, 120138, 24486, 104167, 56154, 108652, 103430, 30446, 105880, 56069, 13094, 168, 235, 105, 81021, 55055, 84734, 106612, 87472, 17835, 114839, 16582, 119978, 13, 113094, 80732, 11, 127798, 80732, 120908, 109580, 110097, 61938, 13, 107067, 100660, 104167, 56154, 112887, 108785, 27797, 61938, 13, 47782, 115300, 122292, 84734, 17835, 114839, 100660, 115106, 1210, 1282, 1, 1743, 794, 330, 58126, 21121, 102772, 95713, 111068, 120155, 87097, 103168, 97237, 87097, 103168, 123926, 114839, 16582, 119978, 9388, 128009, 128006, 882, 128007, 198, 13094, 98934, 102335, 103236, 30446, 56154, 19954, 44215, 35495, 41953, 1981, 100981, 29102, 24486, 101603, 101096, 65677, 38187, 58083, 113110, 126015, 104019, 110513, 198, 101136, 103655, 55421, 41953, 84618, 66965, 56154, 12432, 105131, 102997, 62841, 101003, 58189, 33931, 58083, 115777, 104019, 1981, 114039, 103168, 87097, 32428, 102517, 62060, 71682, 110513, 107545, 103168, 101532, 55430, 106359, 101254, 101136, 29102, 110834, 1981, 107752, 82233, 104019, 108289, 58083, 113110, 126015, 102786, 110208, 66965, 116604, 101834, 101103, 1981, 60861, 101151, 101482, 101193, 96677, 103304, 23955, 98934, 102335, 104193, 123061, 103655, 104841, 55421, 41953, 13, 105429, 108785, 52688, 112003, 21121, 26799, 23955, 98934, 102335, 104193, 123061, 103655, 104841, 55421, 124788, 101003, 58189, 33931, 104019, 107545, 22035, 57575, 103123, 21121, 29833, 108964, 33931, 103686, 42771, 18918, 107472, 101480, 29102, 24486, 101603, 101096, 103686, 115096, 65677, 38187, 34983, 115087, 107387, 220, 20, 33177, 103153, 64189, 101528, 13, 23955, 104684, 103551, 111097, 102517, 107152, 64189, 109126, 55421, 29102, 101136, 57002, 66338, 71682, 106304, 423, 14899, 220, 18, 101353, 101015, 66610, 60798, 20565, 127245, 107205, 103659, 106725, 83719, 38187, 33931, 58083, 113110, 126015, 78102, 423, 14899, 115839, 63171, 104065, 109231, 19954, 102597, 29833, 36811, 20565, 122862, 48936, 29833, 65621, 63207, 118009, 58083, 115777, 104019, 19954, 106603, 102517, 101709, 101327, 66406, 3396, 109864, 104684, 105771, 49085, 127264, 101528, 13, 23955, 102467, 124788, 23955, 106223, 106010, 72043, 89359, 50467, 58189, 84618, 83628, 101136, 123061, 109567, 62841, 57575, 105069, 102423, 84618, 83628, 66965, 52688, 101136, 123061, 127702, 106287, 66406, 101090, 26799, 12432, 110863, 21028, 105131, 102997, 62841, 57575, 101003, 58189, 33931, 58083, 115777, 19954, 106603, 102517, 24486, 125718, 18359, 89946, 56773, 21121, 82818, 103272, 13447, 13, 84618, 66965, 117396, 29833, 83628, 117211, 13094, 47782, 21121, 109644, 101003, 58189, 33931, 58083, 115777, 20565, 107120, 114213, 82068, 109816, 125959, 102612, 103684, 58083, 115777, 112373, 107022, 101015, 101266, 124983, 104019, 48936, 108289, 20565, 91786, 49208, 108, 107779, 80816, 24486, 111850, 101555, 21028, 101003, 58189, 111490, 103686, 42771, 44005, 62398, 104790, 103123, 21121, 29833, 108964, 33931, 103686, 42771, 18918, 107472, 101480, 29102, 24486, 101603, 101096, 103686, 41953, 106593, 101254, 82001, 102005, 65677, 86157, 103686, 67945, 16969, 65677, 38187, 83290, 56773, 21121, 82818, 103272, 13447, 101254, 108537, 13, 121856, 23955, 102467, 124788, 84618, 66965, 117396, 84618, 66965, 109126, 97096, 101066, 78102, 119262, 33931, 103213, 44966, 18359, 110155, 127002, 21028, 65677, 101136, 18359, 66610, 104684, 101360, 112795, 45618, 101711, 101136, 29102, 69508, 59134, 104303, 45618, 66610, 104684, 19954, 123851, 38389, 103618, 113610, 48936, 29833, 91786, 13, 103294, 65677, 101136, 103678, 27797, 118408, 33390, 57575, 36609, 101015, 67945, 71023, 34804, 126761, 104182, 107545, 103168, 24486, 95303, 102156, 13094, 106359, 101360, 119864, 67945, 71023, 34804, 108360, 125189, 101508, 13094, 121751, 113890, 29515, 62060, 71023, 78102, 86503, 58189, 86157, 107022, 102757, 19954, 104441, 101711, 33308, 120, 119567, 116492, 19954, 107138, 103655, 102893, 126652, 84696, 121969, 49208, 108, 84618, 66965, 115296, 65677, 101136, 93917, 104684, 14260, 94772, 27797, 57002, 103966, 24140, 33931, 43139, 107545, 103168, 87097, 32428, 102517, 17835, 112521, 101464, 24486, 62060, 71682, 20565, 108289, 108907, 101254, 109012, 13, 23955, 102467, 124788, 220, 2366, 15, 100392, 101327, 102757, 124141, 126906, 121296, 103185, 113360, 102249, 124141, 777, 113610, 122964, 84618, 66965, 109126, 80307, 107235, 101164, 30446, 20565, 103686, 67945, 65219, 104448, 84618, 66965, 109126, 126902, 97096, 101066, 13094, 112024, 57002, 72043, 101353, 106910, 116686, 72043, 44690, 102193, 84618, 66965, 117396, 29833, 74623, 100551, 63375, 101003, 58189, 33931, 106460, 17835, 19954, 105164, 33390, 24486, 82818, 91786, 49208, 108, 121066, 220, 21, 100551, 111323, 84618, 66965, 109126, 80307, 107235, 101164, 30446, 20565, 220, 2366, 15, 100392, 101003, 58189, 33931, 46810, 21121, 122964, 106287, 101838, 220, 6083, 18287, 117615, 59134, 62841, 116429, 65677, 101136, 93917, 104684, 84618, 101868, 13094, 127992, 115809, 57390, 116039, 91786, 101254, 109012, 13, 106237, 104448, 23955, 102467, 124788, 65677, 50643, 104182, 64432, 24140, 103684, 116492, 18359, 127271, 34983, 101003, 58189, 33931, 123706, 116273, 107573, 54289, 18918, 125744, 101360, 75086, 57002, 65677, 101136, 66610, 104684, 119623, 49085, 106327, 126403, 106313, 109070, 34983, 56773, 21121, 82818, 103272, 13447, 49208, 108, 69508, 103684, 62060, 71023, 102657, 103686, 112037, 106593, 62060, 55430, 55430, 109682, 101482, 101193, 101003, 57002, 102249, 26799, 65677, 101136, 111302, 78102, 103686, 42771, 120908, 110155, 63207, 110534, 54542, 86503, 109126, 18918, 65677, 50643, 104182, 59134, 66338, 48936, 29833, 123644, 107779, 80816, 24486, 111850, 101555, 21028, 101003, 58189, 33931, 103686, 42771, 20565, 108289, 108907, 101254, 102258, 93917, 101528, 13, 49508, 102275, 61394, 23955, 102467, 124788, 36609, 101015, 67945, 71023, 18359, 96270, 30381, 104182, 104019, 101360, 104423, 101272, 103402, 94, 24140, 122298, 116688, 103686, 112037, 44005, 103659, 49085, 104441, 101711, 34983, 104685, 105771, 103153, 64189, 101528, 13, 108154, 84618, 66965, 115296, 36609, 101015, 67945, 71023, 34804, 107545, 103168, 101532, 55430, 20565, 106359, 44005, 101254, 101136, 29102, 109231, 13094, 127002, 18359, 103213, 22035, 101360, 112795, 104193, 29102, 59134, 104303, 45618, 103521, 66965, 115602, 102678, 16582, 113191, 101834, 101103, 20565, 91786, 49208, 108, 107545, 103168, 101532, 55430, 19954, 102597, 101254, 101136, 29102, 62060, 71023, 107545, 102662, 45618, 103213, 109627, 59134, 66338, 122298, 29854, 19954, 107625, 16969, 62060, 71023, 107545, 102662, 93851, 101066, 13094, 37155, 111283, 113191, 29833, 123644, 125718, 18359, 89946, 56773, 30426, 21121, 82818, 103272, 13447, 101254, 109012, 13, 23955, 102467, 124788, 23955, 104684, 103551, 127245, 53400, 423, 14899, 220, 18, 101353, 101015, 66610, 60798, 111323, 103055, 101136, 125935, 83719, 38187, 33931, 58083, 113110, 126015, 78102, 423, 14899, 115839, 116464, 57575, 63171, 104065, 107205, 109231, 19954, 102597, 29833, 36811, 20565, 122862, 48936, 29833, 36439, 34609, 117622, 58083, 115777, 104019, 19954, 64432, 13447, 101327, 66406, 3396, 109864, 55430, 106103, 82818, 103272, 13447, 49208, 108, 125578, 104423, 101272, 103402, 94, 24140, 122298, 29854, 103686, 112037, 18359, 106958, 101412, 54542, 57519, 105115, 18359, 64432, 24140, 104182, 66980, 34983, 62060, 111270, 112037, 65895, 101136, 18359, 107779, 80816, 101709, 103607, 102365, 48936, 108289, 20565, 91786, 101254, 101012, 100, 44852, 247, 102563, 13, 23955, 102467, 124788, 119864, 67945, 71023, 13094, 103966, 30381, 107022, 102757, 19954, 105613, 101711, 119222, 51796, 108438, 84618, 83628, 102612, 56154, 101824, 33229, 102252, 101106, 106064, 102258, 57390, 34983, 109720, 124859, 104064, 29854, 101528, 13, 108154, 84618, 66965, 117396, 104219, 93292, 220, 605, 100392, 63375, 102678, 101136, 29102, 55216, 93917, 101824, 44215, 108955, 106213, 57390, 17835, 29515, 62060, 71023, 78102, 86503, 58189, 86157, 107022, 102757, 18359, 122169, 43139, 119864, 67945, 71023, 18359, 103686, 67945, 34983, 119929, 102772, 101254, 101314, 101096, 100981, 65677, 86157, 18359, 84415, 54780, 102893, 73444, 112039, 113295, 27796, 113469, 86503, 58189, 86157, 112994, 16582, 105316, 19954, 102597, 101834, 101103, 20565, 118957, 106313, 18359, 125714, 34983, 62060, 71023, 107545, 102662, 45618, 110038, 42771, 101438, 13094, 116548, 104965, 100981, 59134, 66338, 122298, 29854, 46810, 55430, 17835, 84618, 83628, 102612, 111636, 107973, 62060, 71023, 107545, 102662, 111323, 102772, 103213, 109627, 101327, 27797, 82001, 102005, 124476, 84618, 64189, 18918, 56773, 21121, 104182, 106313, 109070, 48936, 108289, 20565, 91786, 101254, 108537, 13, 121856, 23955, 102467, 124788, 84618, 66965, 56154, 101266, 124983, 119864, 58126, 83628, 106213, 56154, 101824, 33229, 102252, 101106, 106064, 102258, 57390, 101360, 119262, 116492, 115809, 57390, 19954, 62060, 71682, 34983, 62060, 111270, 112037, 65895, 101136, 69508, 103607, 102365, 109018, 110671, 115954, 56773, 30426, 21121, 82818, 103272, 13447, 49208, 108, 104193, 103655, 55421, 34804, 107036, 29515, 62060, 71023, 19954, 102597, 115888, 33931, 116090, 18918, 125744, 44005, 78102, 119864, 67945, 71023, 62085, 225, 250, 18918, 106313, 109070, 101360, 55925, 99901, 18918, 82818, 120378, 43139, 107022, 101015, 81673, 119864, 58126, 83628, 106213, 56154, 101824, 33229, 102252, 106974, 55170, 108860, 105633, 102611, 117615, 96677, 103304, 48936, 119623, 23955, 105771, 109012, 13, 23955, 102467, 124788, 124141, 777, 109682, 113052, 99458, 64356, 78102, 19954, 62060, 71682, 24486, 107545, 103168, 101532, 55430, 109682, 109018, 125718, 18359, 103153, 64189, 101528, 13, 108154, 84618, 66965, 114333, 65677, 50643, 107065, 72043, 32428, 118711, 126709, 54059, 120916, 78102, 104965, 100981, 93917, 30381, 109682, 113052, 18359, 120952, 34983, 84656, 30426, 104182, 102888, 100981, 82068, 46230, 97, 66406, 19954, 72747, 24486, 103213, 55430, 20565, 66610, 109509, 48918, 101096, 19954, 107067, 113047, 48936, 29833, 123644, 103607, 110616, 103684, 109682, 18359, 121121, 30446, 102423, 13447, 49208, 108, 104350, 34983, 220, 23, 123096, 127798, 102517, 104193, 29102, 32428, 16582, 36811, 89359, 103131, 107065, 101272, 82068, 100994, 30426, 20565, 127245, 65219, 117622, 116534, 103603, 102258, 57390, 120908, 110155, 101327, 27797, 123360, 74623, 101151, 53400, 116534, 21028, 104193, 29102, 64189, 102997, 13094, 44215, 103655, 113191, 29833, 123644, 110187, 125718, 18359, 89946, 56773, 30426, 21121, 82818, 103272, 13447, 101254, 102258, 93917, 101528, 13, 106237, 104448, 23955, 102467, 124788, 119929, 106359, 101136, 106446, 13094, 122862, 44005, 83719, 38187, 33931, 58083, 113110, 126015, 34804, 107545, 103168, 101532, 109627, 59134, 66338, 86503, 102997, 18359, 84656, 30426, 104182, 109720, 58126, 115087, 29833, 65621, 102027, 101838, 13094, 36439, 102077, 104193, 44690, 101661, 57002, 104193, 123061, 125399, 19954, 95713, 88525, 51796, 54059, 102786, 110208, 66965, 116604, 19954, 102597, 101834, 101103, 20565, 65621, 124859, 112024, 23955, 51440, 101203, 104193, 103655, 55421, 34804, 104193, 123061, 82001, 114080, 62841, 81673, 106999, 104193, 123061, 44690, 71682, 26799, 109969, 108964, 63171, 35495, 18918, 106958, 58083, 113110, 126015, 114942, 27796, 101327, 102546, 107545, 103168, 101532, 55430, 127287, 45618, 61816, 102477, 111411, 125744, 104193, 29102, 105178, 30381, 67236, 101577, 103603, 104193, 29102, 100994, 30426, 56773, 21121, 103123, 106734, 122733, 74623, 101151, 101482, 101193, 18359, 96677, 103304, 72043, 19954, 91786, 13, 106603, 103236, 30446, 56154, 12432, 127737, 49085, 74623, 101151, 101482, 101193, 96677, 103304, 57519, 102704, 116534, 19954, 102597, 114942, 101412, 102735, 94, 78102, 43139, 59777, 34983, 102786, 110208, 66965, 116604, 20565, 113610, 88525, 51796, 108438, 65677, 50643, 104182, 104019, 18918, 102258, 57390, 34983, 56773, 30426, 106647, 103153, 64189, 30446, 102423, 13447, 101254, 109012, 13, 23955, 102467, 124788, 84618, 66965, 101096, 101015, 44215, 108955, 29854, 102258, 117216, 107472, 111850, 38187, 107123, 57390, 78102, 126950, 82068, 109682, 18359, 49508, 110833, 22035, 51796, 103373, 105453, 118183, 49085, 116283, 35859, 104828, 13, 108154, 105638, 22035, 117419, 57519, 66338, 45618, 124784, 107625, 13094, 34983, 104193, 123061, 101096, 54780, 75086, 101136, 123061, 101096, 21028, 44215, 101015, 20565, 108785, 101438, 32179, 119073, 103924, 13, 125578, 84618, 66965, 117396, 102558, 227, 102953, 82233, 81673, 21028, 44215, 108955, 106213, 57390, 17835, 84618, 101109, 107022, 102757, 107988, 123851, 108699, 19954, 72747, 34983, 36439, 34609, 117622, 116453, 102132, 41953, 58189, 116688, 97096, 108214, 48936, 29833, 123644, 109682, 16582, 125684, 49208, 108, 105638, 22035, 117419, 57519, 66338, 58935, 42529, 18918, 125714, 34983, 24839, 116, 101090, 101824, 86503, 24140, 101096, 100981, 21028, 115483, 82001, 84618, 66965, 101096, 102517, 107545, 102662, 96451, 107022, 100981, 21028, 50152, 104193, 123061, 101096, 54780, 78453, 101106, 53400, 115888, 19954, 122115, 16969, 104193, 123061, 82001, 19954, 103686, 124784, 103521, 21028, 16582, 125684, 13, 103294, 123102, 102464, 71023, 45618, 109018, 104193, 103655, 123645, 123102, 103315, 29726, 126709, 18918, 120952, 83290, 84618, 66965, 115296, 106460, 17835, 115790, 18359, 61816, 44690, 48936, 29833, 123644, 110671, 113026, 125684, 101254, 108537, 13, 110154, 43139, 23955, 102467, 124788, 104193, 123061, 30426, 41953, 116492, 13094, 103123, 117534, 19954, 74623, 101151, 119222, 120903, 111590, 96717, 57002, 65219, 117622, 122352, 92143, 102735, 94, 18359, 120693, 58083, 115777, 104019, 81673, 104193, 123061, 44690, 71682, 26799, 126110, 19954, 104441, 101711, 34983, 56773, 30426, 106647, 103153, 64189, 30446, 102423, 13447, 49208, 108, 104193, 103655, 55421, 49085, 84618, 66965, 101096, 101015, 81673, 122352, 105711, 101709, 101228, 102233, 116429, 104414, 101096, 64189, 112343, 44215, 108955, 29854, 102258, 117216, 106958, 106434, 111850, 38187, 18918, 74623, 101151, 101360, 62085, 248, 101, 33931, 63171, 35495, 18918, 107472, 102058, 29854, 49085, 67890, 102130, 48936, 72208, 23955, 105771, 109012, 13, 128009, 128006, 78191, 128007, 198, 13922, 285, 31641, 54356, 1232, 3082, 11, 364, 43324, 37888, 533, 1284, 26246, 1232, 2570, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 518, 364, 101436, 30446, 56154, 4181, 364, 43324, 52454, 1232, 2570, 101314, 58189, 33931, 58083, 115777, 518, 364, 29102, 113110, 126015, 518, 364, 35495, 82001, 102005, 65677, 86157, 518, 364, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 4181, 364, 31587, 37888, 533, 1284, 26246, 1232, 10277, 364, 31587, 52454, 1232, 10277, 364, 20489, 5595, 54965, 37888, 533, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 101003, 58189, 33931, 58083, 115777, 104019, 18918, 102258, 93917, 108859, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 102258, 117216, 67890, 30426, 24486, 110005, 84618, 83628, 66965, 52688, 101136, 123061, 127702, 81673, 103236, 30446, 56154, 114026, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 103924, 13, 125578, 11, 101254, 82001, 102005, 65677, 86157, 103686, 67945, 101824, 101480, 29102, 24486, 101603, 101096, 103686, 41953, 65677, 38187, 20565, 126999, 65219, 104448, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 109720, 29833, 103924, 16045, 364, 20489, 5595, 55260, 37888, 533, 1232, 9158, 364, 1743, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 103236, 30446, 56154, 81673, 84618, 83628, 66965, 52688, 101136, 123061, 62841, 111636, 116464, 43139, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 18918, 103153, 64189, 108859, 11, 101003, 58189, 33931, 58083, 115777, 81673, 107545, 103168, 101532, 55430, 62060, 71023, 19954, 102597, 56773, 21028, 18918, 102258, 93917, 101528, 13, 127063, 95713, 104193, 123061, 56154, 106001, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 91786, 3238, 92, 128009]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff2c92cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids 디코딩 결과:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\n",
      "금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(\n",
    "    batch[\"input_ids\"][0].tolist(),\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\ninput_ids 디코딩 결과:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6ba5b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13922, 285, 31641, 54356, 1232, 3082, 11, 364, 43324, 37888, 533, 1284, 26246, 1232, 2570, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 518, 364, 101436, 30446, 56154, 4181, 364, 43324, 52454, 1232, 2570, 101314, 58189, 33931, 58083, 115777, 518, 364, 29102, 113110, 126015, 518, 364, 35495, 82001, 102005, 65677, 86157, 518, 364, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 4181, 364, 31587, 37888, 533, 1284, 26246, 1232, 10277, 364, 31587, 52454, 1232, 10277, 364, 20489, 5595, 54965, 37888, 533, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 101003, 58189, 33931, 58083, 115777, 104019, 18918, 102258, 93917, 108859, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 102258, 117216, 67890, 30426, 24486, 110005, 84618, 83628, 66965, 52688, 101136, 123061, 127702, 81673, 103236, 30446, 56154, 114026, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 103924, 13, 125578, 11, 101254, 82001, 102005, 65677, 86157, 103686, 67945, 101824, 101480, 29102, 24486, 101603, 101096, 103686, 41953, 65677, 38187, 20565, 126999, 65219, 104448, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 109720, 29833, 103924, 16045, 364, 20489, 5595, 55260, 37888, 533, 1232, 9158, 364, 1743, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 103236, 30446, 56154, 81673, 84618, 83628, 66965, 52688, 101136, 123061, 62841, 111636, 116464, 43139, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 18918, 103153, 64189, 108859, 11, 101003, 58189, 33931, 58083, 115777, 81673, 107545, 103168, 101532, 55430, 62060, 71023, 19954, 102597, 56773, 21028, 18918, 102258, 93917, 101528, 13, 127063, 95713, 104193, 123061, 56154, 106001, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 91786, 3238, 92, 128009]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "147705d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/root/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb99f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 18:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.496100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.506600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.442400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.404400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6362ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fec3927-ca43-4d28-a87b-d6f0da8c6505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "HUG 국민·우리은행 등 표준 PF 주관금융사로 재선정\n",
      "주택도시보증공사 HUG 는 주택사업자의 원활한 자금조달을 지원하는 표준 프로젝트 파이낸싱 PF 과 후분양 표준 PF 보증의 주관금융사를 새로 선정했다고 4일 밝혔다. PF 보증이란 주택 건설 사업의 미래 현금수입과 사업성을 담보로 주택 사업자가 대출받는 토지비 등 사업비에 대한 주택사업금융의 원리금 상환을 책임지는 보증을 말한다. 후분양 PF 보증이란 주택 사업자가 주택의 일부를 후분양하는 조건으로 주택 건설 자금 대출금을 조달하는 경우에 원리금 상환을 책임지는 보증이다. HUG는 2014년 제도를 시행한 이후 표준 PF 보증 약 12조6000억원 후분양 표준 PF 보증 약 8600억원을 지원했다. 이번에 새로 선정된 주관금융사는 표준 PF의 경우 국민은행 부산은행 수협은행 우리은행 하나은행이다. 후분양 표준 PF는 부산은행 우리은행 수협은행이다. 권형택 HUG 사장은 “최근 원자재가격 급등 대출금리 인상 등 비용증가로 어려움을 겪는 주택사업자에게 저금리 금융지원을 통하여 비용을 경감시킬 수 있게 된 점을 뜻깊게 생각한다”며 “표준PF 후분양 표준PF 제도운영을 통하여 주택공급 확대를 통한 부동산 시장 안정 주거안정 지원에 큰 도움이 될 것으로 전망한다”고 밝혔다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca96be9f-6b01-42e7-a026-bd64499addad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['국민은행', '우리은행', '부산은행', '수협은행', '하나은행'], 'positive_keywords': ['HUG', '표준 프로젝트 파이낸싱', '주택사업금융', '후분양'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '이 뉴스는 HUG가 주택사업자에게 자금조달을 지원하는 표준 PF 및 후분양 표준 PF 보증의 주관금융사로 선정되었다는 내용으로, 이는 관련 은행들에게 주택건설 관련 금융 지원 기회를 제공함으로써 긍정적인 영향을 미칠 수 있다.', 'summary': 'HUG가 주택사업자 자금조달 지원을 위한 표준 프로젝트 파이낸싱의 주관금융사로 국민은행, 우리은행, 부산은행, 수협은행, 하나은행을 선정하여 저금리 금융지원을 통해 주택 사업자의 비용 증가 부담 완화에 기여할 예정이다.'}\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a007aaea-dee0-4041-bcb5-48861d5f1285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8e619d1-16f8-464d-b3c0-97d33ecdb41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f35382d6014901b9e051e54680a1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"llama3-8b-summarizer-ko/checkpoint-372\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db0e8bdc-6746-4c1f-92d8-ea50a7884e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3b7a1cf-1710-42d9-ac80-8aa95b892444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2f8730e-af15-4f72-b33f-71b6cd00505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC'], 'negative_keywords': ['스마트폰 판매 감소', '반도체 업체', '애플', '엔비디아', 'TSMC'], 'positive_impact_stocks': [], 'positive_keywords': [],'reason_for_negative_impact': '가트너가 전세계 스마트폰 판매량이 7% 감소할 것으로 전망하면서 애플을 비롯한 스마트폰 제조사와 반도체 업체인 엔비디아와 TSMC에 부정적인 영향을 미칠 것으로 예상됩니다.','reason_for_positive_impact': '','summary': '가트너는 전세계 스마트폰 판매량이 올해 7% 감소할 것으로 전망하며, 이는 애플과 같은 스마트폰 제조사 및 엔비디아와 TSMC와 같은 반도체 업체에 부정적인 영향을 미칠 것으로 예상됩니다. 또한, 유럽연합은 가상자산을 이용한 돈세탁을 막기 위해 관련 기업을 규제하는 방안에 잠정 합의하였습니다. 스피릿 항공은 프론티어 항공과의 합병을 연기하고 제트블루 항공과의 합병 가능성을 검토 중이며, 텐센트와 바이트댄스는 하반기 대규모 구조조정을 준비하고 있습니다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC', '텐센트', '바이트댄스'], 'negative_keywords': ['스마트폰 판매 감소', '인력 감축', '비용 절감', '경제 둔화'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '가트너의 보고서에 따르면 스마트폰 판매량의 감소는 애플과 같은 제조업체뿐만 아니라 엔비디아와 TSMC 같은 반도체 업체에게 부정적인 영향을 미칠 것으로 보입니다. 또한, 텐센트와 바이트댄스가 인력을 추가 감원할 계획을 발표함에 따라 이들의 주가에 부정적인 영향이 예상됩니다.', 'reason_for_positive_impact': '', 'summary': '가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망하며, 애플과 반도체 업체들이 영향을 받을 것으로 보입니다. 한편, 텐센트와 바이트댄스는 하반기 대규모 감원을 계획하고 있어, 글로벌 빅테크 기업들이 경제 둔화와 비용 절감 압박에 직면하고 있습니다.'}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자', '포커스미디어'], 'positive_keywords': ['야놀자', '포커스미디어', '동네가게 오래함께 캠페인', '소상공인', '지역경제 활성화'],'reason_for_negative_impact': '','reason_for_positive_impact': '야놀자와 포커스미디어가 협력하여 지역 소상공인을 지원하고 홍보하는 캠페인을 진행함으로써 지역 경제 활성화에 기여할 수 있을 것으로 예상되며, 이는 양사의 이미지와 브랜드 인지도 향상에 긍정적인 영향을 미칠 수 있다.','summary': '야놀자와 포커스미디어가 지역 소상공인을 지원하고 홍보하기 위한 캠페인을 진행한다. 양사는 14억 원 규모의 광고 제작 및 송출 비용을 부담하며, 지역 내 우수 소상공인의 인지도를 제고하고 매출 증대에 기여할 계획이다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자'], 'positive_keywords': ['야놀자', '포커스미디어', '소상공인 지원', '광고 캠페인', '지역경제 활성화'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '야놀자가 포커스미디어와 협력하여 지역 소상공인을 지원하는 캠페인을 진행함으로써 지역사회와의 상생 및 경제 활성화에 기여하며, 이는 브랜드 이미지와 매출 성장에 긍정적인 영향을 미칠 수 있다.', 'summary': \"야놀자가 포커스미디어와 함께 지역 소상공인을 지원하기 위한 '동네가게 오래함께' 캠페인을 진행하여 소상공인들의 인지도와 매출 증가에 기여할 예정이다. 캠페인은 서울시 노원구 동작구를 시작으로 점차 대상 범위를 확대할 계획이다.\"}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['삼성바이오로직스', 'MSD', '위탁생산 공급계약', '의약품'],'reason_for_negative_impact': '','reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 공급계약을 체결하여 매출 증가가 기대되기 때문에 긍정적인 영향을 받을 것으로 예상됩니다.','summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 공급계약을 체결하여 매출 증가가 기대된다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['위탁생산계약', 'MSD', '매출 증대'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업인 MSD와 2768억원 규모의 위탁생산계약을 체결하였으므로, 이는 회사의 매출 증대에 긍정적 영향을 미칠 수 있다.', 'summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 계약을 체결하였으며, 이는 회사의 매출 대비 상당한 규모로, 향후 매출 증가가 기대된다.'}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['LG전자', 'SM엔터테인먼트', '피트니스 캔디', '디지털 피트니스', 'K POP'],'reason_for_negative_impact': '','reason_for_positive_impact': 'LG전자와 SM엔터테인먼트가 합작하여 새로운 디지털 피트니스 브랜드를 출시하고 있으며, 이는 두 회사의 기술력과 콘텐츠 경쟁력을 강화할 수 있는 기회가 될 것으로 보입니다.','summary': 'LG전자와 SM엔터테인먼트가 디지털 피트니스 브랜드 \"피트니스 캔디\"를 출시하여 MZ세대를 겨냥한 맞춤형 운동 데이터 플랫폼을 제공하고 있습니다. 이는 두 회사의 기술력과 콘텐츠 경쟁력을 강화할 것으로 기대됩니다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['피트니스 캔디', '디지털 피트니스', 'LG전자', 'SM엔터테인먼트', 'K POP', '메타버스'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': \"LG전자와 SM엔터테인먼트가 공동으로 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 출시하며 디지털 피트니스 시장에 진출하여, 양사의 디지털 기술력과 K POP 콘텐츠를 바탕으로 새로운 수익 창출과 시장 기회를 얻게 될 가능성이 높기 때문입니다.\", 'summary': \"LG전자와 SM엔터테인먼트는 디지털 피트니스 콘텐츠 합작 브랜드 '피트니스 캔디'를 발표하며, 피트니스 시장의 확장을 기대하고 있습니다. 이 플랫폼은 개인 맞춤형 커뮤니티 서비스와 K POP 콘텐츠를 결합한 디지털 피트니스 트렌드를 지향합니다.\"}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', '공기질 관리'],'reason_for_negative_impact': '','reason_for_positive_impact': '경동나비엔이 장마철에도 실내 미세먼지 관리를 위한 청정환기시스템을 출시하여 시장에서 경쟁력을 확보하고, 고객들의 관심을 끌 수 있을 것으로 예상되기 때문입니다.','summary': '경동나비엔이 장마철에도 실내 미세먼지를 관리할 수 있는 청정환기시스템을 출시하여 건강한 생활 환경을 제공하고자 한다. 이 시스템은 공기청정과 청정환기를 동시에 구현하여 창문을 열지 않고도 실내 공기질을 관리할 수 있다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', 'UV LED 모듈', '공기청정'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '경동나비엔의 청정환기시스템이 미세먼지 문제와 장마철 환기 문제를 동시에 해결하는 혁신적인 솔루션으로 소개되면서, 해당 시스템에 대한 수요 증가가 예상된다.', 'summary': '경동나비엔의 청정환기시스템은 실내 공기질 개선 솔루션으로, 미세먼지 문제와 장마철 환기 어려움을 극복할 수 있는 제품으로 소개되었다. 이 시스템은 실내외의 공기를 깨끗하게 교환하며, 주방에서 발생하는 오염물질 확산도 방지한다.'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bec436-097e-47ba-ac90-c66e36583e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
